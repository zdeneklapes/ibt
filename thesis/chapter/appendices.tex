\documentclass[../xlapes02]{subfiles}
\begin{document}

    \chapter{Reinforcement Learning Algorithms}
    In this appendix, we present pseudocode for various RL methods and algorithms, which are used in the field of RL research and applications.

% TODO: Remove
%    \section{Policy Iteration Algorithm in DP}
%    \begin{algorithm}[h!]
%        \SetKwComment{Comment}{/* }{ */}
%        \caption{An algorithm with caption}
%        \label{alg:two}
%
%        \Comment{1. Initialization}
%        $V(s) \in \mathbb{R}$ and $\pi(s) \in A(s)$ arbitrarily for all $s \in S$; $V(\text{terminal}) = 0$.
%
%        \Comment{2. Policy Prediction}
%        \While{$\Delta < \theta$}{
%            $\Delta \leftarrow 0$
%            \ForAll{$s \in S$}{
%                $v \leftarrow V(s)$\;
%                $V(s) \leftarrow \sum_{s', r} p(s_0, r|s, \pi(s)) \cdot (r + \gamma V(s_0))$\;
%                $\Delta \leftarrow \max(\Delta, |v - V(s)|)$\;
%            }
%        }
%
%        \Comment{3. Policy Improvement}
%        \emph{policy-stable} $\leftarrow$ \emph{true}
%        \ForAll{$s \in S$}{
%            $\text{old-action} \leftarrow \pi(s)$\;
%            $\pi(s) \leftarrow \arg\max_a \sum_{s', r} p(s', r|s, a) \left[ r + \gamma V(s') \right]$\;
%            \If{$\text{old-action} \neq \pi(s)$}{
%                \emph{policy-stable} $\leftarrow$ \emph{false}
%            }
%        }
%        \eIf{$\text{policy-stable}$}
%        {
%            \Return $V_{\pi} \approx v_*$ and $\pi \approx \pi_*$\;
%        }{
%            Goto \texttt{2. Policy Prediction}
%        }
%    \end{algorithm}
%


    \section{Value Iteration Algorithm in DP}
    \begin{algorithm}[H]
        \label{alg:value-iteration}
        \SetAlgoLined
        \textbf{Value Iteration Algorithm}\\
        \textbf{Algorithm parameter:} a small threshold $\epsilon > 0$ determining accuracy of estimation\\
        \textbf{Initialize} $V(s)$, for all $s \in S$, arbitrarily except that $V(\text{terminal}) = 0$\\
        \textbf{Loop:}\\
        \quad \textbf{for each} $s \in S:$\\
        \quad \quad $v \leftarrow V(s)$\\
        \quad \quad $V(s) \leftarrow \max_a \sum_{s', r} p(s', r|s, a)(r + \gamma V(s'))$\\
        \quad \quad $\delta \leftarrow \max(\delta, |v - V(s)|)$\\
        \textbf{until} $\delta < \epsilon$\\
        \textbf{Output} a deterministic policy $\pi$, such that\\
        $\pi(s) \leftarrow \argmax_a \sum_{s', r} p(s', r|s, a)(r + \gamma V(s'))$\\
    \end{algorithm}

% TODO: Remove
%    \section{Monte Carlo Prediction}
%    \begin{algorithm}[h!]
%        \caption{First-visit MC prediction for estimating $V_\pi$}
%        \label{alg:mc}
%
%        \SetKwInput{Input}{Input}
%
%        \Input{A policy $\pi$ to be evaluated}
%
%        \emph{Initialize:}\\
%        $V(s) \in \mathbb{R}$, arbitrarily, for all $s \in S$\\
%        \emph{Returns($s$)}: an empty list, for all $s \in S$
%
%        \emph{Loop forever (for each episode):}\\
%        \While{true}{
%            Generate an episode following $\pi$: $S_0, A_0, R_1, S_1, A_1, R_2, \ldots, S_{T-1}, A_{T-1}, R_T$
%
%            $G \leftarrow 0$
%
%            \emph{Loop for each step of episode, $t = T-1, T-2, \ldots, 0$:}\\
%            \For{$t \leftarrow T-1$ \KwTo $0$}{
%                $G \leftarrow G + R_{t+1}$
%
%                \uIf{$S_t$ does not appear in $S_0, S_1, \ldots, S_{t-1}$}{
%                    Append $G$ to \emph{Returns($S_t$)}
%                    $V(S_t) \leftarrow \text{average}(\text{Returns}(S_t))$
%                }
%            }
%        }
%    \end{algorithm}


    \section{Q-Learning Off-policy TD Control}
    \begin{algorithm}[h!]
        \caption{Q-learning (Off-policy TD Control)}
        \label{alg:q_learning}

        \textbf{Algorithm parameters:} step size $\alpha \in (0, 1]$, small $\epsilon > 0$

        Initialize $Q(s, a)$ for all $s \in \mathcal{S}^+, a \in \mathcal{A}(s)$ arbitrarily, except that $Q(\text{terminal}, \cdot) = 0$

        \textbf{Loop for each episode:}
        \For{each episode}{
            Initialize $S$
            \textbf{Loop for each step of episode:}
            \While{$S$ is not terminal}{
                Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                Take action $A$, observe $R$, $S'$
                $Q(S, A) \leftarrow Q(S, A) + \alpha \cdot \left[R + \max_a Q(S', a) - Q(S, A)\right]$
                $S \leftarrow S'$
            }
        }
    \end{algorithm}

% TODO: Remove
%    \section{Tabular Dyna-Q}
%    \begin{algorithm}[h!]
%        \SetKwComment{Comment}{/* }{ */}
%        \caption{Tabular Dyna-Q}
%        \label{alg:dyna-q}
%        \Comment{1. Initialization}
%        Initialize $Q(s, a)$ and $Model(s, a)$ for all $s \in S$ and $a \in A(s)$\;
%        $S \leftarrow$ current (non-terminal) state\;
%
%        \While{True}{
%            \Comment{2. Action Selection}
%            $A \leftarrow$ $\epsilon$-greedy($S, Q$)\;
%
%            \Comment{3. Take action and observe the environment}
%            Take action $A$; observe resultant reward $R$, and new state $S_0$\;
%
%            \Comment{4. Q-Value Update}
%            $Q(S, A) \leftarrow Q(S, A) + \alpha \left( R + \gamma \max_{a'} Q(S_0, a') - Q(S, A) \right)$\;
%
%            \Comment{5. Model Update}
%            $Model(S, A) \leftarrow R, S_0$ (assuming deterministic environment)\;
%
%            \For{$i \leftarrow 1$ to $n$}{
%                \Comment{6. Model-based Planning}
%                $S \leftarrow$ random previously observed state\;
%                $A \leftarrow$ random action previously taken in $S$\;
%                $R, S_0 \leftarrow Model(S, A)$\;
%                $Q(S, A) \leftarrow Q(S, A) + \alpha \left( R + \gamma \max_{a'} Q(S_0, a') - Q(S, A) \right)$\;
%            }
%
%            $S \leftarrow S_0$\;
%
%        }
%    \end{algorithm}


%    TODO: Remove
%    \section{Random Sample One-Step Tabular Q-Planning}
%    \begin{algorithm}[H]
%        \label{alg:random-sample-one-step-tabular-q-planning}
%        \SetKwInOut{Input}{Input}
%        \SetKwInOut{Output}{Output}
%        \SetAlgoLined
%        \Input{
%            $S$: Set of states \\
%            $A$: Set of actions \\
%            $\alpha$: Learning rate \\
%            $\gamma$: Discount factor \\
%        }
%        \While{True}{
%            Select a state $s \in S$, action $a \in A(s)$ at random\;
%            Send $s, a$ to a sample model, and obtain a sample next reward $r$ and a sample next state $s'$\;
%            Apply one-step tabular Q-learning update to $s, a, r, s'$:
%            \[
%                Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left( r + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a) \right)
%            \]
%        }
%        \caption{Random-sample one-step tabular Q-planning}
%    \end{algorithm}


    \section{REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\pi_*$}
    \begin{algorithm}[h!]
        \SetKwComment{Comment}{/* }{ */}
        \caption{REINFORCE Algorithm}
        \label{alg:REINFORCE}

        \Comment{Input:}
        a differentiable policy parameterization $\pi(a|s, \bm{\theta})$\;
        a differentiable state-value parameterization $\hat{v}(a|\bm{w})$\;
        Algorithm parameters: step size $\alpha_{\bm{\theta}} > 0$, $\alpha_{\bm{w}} > 0$\;
        Initialize policy parameter $\bm{\theta} \in \mathbb{R}^{d'}$ and state-value weights $\bm{w} \in \mathbb{R}^{d}$ arbitrarily (e.g., $\bm{\theta} = \bm{0}, \bm{w} = \bm{0}$)\;

        \While{Loop forever (for each episode):}{
            Generate an episode $S_0, A_0, R_1, \ldots , S_T-1, A_T-1, R_T$, following $\pi(\cdot|\cdot, \theta)$\;
            \For{Loop for each step of the episode $t=0,1,\ldots,T-1$:}{
                $G_t\leftarrow\sum_{k=t+1}^{T}R_k$\;
                $\delta\leftarrow G-\hat{v}(S_t,\bm{w})$\;
                $\bm{w}\leftarrow\bm{w}+\alpha^{\bm{w}}\delta\nabla\hat{v}(S_t,\bm{w})$\;
                $\theta\leftarrow\bm{\theta}+\alpha^{\bm{\theta}}\gamma^{t}\delta\nabla \ln\pi(A_t|S_t,\bm{\theta})$\;
            }
        }
    \end{algorithm}


    \chapter{Setting up and running the program}
    The root directory of the thesis code contains a \texttt{README.md} file that provides instructions for installing the program. Following the steps outlined in this file should be sufficient for completing the installation process.

    The program has been tested on both MacOS 13.2.1 and Ubuntu 20.04.6 LTS (GNU/Linux 5.4.0-146-generic x86\_64), using Python version 3.10.11.


    \section{Prepare Environment}
    \begin{itemize}
        \item Create a virtual environment with \texttt{python3 -m venv venv}
        \item Activate the virtual environment with \texttt{source venv/bin/activate}
        \item Install the requirements with \texttt{pip3 install -r requirements.txt}
    \end{itemize}


    \section{Examples of running the program}
    All examples assume that the current working directory is the root directory of the thesis code.

    \subsection{Run the program to print the help message}
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/run/portfolio_allocation/thesis/train.py \
        --help
    \end{lstlisting}

    \subsection{Single Run (train/test)}
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/run/portfolio_allocation/thesis/train.py \
    --dataset-path out/dataset/stockfadailydataset.csv \
    --algorithms ppo \
    --wandb=1 \
    --project-verbose=1 \
    --train-verbose=1 \
    --wandb-verbose=1 \
    --total-timesteps=1000 \
    --train=1 \
    --test=1 \
    --env-id=1 \
    --wandb-run-group="exp-run-1" \
    --baseline-path=out/baseline/baseline.csv
    \end{lstlisting}

    \subsection{Sweep Run: 3 runs with random hyperparameters over 2 datasets and 5 algorithms (train/test)}
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/run/portfolio_allocation/thesis/train.py \
    --dataset-path \
        out/dataset/stockfadailydataset.csv \
        out/dataset/stockcombineddailydataset.csv \
    --wandb=1 \
    --wandb-sweep=1 \
    --wandb-sweep-count=3 \
    --algorithms \
        ppo \
        a2c \
        td3 \
        ddpg \
        sac \
    --project-verbose=1 \
    --train-verbose=1 \
    --wandb-verbose=1 \
    --total-timesteps=1000 \
    --train=1 \
    --test=1 \
    --env-id=1 \
    --wandb-run-group="exp-sweep-1" \
    --baseline-path=out/baseline/baseline.csv
    \end{lstlisting}

    \subsection{Create Datasets}
    The scripts require a folder that contains downloaded and processed raw data. As a result, these scripts may not function properly on your computer. However, all of these datasets are accessible on W\&B at the following URL: \url{https://wandb.ai/investai/portfolio-allocation}.
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
investai/run/portfolio_allocation/thesis/dataset/stockfadailydataset.py \
    --dataset-paths out/dataset/stockfadailydataset.csv \
    --project-verbose=1

PYTHONPATH=$PWD/investai python3 \
investai/run/portfolio_allocation/thesis/dataset/stocktadailydataset.py \
    --dataset-paths out/dataset/stocktadailydataset.csv \
    --project-verbose=1

PYTHONPATH=$PWD/investai python3 \
investai/run/portfolio_allocation/thesis/dataset/stockcombinedailydataset.py \
--project-verbose=1 \
--dataset-paths \
    out/dataset/stockfadailydataset.csv \
    out/dataset/stocktadailydataset.csv \
    out/dataset/stockcombineddailydataset.csv
    \end{lstlisting}

    \subsection{Create Baseline}
    Firstly, it is necessary to create or download the dataset. In this case, we will utilize the \emph{stockfadailydataset.csv} dataset.
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/extra/math/finance/shared/baseline.py \
    --project-verbose=1 \
    --dataset-paths out/dataset/stockfadailydataset.csv \
    --baseline-path=out/baseline/baseline.csv
    \end{lstlisting}

\end{document}
