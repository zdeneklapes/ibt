\documentclass[../xlapes02]{subfiles}
\begin{document}

    \chapter{Reinforcement Learning Algorithms}
    In this appendix, we present pseudocode for various RL methods and algorithms, which are used in the field of RL research and applications.


    \section{Value Iteration Algorithm in DP}
    \begin{algorithm}[H]
        \label{alg:value-iteration}
        \SetAlgoLined
        \textbf{Value Iteration Algorithm}\\
        \textbf{Algorithm parameter:} a small threshold $\epsilon > 0$ determining accuracy of estimation\\
        \textbf{Initialize} $V(s)$, for all $s \in S$, arbitrarily except that $V(\text{terminal}) = 0$\\
        \textbf{Loop:}\\
        \quad \textbf{for each} $s \in S:$\\
        \quad \quad $v \leftarrow V(s)$\\
        \quad \quad $V(s) \leftarrow \max_a \sum_{s', r} p(s', r|s, a)(r + \gamma V(s'))$\\
        \quad \quad $\delta \leftarrow \max(\delta, |v - V(s)|)$\\
        \textbf{until} $\delta < \epsilon$\\
        \textbf{Output} a deterministic policy $\pi$, such that\\
        $\pi(s) \leftarrow \argmax_a \sum_{s', r} p(s', r|s, a)(r + \gamma V(s'))$\\
    \end{algorithm}


    \section{Q-Learning Off-policy TD Control}
    \begin{algorithm}[h!]
        \caption{Q-learning (Off-policy TD Control)}
        \label{alg:q_learning}

        \textbf{Algorithm parameters:} step size $\alpha \in (0, 1]$, small $\epsilon > 0$

        Initialize $Q(s, a)$ for all $s \in \mathcal{S}^+, a \in \mathcal{A}(s)$ arbitrarily, except that $Q(\text{terminal}, \cdot) = 0$

        \textbf{Loop for each episode:}
        \For{each episode}{
            Initialize $S$
            \textbf{Loop for each step of episode:}
            \While{$S$ is not terminal}{
                Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
                Take action $A$, observe $R$, $S'$
                $Q(S, A) \leftarrow Q(S, A) + \alpha \cdot \left[R + \max_a Q(S', a) - Q(S, A)\right]$
                $S \leftarrow S'$
            }
        }
    \end{algorithm}


    \section{REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\pi_*$}
    \begin{algorithm}[h!]
        \SetKwComment{Comment}{/* }{ */}
        \caption{REINFORCE Algorithm}
        \label{alg:REINFORCE}

        \Comment{Input:}
        a differentiable policy parameterization $\pi(a|s, \bm{\theta})$\;
        a differentiable state-value parameterization $\hat{v}(a|\bm{w})$\;
        Algorithm parameters: step size $\alpha_{\bm{\theta}} > 0$, $\alpha_{\bm{w}} > 0$\;
        Initialize policy parameter $\bm{\theta} \in \mathbb{R}^{d'}$ and state-value weights $\bm{w} \in \mathbb{R}^{d}$ arbitrarily (e.g., $\bm{\theta} = \bm{0}, \bm{w} = \bm{0}$)\;

        \While{Loop forever (for each episode):}{
            Generate an episode $S_0, A_0, R_1, \ldots , S_T-1, A_T-1, R_T$, following $\pi(\cdot|\cdot, \theta)$\;
            \For{Loop for each step of the episode $t=0,1,\ldots,T-1$:}{
                $G_t\leftarrow\sum_{k=t+1}^{T}R_k$\;
                $\delta\leftarrow G-\hat{v}(S_t,\bm{w})$\;
                $\bm{w}\leftarrow\bm{w}+\alpha^{\bm{w}}\delta\nabla\hat{v}(S_t,\bm{w})$\;
                $\theta\leftarrow\bm{\theta}+\alpha^{\bm{\theta}}\gamma^{t}\delta\nabla \ln\pi(A_t|S_t,\bm{\theta})$\;
            }
        }
    \end{algorithm}


    \chapter{Setting up and running the program}
    The root directory of the thesis code contains a \texttt{README.md} file that provides instructions for installing the program. Following the steps outlined in this file should be sufficient for completing the installation process.

    The program has been tested on both MacOS 13.2.1 and Ubuntu 20.04.6 LTS (GNU/Linux 5.4.0-146-generic x86\_64), using Python version 3.10.11.


    \section{Prepare Environment}
    Run this command from the root directory of the thesis code to create a virtual environment and install the required packages:
    \begin{lstlisting}[language=bash]
./start.sh --project-install
    \end{lstlisting}

    The virtual environment must be activated before running the program. To activate the virtual environment, run the following command from the root directory of the thesis code:
    \begin{lstlisting}[language=bash]
source venv/bin/activate
    \end{lstlisting}


    \section{Datasets}
    You can download the datasets from Weights \& Biases or create it yourself, but raw data is required to create it.

    \subsection{Download}
    In the root directory of the thesis code, run the following commands to download the datasets from Weights \& Biases:
    \begin{lstlisting}[language=bash]
mkdir out/dataset
wandb artifact get investai/portfolio-allocation/dataset:v520 --root out/dataset
wandb artifact get investai/portfolio-allocation/dataset:v519 --root out/dataset
wandb artifact get investai/portfolio-allocation/dataset:v517 --root out/dataset
    \end{lstlisting}

    \subsection{Create}
    The scripts require a folder that contains downloaded and processed raw data. As a result, these scripts may not function properly on your computer. However, all of these datasets are accessible on W\&B at the following URL: \url{https://wandb.ai/investai/portfolio-allocation/artifacts/dataset/dataset}.
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
investai/run/portfolio_allocation/thesis/dataset/stockfadailydataset.py \
    --dataset-paths out/dataset/stockfadailydataset.csv \
    --project-verbose=1

PYTHONPATH=$PWD/investai python3 \
investai/run/portfolio_allocation/thesis/dataset/stocktadailydataset.py \
    --dataset-paths out/dataset/stocktadailydataset.csv \
    --project-verbose=1

PYTHONPATH=$PWD/investai python3 \
investai/run/portfolio_allocation/thesis/dataset/stockcombinedailydataset.py \
--project-verbose=1 \
--dataset-paths \
    out/dataset/stockfadailydataset.csv \
    out/dataset/stocktadailydataset.csv \
    out/dataset/stockcombineddailydataset.csv
    \end{lstlisting}


    \section{Baseline}
    You can either download the baseline from Weights \& Biases or create it yourself.

    \subsection{Download}
    In the root directory of the thesis code, run the following commands to download the baseline from Weights \& Biases:
    \begin{lstlisting}[language=bash]
mkdir out/baseline
wandb artifact get investai/portfolio-allocation/baseline:latest --root out/baseline
    \end{lstlisting}

    \subsection{Create}
    Firstly, it is necessary to create or download the dataset. In this case, we will utilize the \emph{stockfadailydataset.csv} dataset.
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/extra/math/finance/shared/baseline.py \
    --project-verbose=1 \
    --dataset-paths out/dataset/stockfadailydataset.csv \
    --baseline-path=out/baseline/baseline.csv
    \end{lstlisting}


    \section{Examples of running Train/Test program}
    All examples assume that the current working directory is the root directory of the thesis code.

    The examples expect the \emph{.env} file in the root directory of the thesis code. This file contains the following variables, please fill the \texttt{WANDB\_API\_KEY} with your API key:
    \begin{lstlisting}[language=bash]
# W&B
WANDB_API_KEY=''
WANDB_ENTITY='investai'
WANDB_PROJECT='portfolio-allocation'
WANDB_TAGS='["None"]'
WANDB_JOB_TYPE='train'
WANDB_RUN_GROUP='exp-1'
WANDB_MODE='online'
WANDB_DIR='${PWD}/out/model'
    \end{lstlisting}
    If you don't want to use Weights \& Biases, you can remove the arguments with prefix \texttt{--wandb} from the examples (sweep run will not work).

    \subsection{Run the program to print the help message}
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/run/portfolio_allocation/thesis/train.py \
        --help
    \end{lstlisting}

    \subsection{Single Run (train/test)}
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/run/portfolio_allocation/thesis/train.py \
    --dataset-paths out/dataset/stockfadailydataset.csv \
    --algorithms ppo \
    --project-verbose=1 \
    --train-verbose=1 \
    --total-timesteps=1000 \
    --train=1 \
    --test=1 \
    --env-id=1 \
    --wandb=1 \
    --wandb-run-group="exp-run-1" \
    --wandb-verbose=1 \
    --baseline-path=out/baseline/baseline.csv
    \end{lstlisting}

    \subsection{Sweep Run: 3 runs with random hyperparameters over 2 datasets and 5 algorithms (train/test)}
    \begin{lstlisting}[language=bash]
PYTHONPATH=$PWD/investai python3 \
    investai/run/portfolio_allocation/thesis/train.py \
    --dataset-paths \
        out/dataset/stockfadailydataset.csv \
        out/dataset/stockcombineddailydataset.csv \
    --algorithms \
        ppo \
        a2c \
        td3 \
        ddpg \
        sac \
    --project-verbose=1 \
    --train-verbose=1 \
    --total-timesteps=1000 \
    --train=1 \
    --test=1 \
    --env-id=1 \
    --wandb=1 \
    --wandb-sweep=1 \
    --wandb-sweep-count=3 \
    --wandb-verbose=1 \
    --wandb-run-group="exp-sweep-1" \
    --baseline-path=out/baseline/baseline.csv
    \end{lstlisting}

\end{document}
