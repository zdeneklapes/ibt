\documentclass[../xlapes02]{subfiles}
\begin{document}
    \chapter{Reinforcement Learning}\label{ch:reinforcement-learning}

    The motivation for this chapter comes from the influential book on reinforcement learning by Richard Sutton and Andrew Barto and Foundations of Reinforcement Learning with Application in Finance by Ashwin Rao and Tikhon Jelvis~\cite{rao2022foundations, sutton2018reinforcement}.

    This chapter explore the application of Reinforcement Learning techniques in the context of portfolio allocation. In \cref{sec:markov-models-for-rl} we will provide an introduction to Markov models, first explore \emph{Markov Processes}, \emph{Markov Reward Processes} and \emph{Markov Decision Processes}, which are the fundamental building blocks of Reinforcement Learning. Afterward, we divide Reinfocement Learning into two main categories. In \cref{sec:model-based-methods}, we will discuss \emph{Model-based} Reinforcement Learning methods, and in \cref{sec:model-free-methods} we will discuss \emph{Model-free} Reinforcement Learning methods in details with advantages and disadvantages of each approach. Let start with introduction to AI in general and its different types of learning:

    \paragraph{Supervised Learning}
    In supervised learning (SL), a model is trained on a labeled dataset, where the input data is paired with corresponding output labels. The model learns to make predictions based on the labeled examples, and the goal is to minimize the error between predicted outputs and actual labels. Common applications of supervised learning include image classification, speech recognition, and sentiment analysis.

    \paragraph{Semi-supervised Learning}
    Semi-supervised learning (SSL) is a combination of supervised and unsupervised learning. It uses a small labeled dataset along with a large unlabeled dataset for training. The model leverages the limited labeled examples to learn patterns from the unlabeled data, and then makes predictions on unseen data. SSL is useful when obtaining labeled data is expensive or time-consuming. It is often used in scenarios where obtaining a large labeled dataset is challenging, such as in medical diagnosis or fraud detection.

    \paragraph{Unsupervised Learning}
    In unsupervised learning (UL), the model learns from unlabeled data without any predefined output labels. The goal is to find underlying patterns, structures, or relationships within the data. Common unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection. UL is used in scenarios where labeled data is scarce or not available, and the model needs to discover patterns autonomously from the data.

    The last type is Reinforcement Learning and this entire chapter will be devoted to it, let's dive into it in more detail.


    \section{Introduction}\label{sec:rl-introduction}
    Reinforcement learning (RL) is an exciting field at the intersection of artificial intelligence (AI) and machine learning (ML) that deals with training agents to make optimal decisions in dynamic environments. RL is inspired by the way humans learn from experience, like \texttt{trial-and-error}, and an agent interacts with an environment the same way and receives feedback in the form of \emph{rewards} (typically positive number, e.g.: $1$) or \emph{penalties} (typically negative number, e.g.: $-1$), and uses this feedback to learn and improve its decision-making abilities.

    At the heart of RL lies the concept of an agent, which that takes actions in an environment to achieve specific goals. The environment is typically modeled as a Markov decision process (MDP), which is a mathematical framework that describes how an agent interacts with an environment in discrete time steps. An MDP is defined by a tuple $(S, A, P, R)$, where $S$ is the state space, $A$ is the action space, $P$ is the transition probability function, and $R\in\mathbb{R}$ is the reward.

    The goal of an RL agent is to learn a policy, denoted by $\pi$, which is a mapping from states to actions that maximizes the cumulative reward $G_t$ over time $T$. The agent uses this policy to select actions at each time step, and the environment responds with a new state and a reward. The agent then updates its policy based on the observed rewards and states, aiming to improve its decision-making abilities and achieve higher rewards in the long run. RL does not require labeled data, because learns from observations and rewards via interaction with the environment.

%    RL algorithms can be broadly categorized into two main types: model-free and model-based. Model-free algorithms are then further divided into value-based and policy-based methods, such as Q-learning (value-based) and policy gradient methods (policy-based), learn directly from the interactions with the environment without explicitly modeling the transition probabilities and rewards. Model-based algorithms, on the other hand, learn a model of the environment and use it to make decisions. These algorithms have their strengths and weaknesses, and the choice between them depends on the specific problem and requirements.

    The sequence of states, actions and rewards that the agent experiences is called a trajectory, and it look like this:
    \begin{equation}
        \label{eq:trajectory}
        (S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \dots, S_{T-1}, A_{T-1}, R_T, S_T)
    \end{equation}

    This sequence of \emph{state-action-reward} can be finite or infinite, depending on the environment and the agent's goal. Pretty good example of this is the game of chess, where the game ends when one of the players wins or the game is a draw. In this case, the trajectory is finite, and the agent's goal is to maximize the cumulative reward over time $T$. On the other hand, the self-driving car example is an infinite-horizon problem, where the agent's goal is to maximize the cumulative reward over an infinite time horizon or until the car reaches its destination~\cite{FITMT25127}.

    \begin{figure}[h]
        \includegraphics[width=0.7\linewidth]{image/agent-environment}
        \centering
        \caption{The agent interacts with the environment and learns to maximize the cumulative reward over time $T$.}
        \label{fig:rl-introduction}
    \end{figure}

    Because RL agent is categorized by the way it learns, there are many different types of RL agents. We will try to differentiate them in this chapter, because it is important to understand teh differences between them, to understand the behavior of the agent and to choose the right algorithm for the problem. Based on the different computable approaches we can categorize RL agents:
    \begin{multicols}{2}
        \begin{itemize}
            \item \textbf{Value-based}
            \begin{itemize}
                \item \textcolor{darkgray}{No Policy (implicit)}
                \item Value function
            \end{itemize}
            \item \textbf{Policy-based}
            \begin{itemize}
                \item Policy
                \item \textcolor{darkgray}{No Value function}
            \end{itemize}
            \item \textbf{Actor-critic}
            \begin{itemize}
                \item Policy
                \item Value function
            \end{itemize}
        \end{itemize}
        \columnbreak
        \begin{itemize}
            \item \textbf{Model-based}
            \begin{itemize}
                \item Policy and/or Value function
                \item \textcolor{darkgray}{No model}
            \end{itemize}
            \item \textbf{Model-free}
            \begin{itemize}
                \item Policy and/or Value function
                \item Model
            \end{itemize}
        \end{itemize}
    \end{multicols}

    \emph{Model-based} algorithm are closely relate to \emph{Planning algorithms} which are trying to create a model of the environment and use it to make decisions, they have knowledge of the transition probabilities and rewards from the beginning, we will discuss them in \cref{sec:model-based-methods}. \emph{Value-based} algorithms can implicitly infer a policy, but do not explicitly compute it, we will discuss them in \cref{sec:value-based-methods}, and \emph{Policy-based} algorithms compute a policy function by themselves, and are discussed in \cref{sec:policy-based-methods}. Another agents like \emph{Actor-critic methods} compute a policy function and a state-value function at the same time~\cite{rl-course-david-silver, sutton2018reinforcement}.

    \begin{figure}[h]
        \includegraphics[width=0.5\linewidth]{image/model-value-policy}
        \centering
        \caption{The relationship between model, value function and policy. Source:~\cite{FITMT25127}}
        \label{fig:model-value-policy-introduction}
    \end{figure}


    \section{Markov Theory}\label{sec:markov-models-for-rl}
    This section provides introduction to Markov theory, which is the fundamental building blocks of Reinforcement Learning. We begin by \emph{Markov Process}, described in~\cref{subsec:markov-process}, which is a stochastic process that satisfies the Markov property. We then move on to \emph{Markov Reward Process}, described in~\cref{subsubsec:markov-reward-process}, which is a Markov process with a reward function. Finally, we describe the most important for RL \emph{Markov Decision Process}, described in~\cref{subsec:markov-decision-process}, which is a Markov Reward process with a decision-making ability.

    \subsection{Markov Process}\label{subsec:markov-process}
    The Markov process (MP) (also called Markov Chains) describes the states of an environment and models the dynamics of state transitions. In an MP, an agent can only observe the changing states of the environment and has no influence over them. Markov processes have two key properties. Firstly, state transitions are non-deterministic and influenced by randomness. States are modeled as realizations of random variables, defined in \cref{subsubsec:probability-functions}. Secondly, the future state is only dependent on the current state, and not on previous states, simplifying causality with the Markov property~\cite{FITMT25127}.

    \subsubsection{Probability Functions}\label{subsubsec:probability-functions}

    The first property of an MP states that each concrete state of an environment is the realization of a discrete random variable \(X\) from set \(V\), with a certain probability \(P(X=x)\), where \(x \in V\). A state is a realization of a random experiment that the environment assumes with a certain probability, and this can be represented as a probability function:
    \begin{equation}
        P(X = X(\omega)) = P(X = x)
    \end{equation}

    The repeated successive execution of a random experiment can be represented as a stochastic process, which is a sequence of random variables, e.g., \(X_t(\omega), X_{t+1}(\omega), \ldots X_n(\omega)\), where a single term can be shortened to \(X_t\) and it represents the state of the environment at time \(t\)~\cite{FITMT25127, rao2022foundations}.

    \subsubsection{Stochastic Process (Random Process)}\label{subsubsec:stochastic-process}
    A stochastic process is defined as a collection of random variables defined on a common probability space $(\Omega, \mathcal{F}, P)$, where $\Omega$ is a sample space, $\mathcal{F}$ is a $\sigma$-algebra, and $P$ is a probability measure; and the random variables, indexed by some set $T$, all take values in the same mathematical space $S$, which must be measurable with respect to some $\sigma$-algebra $\Sigma$.

    In other words, for a given probability space $(\Omega, \mathcal{F}, P)$ and a measurable space $(S, \Sigma)$, a stochastic process is a collection of $S$-valued random variables, which can be written as:~\cite{enwiki:1148510872}
    \[
        \{X(t) : t \in T\}.
    \]

    In stochastic processes the probability that the environment assumes a certain state depends on the realized states of previous random variables. For example, if the weather forecast is assumed to be a stochastic process, then the yesterday’s weather may still have an influence on tomorrow’s weather. To represent this causality complicates the modeling of stochastic processes, so that with definition of Markov property the dependence of future states is assumed only on the current state. This is the second important property of Markov process~\cite{FITMT25127}.

    \subsubsection{Markov Property}\label{subsubsec:markov-property}
    The Markov property, which is defined using conditional probability, states that ``The future is independent of the past, given the present."

    The stochastic process has the Markov property if and only if, for all time steps $t \in N$, the conditional probability of the next state given the current state is equal to the conditional probability of the next state given all the previous states $X_1, \ldots, X_t$. $\probP[X_{t+1}|X_t] = \probP[X_{t+1}|X_1,\ldots,X_t]$

    This property has several advantages in practical reinforcement learning, including the uniqueness and distinctiveness of states, as well as the ability to precisely formulate the probability of state transitions, defined as:~\cite{FITMT25127}
    \begin{equation}
        \mathcal{P}(x'|x)=\probP(X_{t+1}=x'|X_t=x)
    \end{equation}

    Given n possible states, $s\in S$, then the probability of transitioning from state $s$ to state $s'$ can be represented as a matrix $P$, and because probability summation rule, the sum of transition probabilities from state $s$ to any other state $s'$ must equal to 1.

    \begin{definition}
        The Markov process is stochastic process that satisfies the Markov property and is described as tuple $\left(\mathcal{S}, \mathcal{P}\right)$ for which holds:\cite{ABATE2021102207}
        \begin{itemize}
            \item $\mathcal{S} = s_1, s_2, ..., s_n$ is a finite set of states
            \item $\mathcal{P}$ is an $n\times n$ transition probability matrix which sums to 1 for each row, so each value $p_{ij}$ is the probability of transitioning from state $s_i$ to state $s_j$ in interval $\left< 0;1 \right>$
        \end{itemize}
    \end{definition}

    \begin{align*}
        \mathcal{P}=\begin{bmatrix}
                        p_{11} & p_{12} & p_{13} & \dots  & p_{1n} \\
                        p_{21} & p_{22} & p_{23} & \dots  & p_{2n} \\
                        \vdots & \vdots & \vdots & \ddots & \vdots \\
                        p_{n1} & p_{n2} & p_{n3} & \dots  & p_{nn}
        \end{bmatrix}
    \end{align*}

    \subsubsection{Starting States}
    The probability distribution of start states is denoted as $\mu : N \rightarrow [0,1]$ in order to perform simulations and compute the probability distribution of states at specific future time steps. A Markov Process is fully specified by the transition probability function $P$, which governs the complete dynamics of the process.
    \begin{itemize}
        \item Specification of the transition probability function $P$.
        \item Specification of the probability distribution of start states (denote this as $\mu : N \in[0, 1])$.
    \end{itemize}
    Given $\mu$ and $P$, we can generate sampling traces of the Markov Process and answer questions such as probability distribution of states at specific future time steps or expected time of first occurrence of a specific state, given a certain starting probability distribution $\mu$. The separation of concerns between $P$ and $\mu$ is key to the conceptualization of Markov Processes~\cite{rao2022foundations}.

    \subsubsection{Terminal States}
    Markov Processes can terminate at specific states (e.g., based on rules for winning or losing in games). Termination can occur after a variable number of time steps (episodic) or after a fixed number of time steps (as in many financial applications). If all sampling traces of the Markov Process reach a terminal state, they are called episodic sequences. The notion of episodic sequences is important in Reinforcement Learning. In some financial applications, the Markov Process terminates after a fixed number of time steps $T$, and states with time index $t = T$ are labeled as terminal states. States with time index $t < T$ transition to states with time index $t + 1$~\cite{rao2022foundations}.

    \begin{figure}[h!]
        \begin{center}
            \begin{tikzpicture}
                [->,>=stealth,shorten >=1pt,auto,node distance=3cm,
                thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

                % Nodes
                \node[main node] (A) {$S_0$}; % Starting state
                \node[main node] (B) [right of=A] {$S_1$};
                \node[main node] (C) [right of=B] {$S_2$};
                \node[main node] (T) [right of=C] {$S_3$}; % Terminal state

                % Edges
                \path[every node/.style={font=\sffamily\small}]
                (A) edge [bend left] node [above] {0.5} (B)
                (B) edge [bend left] node [below] {0.5} (A)
                (B) edge [bend left] node [above] {0.5} (C)
                (C) edge [bend left] node [below] {0.5} (B)
                (C) edge [bend left] node [above] {0.5} (T);
            \end{tikzpicture}
            \caption{Markov Process with Start state $S_0$ and Terminal state $S_3$, because there is no edge from $S_3$.}
            \label{fig:markov-process}
        \end{center}
        \centering
    \end{figure}

    The examples shown here only include states and transition probabilities. To fully define an environment within the framework of RL, actions and rewards also need to be defined.

    \subsection{Markov Reward Process}\label{subsubsec:markov-reward-process}
    Markov Reward Process (MRP) is a Markov Process with rewards. These rewards are random, and all we need to do is to specify the probability distributions of these rewards as we make state transitions. The main purpose of Markov Reward Processes is to calculate how much reward we would accumulate (in expectation, from each of the non-terminal states) if we let the process run indefinitely, bearing in mind that future rewards need to be discounted appropriately $\gamma$ (otherwise, the sum of rewards could blow up to $\infty$). In order to solve the problem of calculating expected accumulative rewards, defined in \cref{par:expected-reward}, from each non-terminal state, we will first set up some formalism for Markov Reward Processes and develop some theory on calculating rewards accumulation.

    The main objective of an RL agent is to maximize the sum of rewards from each time-step. The agent can observe different episodes in the Markov process, but lacks the means to determine the actual quality of an episode. By calculating the reward, we can precisely measure the goodness of an episode or even a single state using the \emph{state-value function}, defined in \cref{subsubsec:state-value-function}. This allows the agent to actively transition to favorable states and maximize the reward.

%    \begin{definition}
%        A Markov Reward Process is a Markov Process with a time-indexed sequence of Reward random variables $R_t \in D$ (a countable subset of $R$) for time steps $t = 1, 2, \cdot$, satisfying the Markov Property (including Rewards): $P[(Rt+1, St+1)|St, St−1, ..., S0] = P[(Rt+1, St+1)|St]$ for all $t ≥ 0$.
%    \end{definition}

%    \begin{definition}
%        \textbf{Markov Reward Process} is a Markov Process, along with a time-indexed sequence of Reward random variables $R_t \in D$ (a countable subset of $\mathbb{R}$) for time steps $t = 1, 2, \ldots$, satisfying the Markov Property (including Rewards): $P[(R_{t+1}, S_{t+1})|S_t, S_{t-1}, \ldots, S_0] = P[(R_{t+1}, S_{t+1})|S_t]$ for all $t \geq 0$.
%    \end{definition}

    \begin{definition}
        Markov Reward Processes is a tuple $(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$ for which holds:\cite{rao2022foundations}
        \begin{itemize}
            \item $\mathcal{S} = s_1, s_2, ..., s_n$ is a finite set of states
            \item $\mathcal{P}$ is an $n\times n$ transition probability matrix which sums to 1 for each row, so each value $p_{ij}$ is the probability of transitioning from state $s_i$ to state $s_j$ in interval $\left< 0;1 \right>$
            \item $\mathcal{R}$ is a sequence of random variables $R_1, R_2, ..., R_n$ where $R_t$ is a random variable that represents the reward for transitioning from state $s_t$ to state $s_{t+1}$
            \item $\gamma$ is a discount factor in interval $\left< 0;1 \right>$
        \end{itemize}
        \begin{equation}
            \mathcal{P}(s,r,s') = \probP\left[ R_{t+1} = r, S_{t+1} = s' | S_t = s \right] \text{ for time steps } t = 0, 1, 2, \ldots
        \end{equation}
        \begin{align*}
            \text{ for all } s \in N, r \in D, s' \in S, \text{ such that } \sum_{s' \in S} \sum_{r \in D} \mathcal{P}(s,r,s') = 1 \text{ for all } s \in N
        \end{align*}
    \end{definition}

    \subsubsection{Reward function}\label{par:reward-function}
    The reward function $\mathcal{R}(s)$ is a function that maps a state $s$ to a reward $r$ and specify how much reward and agent expects from the environment given current state $s$. If an agent is in a state $s$ at time $t$, the agent receives reward  $R_{t+1}$  at time $t + 1$,  when it transitions to a subsequent state $s'$. Rewards of an episode can be represented as a sequence $(R_1, R_2, \ldots, R_t)$~\cite{FITMT25127}.

    % TODO: Agent reference

    \paragraph{When to receive reward?} An RL agent, defined in \verb|\cref{TODO}|, should receive a reward for a good action and a penalty for a bad action. Good actions are those that lead to the agent's main goal, while bad actions are those that lead to a state that is not desirable for the agent. That is, an agent should not receive (one small) reward when it can then receive a large penalty, for example in chess by taking one piece, when it can then lose the game by getting checkmate~\cite{sutton2018reinforcement}.

    \subsubsection{Expected reward}\label{par:expected-reward}
    The expected reward, denoted as $G_t$ at time $t$ is the discounted sum of rewards in a single episode. Can be used to calculate the sum of discounted rewards in an episode.

    \begin{equation}
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    \end{equation}


    We can also calculate the expected rewards for state-action pairs as a two-argument function $r: S \times A \to R$.
    The expected reward of a state-action pair as two-argument function $r: \mathcal{S}\times \mathcal{A}\leftarrow \mathbb{R}$ is defined as:
    \begin{equation}
        r(s, a) = \sum_{t=0}^{\infty} \gamma^t R_{t+1} P(S_{t+1} = s|S_t = s, A_t = a)
    \end{equation}

    \paragraph{Discount factor $\gamma$}\label{par:discount-factor}
    The calculation also involves the discount factor $\gamma$ which is a value in the interval $\langle 0, 1 \rangle$. If $\gamma$ is equal to one, the series value goes to infinity. The agent can only calculate the reward in the case of always terminating episodes. If $\gamma$ is less than one, the reward has a finite value, allowing the agent to determine the quality of an episode. The discount factor is not only useful mathematically but also for tuning the agent's rewards. If early rewards in an episode are more significant than later ones, $\gamma$ should be close to zero. If the rewards represent monetary gains, then it is the case, as early rewards earn additional interest. On the other hand, the closer $\gamma$ is to one, the more important later rewards are.

    \subsubsection{State-value Function}\label{subsubsec:state-value-function}
    The state-value function provides information about the long-term expected reward for each state in an environment. With this information, an agent can determine which state to transition to in order to maximize the reward of an episode. Specifically, the agent should choose the state that has the highest long-term expected reward. If the agent observes a sequence of states and rewards, it can remember the subsequent rewards for each state, calculate the reward of an episode, and iteratively update the probabilities for higher occurrence of rewards over multiple episodes. As the number of episodes approaches infinity, the estimated probabilities converge to the true probabilities, and the long-term expected reward of a state can be accurately determined. Intuitively, the more episodes and consequent rewards an agent observes, the better it can estimate the value of each state. We will introduce the methods for determining the state-value function and the action-value function in the next subsection, along with a recursive iterative approach for calculating the state-value function based on Bellman's equation~\cite{FITMT25127}.

    Since episodes may start with different states due to state transitions probabilities, the expected reward of a particular state is the expected value of the conditional density function over the probabilities of rewards for that state. Thus, $G_t$ can be treated mathematically as a continuous random variable.
    To derive \emph{Bellman's equation}, defined later in this subsection, we need to make use of the definition of the \emph{(Recursive) State-value function}~\cite{FITMT25127, rao2022foundations}.
    \begin{equation}
        \begin{split}
            \textcolor[RGB]{51, 204, 51}{v(s)} &= E[G_t | S_t = s] \\
            &= E[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
            &= E[R_{t+1} + \gamma \textcolor[RGB]{51, 204, 51}{v(S_{t+1})} | S_t = s]\\
            &= \mathcal{R}(s) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}(s' | s) \textcolor[RGB]{51, 204, 51}{v(s')}
        \end{split}
    \end{equation}
    where $\gamma$ is the discount factor, $\mathcal{R}(s)$ is the immediate reward for state $s$, $\mathcal{P}(s' | s)$ is the transition probability from state $s$ to state $s'$, and $\mathcal{S}$ is the set of all possible states in the environment. The last equation expresses that the long-term expected reward of a state depends only on the immediate reward and the long-term expected reward of the subsequent states.

    In the case of Finite Markov Reward Processes, let's assume that the state space is denoted as $\mathcal{S} = \{s_1, s_2, ..., s_n\}$, and the subset of states of interest is denoted as $\mathcal{N}$ with $m \leq n$ states. We can use bold-face notation to represent functions as column vectors and matrices, since we are dealing with finite states/transitions. So, $V$ is a column vector of length $m$, $P$ is an $m \times m$ matrix, and $R$ is a column vector of length $m$ (with rows/columns corresponding to states in $\mathcal{N}$). We can express the equation in vector and matrix notation as follows:
    \begin{equation}
        \begin{split}
            V &= R + \gamma P \cdot V \\
            &= (I_m - \gamma P)^{-1} \cdot R
        \end{split}
    \end{equation}
    where $I_m$ is the identity matrix of size $m \times m$, and $\gamma$ is the discount factor~\cite{rao2022foundations}.

    By extending the equation to include actions, we arrive at the most important equation in all of reinforcement learning: the Bellman equation, defined in ~\cref{subsubsec:bellman-equation}. It states that to calculate the long-term expected reward from a state, an agent only needs to add together the reward of the current state and the long-term expected reward of the next state~\cite{FITMT25127}.

    \subsection{Markov Decision Process}\label{subsec:markov-decision-process}
    The Markov Decision Process (MDP) allows an agent to actively influence changes in the state of the environment through its actions. Within the MDP, the agent has the ability to jointly determine the subsequent state to which the environment should transition. The agent's primary goal is to strategically choose actions that maximize expected payoff. In the previous subsections, we addressed the aspect of \emph{sequential uncertainty (e.g., MRP)} using the Markov process framework and extended it to include the uncertain reward $R_t\in\mathcal{R}$ at each state transition $p(s',r,s)$, referred to as Markov reward processes. However, this framework lacks the notion of \emph{sequential decision making} and in this section this notion is introduced in terms of MDP, a generalization of MRP that includes the notion of \emph{sequential decision making}~\cite{rao2022foundations}.

    \begin{definition}
        Markov decision process (MDP) is Markov reward process with actions. It is an tuple $(S, A, \mathcal{P}, \mathcal{R}, \gamma)$, where:
        \begin{itemize}
            \item $\mathcal{S}$ is finite set of states $S$ (known as the State Space), a set $T \subseteq S$ (known as the set of Terminal States)
            \item $\mathcal{A}$ is finite set of actions $A$ (known as the Action Space)
            \item $\mathcal{P}$ is a transition probability function $p(s',r,s,a)$, which is a function that maps a state $s$, an action $a$, a next state $s'$ and a reward $r$ to a probability $p(s',r,s,a)$
            \item $\mathcal{R}$ is a reward function $r(s,a)$, which is a function that maps a state $s$ and an action $a$ to a reward $r(s,a)$
            \item $\gamma \in [0,1]$ is the discount factor
        \end{itemize}
    \end{definition}

    \subsubsection{Stochastic Policy}\label{subsubsec:policy}
    A policy, denoted by $\pi$, in the context of MDPs, is a function that maps states to actions. It represents the agent's strategy or decision-making rule for selecting actions based on the current state.
    \begin{definition}
        A policy is defined as: $\pi: S \rightarrow A$, where $S$ is the set of states and $A$ is the set of actions. Notation for a policy is as follows:
        \begin{equation}
            \pi(a|s)=\mathbb{P}(A_t=a|S_t=s)
        \end{equation}
    \end{definition}
    Policy refers to the specification of an Agent's actions based on the current state in a Markov Decision Process. The policy can be deterministic, meaning it selects a single action for each state. It is represented as a function $\pi_D : N \rightarrow A$, where $\pi_D(s)$ represents the action to be taken in state $s$. Or it can be stochastic, meaning it selects actions probabilistically based on some probability distribution over actions for each state. A policy is a function that maps states to actions and represents the agent's decision-making strategy. Mathematically, a Policy is represented as a function $\pi : N \times A \rightarrow [0, 1]$, where $N$ represents the state space and $A$ represents the action space. The function $\pi(s, a)$ represents the probability of taking action $a$ in state $s$ at time step $t = 0, 1, 2, \ldots$, for all $s \in N$ and $a \in A$. It is assumed that the sum of probabilities for all actions in a given state is equal to 1. A Policy is usually assumed to be Markovian, meaning that the action probabilities depend only on the current state and not the history. It is also assumed to be stationary, meaning that the action probabilities do not change over time. However, if the policy needs to depend on the time step $t$, we can include $t$ as part of the state, which would make the policy stationary but may increase computational cost due to the enlarged state space. In the more general case, where states or rewards are uncountable, the same concepts apply except that the mathematical formalism needs to be more detailed and more careful. Specifically, we'd end up with integrals instead of summations, and probability density functions (for continuous probability distributions) instead of probability mass functions (for discrete probability distributions). For ease of notation and more importantly, for ease of understanding of the core concepts (without being distracted by heavy mathematical formalism), we've chosen to stay with discrete-time, countable $S$, countable $A$~\cite{rao2022foundations}.

    Policies and action-value functions are closely related and are used interchangeably in many reinforcement learning algorithms, but they are conceptually distinct.

    \subsubsection{State-value Function for Stochastic Policy $\pi$}
    The value function $V^{\pi}(s)$ for a stochastic policy $\pi$ is the expected cumulative discounted reward the agent can obtain from state $s$ by following policy $\pi$ and then continuing to follow $\pi$ thereafter. It can be computed recursively using the Bellman equation, which relates the Value Function of a state to the rewards and transitions of the MDP.
    It is defined as:
    \begin{equation}
        \label{eq:Vpi}
        \begin{split}
            \textcolor[RGB]{51,204,51}{v^{\pi}(s)} &\doteq \mathbb{e}_{\pi}\left[ g_t|S_t=s \right] \\
            &= \mathbb{e}\left[ r_{t+1} = \gamma g_{t+1} | S_t=s \right] \\
            &= \mathbb{e}_{\pi}\left[ \sum_{k=0}^{\infty}\gamma^k r_{t+k+1}|S_t=s \right] \\
            &= \sum_{a} \pi(a|s) \sum_{s', r} p(r, s'|s, a) \left[r + \gamma \textcolor[RGB]{51,204,51}{v_{\pi}(s')}  \right]\text{, for all } s \in \mathcal{S}
        \end{split}
    \end{equation}
    where $\mathbb{E}_{\pi}$ denotes the expectation with respect to the states and rewards generated by following policy $\pi$.

    \subsubsection{Action-value Function}

    Action-Value Function $q^\pi(s, a)$, which maps a (state, action) pair to the expected reward originating from that pair when evaluated with policy $\pi$. Mathematically, an action-value function is defined as $Q: S \times A \rightarrow \mathbb{R}$, where $S$ is the set of states and $A$ is the set of actions. The Action-value function $a^{\pi}(s,a)$ represents the expected cumulative reward from taking action $a$ in state $s$, following the policy $\pi$ thereafter. In other words, it quantifies the value of selecting a particular action in a particular state under a specific policy. The Action-Value Function is denoted by $q^\pi$ and is crucial in developing various Dynamic Programming and Reinforcement Learning algorithms for the MDP Prediction problem.

    \begin{definition}
        The action-value function $Q^{\pi}(s, a)$ for a policy $\pi$ is the expected cumulative discounted reward the agent can obtain from state $s$, taking action $a$, and then following policy $\pi$ thereafter. It is defined as:
        \[
            q^{\pi}(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s, A_0 = a\right]
        \]
    \end{definition}

    To avoid confusion, $v^\pi$ is referred to as the State-Value Function, while $q^\pi$ is referred to as the Action-Value Function. The Action-Value Function provides information about the expected returns from specific state-action pairs and is useful for making decisions about which actions to take in an MDP. The relationship between the state-value function and the action-value function can be defined as:
    \[ v_\pi(s) = \max_{a\in\mathcal{A}} q_\pi(s, a) \]

    \subsubsection{Optimal State-value function}
    The goal of an agent is to maximize the reward, which means finding a policy that yields the highest possible return. To compare two policies, let $\pi$ and $\pi'$ be two different stochastic policies. Then $\pi'$ is considered better or equal to $\pi$ if $v_{\pi'}(s) \geq v_{\pi}(s)$ holds for all states $s$. It can be shown that there is always at least one policy that is better or equal to all other policies, and this policy is called the optimal policy $\pi^*$. If an agent uses the optimal policy, then for all states and actions the agent will use the optimal state-value function and the optimal action-value function:
    \begin{definition}
        The Optimal State-value function $v^*(s)$ is the maximum value function over all possible policies and is defined as:
        \begin{equation}
            \begin{split}
                v^*(s) = \max_{\pi} v^{\pi}(s) \\
            \end{split}
        \end{equation}
    \end{definition}

    \subsubsection{Optimal Action-value function}

    Conversely, if the optimal action-value function is found, the optimal policy can be derived.
    \begin{definition}
        The Optimal Action-value function $q^*(s, a)$ is the maximum action-value function over all possible policies. It is defined as:
        \begin{equation}
            \begin{split}
                q^*(s, a) &= \max_{\pi} q^{\pi}(s, a) \\
            \end{split}
        \end{equation}
    \end{definition}

    The optimal policy function $\pi^*(a | s)$ always selects an action $a$ with a selection probability of 1 for which $q_*(s, a)$ is maximal for a given state $s$. It also follows that the optimal policy is deterministic, meaning for the same state, the optimal policy function always selects exactly the same action. The transition from stochastic to deterministic policy is explained by the \emph{greedy selection}\footnote{Always choose action with the highest expexted reward.} of actions in the last equation.

    \subsubsection{Bellman's Optimality Equation}\label{subsubsec:bellman-optimality-equation}
    The optimal state-value function $v^*(s)$ must satisfy Bellman's equation, which provides an recursive rule for determining the long-term expected return of each state. In order to incorporate actions, the State-value function for a Markov Reward Process (MRP) is extended to include actions as the second parameter, leading to Bellman's optimality equation for $v^*(s)$:
    \begin{equation}
        \begin{split}
            \textcolor[RGB]{51,204,51}{v^*(s)} &= \max_{a\in\mathcal{A}} \in \mathcal{A} \mathbb{E}[R_{t+1} + \gamma \textcolor[RGB]{51,204,51}{v^*(S_{t+1})} | S_t = s, A_t = a] \\
            &= \max_{a\in\mathcal{A}} \sum_{s', r} p(r, s'|s, a) \left[r + \gamma \textcolor[RGB]{51,204,51}{v^*(s')}  \right]
        \end{split}
    \end{equation}

    This equation derives the recursive relation to the subsequent state, allowing the reference to the optimal policy to be omitted. Similarly, the Action-value function $q^*(s, a)$ can be derived as~\cite{rao2022foundations}:
    \begin{equation}
        \begin{split}
            \textcolor[RGB]{51,204,51}{q^*(s, a)} &= \mathbb{E}[R_{t+1} + \gamma \max_{a'\in\mathcal{A}} \textcolor[RGB]{51,204,51}{q^*(S_{t+1}, a')} | S_t = s, A_t = a] \\
            &= \sum_{s', r} p(r, s'|s, a) \left[r + \gamma \max_{a'\in\mathcal{A}} \in \mathcal{A} \textcolor[RGB]{51,204,51}{q^*(s', a')}  \right]
        \end{split}
    \end{equation}


    The Bellman Optimality equation can be used to recursively calculate value functions by updating the value of each state based on the immediate reward and the long-term expected reward of the next state~\cite{rao2022foundations}.

    Now that we have a solid understanding of Markov Decision Processes (MDP), which form the foundation for Reinforcement Learning (RL), we can delve into the different learning methods used in RL.


    \section{Value-based learning}\label{sec:value-based-learning}
    This section presents value-based algorithms that iteratively compute state-value or action-value functions of states and actions. These functions are then used to derive continuously improving policies. Therefore, these algorithms are referred to as value-based algorithms. First, we need to mention some concepts: $Prediction==Valuation$ and $Improvement==Control$, these two concepts will be used later in this section. However, we will try to stick to the former, as it is more common in the literature and makes a bit more sense based on the context of the topic.

    \subsection{Dynamic Programming}\label{subsec:dynamic-programming}
    The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies in Markov decision processes (MDPs) when a perfect model of the environment is available. Classical DP algorithms are limited in reinforcement learning due to their assumptions of a perfect model and computational expense. DP uses value functions to organize the search for good policies, and the Bellman optimality equations define optimal value functions (\emph{State-value function} and \emph{Action-value function}). DP algorithms are effective for problems with finite state, action, and reward sets. However, for continuous problems, exact solutions are limited. One approach is to approximate solutions by discretizing the state and action spaces, and applying finite-state DP methods. Second approach is to use function approximation to approximate the value functions. The latter approach is the focus of this thesis.

    DP planning algorithms consists of the evaluation of a given policy, called policy prediction, defined in \cref{par:policy-prediction} and the subsequent improvement of the policy, called policy improvement, defined in \cref{par:policy-improvement}. The goal of these two steps is to infer $v^*(s)$ or $q^*(s,a)$ for all the states and actions in a procedure called Generalized Policy Iteration, defined in \cref{subsubsec:generalized-policy-iteration}~\cite{sutton2018reinforcement, FITMT25127}.

    \subsubsection{Policy Iteration}\label{subsubsec:policy-iteration}
    Policy iteration consist of 2 steps: \emph{policy evaluation} and \emph{policy improvement}. The policy prediction step, defined in \cref{par:policy-prediction}, is used to estimate the value function $v_{\pi}$ for a given policy $\pi$. The policy improvement step, defined in \cref{par:policy-improvement} is used to improve the policy $\pi$ by selecting the action that maximizes the value function $v_{\pi}$~\cite{sutton2018reinforcement}.

    \paragraph{Policy Prediction}\label{par:policy-prediction}
    Policy evaluation, or prediction, is the process of computing the state-value function for an arbitrary policy. The state-value function, denoted as $v^\pi(s)$, represents the expected sum of discounted rewards from a given state $s$ when following policy $\pi$. If the dynamics of the environment are completely known, the policy evaluation can be formulated as a system of simultaneous linear equations, which can be solved iteratively using update rules based on the Bellman equation for $v^\pi$. This process involves finding successive approximations of the value function, starting from an initial approximation, and updating it according to the Bellman equation until convergence is achieved to $v_k(s) \approx v_{\pi}(s)$ with:
    \[
        v_{k+1}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) v_k(s') \right)
    \]
    for all states. This equation is the analogous equation to Bellman's optimality equation for $v^*(s)$ in iteration form. Once $v_{\pi}(s)$ is found for a given policy the action-value function can be determined~\cite{sutton2018reinforcement, FITMT25127}:
    \[
        q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) v_{\pi}(s')
    \]

    \paragraph{Policy Improvement}\label{par:policy-improvement}
    This equation is then the Bellman's equation for $q_{\pi}(s,a)$ and can be used in the Policy Improvement step to create a new policy. It should also be ensured that the new, deterministic policy is better or equal to the old policy, when for all states applies:
    \[
        q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)
    \]
    This means that choosing action $a' = \pi'(s)$ in state $s$ under policy $\pi$ produces a better result than action $a = \pi(s)$. Next, $q_{\pi'}(s, a)$ is computed for the new policy $\pi'$ in the Policy Evaluation step and improved with $\pi'(s) = \arg\max_a q_{\pi'}(s, a)$~\cite{sutton2018reinforcement, FITMT25127}.

    \subsubsection{Value Iteration}\label{subsubsec:value-iteration}
    Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates. Because the $\max$ operation in the update equation \cref{def:value-iteration} is the only difference between these updates, this just means that the $\max$ operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs. The algorithm pseudocode is presented in \cref{alg:value-iteration}~\cite{sutton2018reinforcement}.

    \begin{definition}
        \label{def:value-iteration}
        \textbf{Value Iteration Equation} is that iteratively computes the state-value function $V_k(s)$ for a given policy $\pi$ using the Bellman equation:
        \[
            V_{k+1}(s) \doteq \max_a \mathbb{E}\left[ R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a \right]
            = \max_a \sum_{s', r} p(s', r|s, a) [r + V_k(s')]
        \]
    \end{definition}

    \subsubsection{Generalized Policy Iteration}\label{subsubsec:generalized-policy-iteration}
    If Policy Evaluation and Policy Improvement steps are executed enough times and no more improvement of the old policy over the new policy is found, then optimal $q^*$ and $\pi^*$ are found. Well-known algorithms in DP which implement this Generalized Policy Iteration are the Policy Iteration algorithm and its special case the Value Iteration algorithm. They use the equations Bellman's equations for $v^*(s)$ and $q^*(s,a)$ and a variant of the Generalized Policy Iteration method, where $v_{\pi}(s)$ does not have to be determined. The iteration in the Policy Evaluation step is immediately followed by the Policy Improvement step~\cite{sutton2018reinforcement, 7110602}.

    \begin{figure}[h!]
        \centering
        \begin{minipage}[b]{0.5\textwidth}
            \includegraphics[width=\textwidth]{image/gpi2}
            \caption{Generalized Policy Iteration convergence.}
            \label{fig:gpi}
        \end{minipage}
    \end{figure}

    \subsection{Monte Carlo Methods}\label{subsec:monte-carlo-methods}
    In this chapter, we explore Monte Carlo methods for estimating value functions in the context of reinforcement learning (RL) without assuming complete knowledge of the environment (\emph{transition probabilities} and \emph{reward function}). Monte Carlo methods use sample sequences of states, actions, and rewards from actual or simulated interactions with the environment for learning. These methods can be used to solve the RL problem for episodic tasks, where experience is divided into episodes and value estimates and policies are updated only on the completion of an episode.

    \begin{wrapfigure}{r}{0.35\textwidth}
        \begin{center}
            \includegraphics[width=0.34\textwidth]{image/mc-evaluation-improvement}
        \end{center}
        \caption{MC Policy Evaluation and Improvement.}
    \end{wrapfigure}

    MC methods allow an agent to infer the optimal policy $\pi^*$ from the optimal action-value function $q^*(s,a)$ estimated through experience. MC methods use the iterative procedure Generalized Policy Iteration to incrementally infer $\pi^*$. First, a finite trajectory $(S_0, A_0, R_1, S_1, A_1, ..., R_n, S_n)$ is generated, where actions are selected according to a stochastic policy $\pi(a|s)$ and states and rewards come from the unknown environmental dynamics. From this episode, the return $G_t$ is computed for all state-action pairs reached. Then, the policy evaluation step is performed using the update equation:
    \begin{equation}
        \label{eq:mc}
        q_{k+1}(s, a) = q_k(s, a) + \alpha(G_t - q_k(s, a))
    \end{equation}
    where $\alpha \in (0,1)$ is a learning rate and $G_t$ is the sum of discounted rewards starting from action $a$ in state $s$. The term $G_t - q_k(s, a)$ corrects the value of $q_{k+1}(s,a)$ in the direction of the target $G_t$. The index $k$ represents the current episode, and in the limiting case, $q_k(s,a) = q_{\pi}(s,a)$, where $\pi$ is the optimal policy~\cite{sutton2018reinforcement}.
    \begin{equation}
        \pi_0 \xrightarrow{E} q_{\pi_0} \xrightarrow{I}
        \pi_1 \xrightarrow{E} q_{\pi_1} \xrightarrow{I}
        \pi_2 \xrightarrow{E} q_{\pi_2} \xrightarrow{I}
        \cdot
        \pi_* \xrightarrow{E} q_{\pi_*}
    \end{equation}

%    \begin{wrapfigure}[h!]
%        \includegraphics[width=\textwidth]
%        \caption{Monte Carlo convergence.}
%        \label{fig:mc}
%    \end{wrapfigure}


    Since MC methods do not require knowledge of the environment dynamics, a non-deterministic policy should be used for episode generation to ensure exploration of all states and actions. A stochastic policy, such as the $\epsilon$-greedy approach, is commonly used in MC methods for exploration.

    \paragraph{Epsilon-Greedy Policy Exploration}
    The \textbf{$\epsilon$-greedy} approach for exploration selects actions randomly according to:
    \begin{equation}
        \pi'(a | S) =
        \begin{cases}
            1 - \epsilon + \frac{\epsilon}{|\mathcal{A}|} & \text{if } a = A^* \\
            \frac{\epsilon}{|\mathcal{A}|} & \text{if } a \neq A^* \text{ for } a \in \mathcal{A}
        \end{cases}
    \end{equation}
    where the greedy action is the one with the highest estimated action-value according to the current policy. The process of adjusting action-value values in the policy evaluation step and determining a new policy in the policy improvement step is referred to as training the RL agent. The \cref{eq:mc} shows that MC methods use the return $G_t$ of an episode, which can only be calculated when the episode is terminated. Therefore, MC methods can only be used in environments with finite, i.e., always terminating episodes~\cite{FITMT25127, rao2022foundations, sutton2018reinforcement}

    \subsection{Off Policy Monte Carlo Control}\label{subsec:off-policy-monte-carlo-control}

    \subsection{Temporal-Difference Methods}\label{subsec:temporal-difference-methods}
    Temporal Difference (TD) methods, unlike Monte Carlo (MC) methods, can also be used in environments with infinite episodes. The mechanism that makes this possible is called bootstrapping, which determines $v_{k+1}(s)$ or $q_{k+1}(s,a)$ from the immediate reward plus $v_k(s')$ or $q_k(s',a')$, respectively, and is already applied in a similar way in Dynamic Programming (DP) through Bellman's equations. Unlike algorithms of DP, however, TD methods do not depend on the environment dynamics $\mathcal{P}$ and $R$. Like DP and MC methods, TD methods also use the iterative Generalized Policy Iteration procedure to determine the optimal policy. The policy evaluation step differs in TD methods from MC methods, but the policy improvement step is the same for both with the $\epsilon$-greedy approach. The difference between TD methods and MC methods in the evaluation step can be most easily illustrated by the iterative calculation of the state-value function of a policy with the following equation:
    \[
        v_{t+1}(s) = v_t(s) + \alpha(R_{t+1} + \gamma v_t(s') - v_t(s))
    \]
    where the term $(R_{t+1} + \gamma v_t(s') - v_t(s))$ is called TD-error and the term $G(1) = R_{t+1} + \gamma v_t(s')$ is called TD-target. This equation illustrates the difference between TD and MC methods. TD methods only have to wait until the next time-step before adjusting $v_{t+1}(s)$, whereas MC methods usually have to wait until the end of the episode for the computation of $G_t$. Therefore, the index $t$ now corresponds to the time-step. More generally, the TD target can be expressed as the $n$-step return, formulated as:
    \[
        G(n) = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n v(S_{t+n} = s(n))
    \]
    where for the limiting case of $n$ nearing infinity, the TD methods become MC methods. TD methods that use $G(1)$ are called TD(0). There are methods where the specific number of steps $n$ in $G(n)$ does not have to be specified. These methods are called TD($\lambda$), which we will not discuss in detail here. Later, when we discuss the Proximal Policy Optimization (PPO) algorithm, this concept will be used for the definition of Generalized Advantage Estimation (GAE), one of the major aspects of PPO. We can mention that the concept is known as exponentially weighted average.

    \subsection{Sarsa On-Policy TD Control}\label{subsec:sarsa-on-policy-td-control}
    The TD methods use the action-value function for the evaluation step and iteratively determine $q_\pi(s,a)$. A TD method that implements this practically is called SARSA and adjusts the action-value function in the evaluation step for a state-action pair at each time-step of the episode as

    \[
        q_{t+1}(s, a) = q_t(s, a) + \alpha \left( R_{t+1} + \gamma q_t(s', a') - q_t(s, a) \right) \quad (2.21)
    \]

    The TD methods, such as SARSA, utilize the action-value function for evaluating and updating the action-value estimates. SARSA adjusts the action-value function at each time-step of the episode based on the sequence of transitions \( (s, a, r, s', a') \) generated by the environment dynamics and the current epsilon-greedy policy. The updated action-value function is used to determine the new policy at the next time-step. This process is repeated iteratively until optimal action-value and policy functions are found. SARSA is an on-policy algorithm, as it evaluates and improves the same policy used for selecting actions.

    \[
        q_{t+1}(s, a) = q_t(s, a) + \alpha \left( R_{t+1} + \gamma \max_{a^*} q_t(s', a^*) - q_t(s, a) \right)
    \]

    On the other hand, off-policy algorithms, like Q-learning, evaluate and improve one policy while using a different policy for selecting actions. This can be useful, for example, when introducing a new version of an agent with a different policy to learn from an old agent with a well-performing policy. In Q-learning, the action maximizing action (\( a^* \)) of all action-value functions at a fixed state \( s' \) is always chosen when updating the action-value function for a given state-action pair \( (s, a) \), regardless of which action \( a^* \) of the policy was chosen. After updating the action-value function, the policy improvement step is performed using the epsilon-greedy approach with respect to the current policy (\( \pi \)) and optionally the old policy (\( \mu \)) as well.

    \subsection{Function Approximation}\label{subsec:function-approximation}
    TODO


    \section{Policy-based learning}\label{sec:policy-based}
    TODO

    \subsection{Stochastic Policy Gradient Methods}\label{subsec:stochastic-policy-gradient-methods}
    TODO

    \subsection{Monte Carlo Policy Gradient Methods}\label{subsec:monte-carlo-policy-gradient-methods}
    TODO

    \subsection{Actor-Critic Policy Gradient Methods}\label{subsec:actor-critic-policy-gradient-methods}
    TODO

    \subsection{Trust Region Policy Methods}\label{subsec:trust-region-policy-methods}
    TODO


    \section{Actor-Critic}\label{sec:actor-critic}
    TODO


    \section{Model-based}\label{sec:model-based}
    As mentioned in Section~\cref{sec:rl-introduction}, RL algorithms can be broadly categorized into two main types: model-free and model-based. Model-based methods rely on \emph{planning} as they primary component, while model-free methods rely on \emph{learning}~\cite{sutton2018reinforcement}. Despite their distinctions, both approaches involve computing value functions and using them to update approximate value functions based on future events.

    One popular approach is to use a probabilistic model of the environment, which can be represented as a transition function $P(s_{t+1}|s_t, a_t)$ that describes the probability of transitioning from state $s_t$ to state $s_{t+1}$ when taking action $a_t$. Some popular probabilistic model-based RL methods include Monte Carlo Tree Search, Neural Network Dynamics, and Probabilistic Ensembles with Trajectory Samplin.

    Another approach in model-based RL is to use a learned deterministic model of the environment, which can be represented as a function $f(s_t, a_t)$ that directly maps states and actions to next states. Deterministic model-based RL methods, such as World Models, learn an encoder to represent states, a recurrent neural network (RNN) to model dynamics, and a decoder to generate predicted next states.

    \subsection{Planning}\label{subsec:planning}
    A model of the environment refers to anything that an agent can use to predict the outcome of its actions. It can be either stochastic, where there are multiple possible outcomes with associated probabilities, or deterministic, where the outcome is fixed. Distribution models provide a description of all possible outcomes and their probabilities, while sample models generate a single outcome sampled from the probabilities.

    Planning, in the context of artificial intelligence, refers to a computational process that takes a model of the environment as input and produces or improves a policy for interacting with the environment. There are two main approaches to planning: \emph{state-space} planning and \emph{plan-space} planning. In \emph{state-space} planning, the focus is on searching through the \textbf{space of states} to find an optimal policy or path to a goal. Value functions are computed over states to guide the search. On the other hand, in \emph{plan-space} planning, the search is conducted through the \textbf{space of plans}, where operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space methods are challenging to efficiently apply to the stochastic sequential decision problems that are the primary focus in reinforcement learning~\cite{sutton2018reinforcement}.

    The common structure in \emph{state-space} planning and learning methods, as presented in this chapter, is that value functions are computed as intermediate steps using simulated experience. Value functions are used to estimate the expected future rewards of different actions or states, and they play a key role in improving the policy of an agent. Simulated experience is generated by using the model to simulate the environment, and the agent can update its value functions based on this simulated experience to guide its decision-making process.

    \begin{center}
        \begin{tikzpicture}[auto]
% Nodes
            \node[] (s) {model};
            \node[right=1cm of s] (a) {simulated experience};
            \node[right=2cm of a] (r) {values};
            \node[right=1cm of r] (s_p) {policy};

% Arrows
            \draw[thick, ->] (s) -- (a);
            \draw[thick, ->] (a) -- (r) node[midway, anchor=south] {backups};
            \draw[thick, ->] (r) -- (s_p);
        \end{tikzpicture}
    \end{center}

    Dynamic programming methods fit the structure of making sweeps through the space of states, generating possible transitions, computing backed-up values, and updating state estimates. Other state-space planning methods also fit this structure, with differences in updates, order, and retention of backed-up information. Planning methods are related to learning methods in estimating value functions through backing-up operations. Learning methods use real experience from the environment, while planning uses simulated experience from a model. Ideas and algorithms can be transferred between planning and learning. Planning in small, incremental steps allows for efficient interruption and redirection, which is beneficial for intermixing planning with acting and learning of the model. Planning in small steps may be the most efficient approach for large planning problems~\cite{sutton2018reinforcement}.
    \begin{algorithm}[H]
        \label{alg:random-sample-one-step-tabular-q-planning}
        \SetKwInOut{Input}{Input}
        \SetKwInOut{Output}{Output}
        \SetAlgoLined
        \Input{
            $S$: Set of states \\
            $A$: Set of actions \\
            $\alpha$: Learning rate \\
            $\gamma$: Discount factor \\
        }
        \While{True}{
            Select a state $s \in S$, action $a \in A(s)$ at random\;
            Send $s, a$ to a sample model, and obtain a sample next reward $r$ and a sample next state $s'$\;
            Apply one-step tabular Q-learning update to $s, a, r, s'$:
            \[
                Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left( r + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a) \right)
            \]
        }
        \caption{Random-sample one-step tabular Q-planning}
    \end{algorithm}

    \subsection{Trajectory Sampling}\label{subsec:trajectory-sampling}

    \subsection{Summary}\label{subsec:summary}

    Model-based RL offers several advantages over model-free methods. One major advantage is sample efficiency, as the agent can use the learned model of the environment to plan and generate simulated trajectories for learning, reducing the need for costly real-world interactions. Additionally, model-based RL can enable the agent to handle complex, high-dimensional state spaces and long-horizon tasks more effectively.

    However, model-based RL also faces challenges. One challenge is the accuracy of the learned model, as any errors in the model can lead to suboptimal policies. Another challenge is the computational cost of planning and decision-making using the learned model, as it requires additional computation compared to direct action selection in model-free methods.

    In conclusion, model-based RL is a promising approach that can offer sample-efficient learning and improved performance in complex environments. Various methods, such as probabilistic models and deterministic models, have been proposed in the literature. Despite some challenges, model-based RL continues to be an active area of research in machine learning.


    \section{Dyna Architecture}\label{sec:dyna-architecture}
    TODO


    \section{Exploration vs. Exploitation}\label{sec:exploration-vs-exploitation}

    \subsection{Greedy}\label{subsec:greedy}

    \subsection{Epsilon-Greedy}\label{subsec:epsilon-greedy}

    \subsection{Upper Confidence Bound}\label{subsec:upper-confidence-bound}


%    \section{Deep Reinforcement Learning}\label{sec:deep-reinforcement-learning}
%    TODO

\end{document}
