\documentclass[../xlapes02]{subfiles}
\begin{document}
    \chapter{Reinforcement Learning}\label{ch:reinforcement-learning}

    The motivation for this chapter comes from the influential book on reinforcement learning by Richard Sutton and Andrew Barto and Foundations of Reinforcement Learning with Application in Finance by Ashwin Rao and Tikhon Jelvis~\cite{rao2022foundations, sutton2018reinforcement}.

    This chapter explore the application of Reinforcement Learning techniques in the context of portfolio allocation. In \cref{sec:markov-models-for-rl} we will provide an introduction to Markov models, first explore \emph{Markov Processes}, \emph{Markov Reward Processes} and \emph{Markov Decision Processes}, which are the fundamental building blocks of Reinforcement Learning. Afterward, we divide Reinfocement Learning into two main categories. In \cref{sec:model-based-methods}, we will discuss \emph{Model-based} Reinforcement Learning methods, and in \cref{sec:model-free-methods} we will discuss \emph{Model-free} Reinforcement Learning methods in details with advantages and disadvantages of each approach. Let start with introduction to AI in general and its different types of learning:

    \paragraph{Supervised Learning}
    In supervised learning (SL), a model is trained on a labeled dataset, where the input data is paired with corresponding output labels. The model learns to make predictions based on the labeled examples, and the goal is to minimize the error between predicted outputs and actual labels. Common applications of supervised learning include image classification, speech recognition, and sentiment analysis.

    \paragraph{Semi-supervised Learning}
    Semi-supervised learning (SSL) is a combination of supervised and unsupervised learning. It uses a small labeled dataset along with a large unlabeled dataset for training. The model leverages the limited labeled examples to learn patterns from the unlabeled data, and then makes predictions on unseen data. SSL is useful when obtaining labeled data is expensive or time-consuming. It is often used in scenarios where obtaining a large labeled dataset is challenging, such as in medical diagnosis or fraud detection.

    \paragraph{Unsupervised Learning}
    In unsupervised learning (UL), the model learns from unlabeled data without any predefined output labels. The goal is to find underlying patterns, structures, or relationships within the data. Common unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection. UL is used in scenarios where labeled data is scarce or not available, and the model needs to discover patterns autonomously from the data.

    The last type is Reinforcement Learning and this entire chapter will be devoted to it, let's dive into it in more detail.


    \section{Introduction}\label{sec:rl-introduction}
    Reinforcement learning (RL) is an exciting field at the intersection of artificial intelligence (AI) and machine learning (ML) that deals with training agents to make optimal decisions in dynamic environments. RL is inspired by the way humans learn from experience, like \texttt{trial-and-error}, and an agent interacts with an environment the same way and receives feedback in the form of \emph{rewards} (typically positive number, e.g.: $1$) or \emph{penalties} (typically negative number, e.g.: $-1$), and uses this feedback to learn and improve its decision-making abilities.

    At the heart of RL lies the concept of an agent, which that takes actions in an environment to achieve specific goals. The environment is typically modeled as a Markov decision process (MDP), which is a mathematical framework that describes how an agent interacts with an environment in discrete time steps. An MDP is defined by a tuple $(S, A, P, R)$, where $S$ is the state space, $A$ is the action space, $P$ is the transition probability function, and $R\in\mathbb{R}$ is the reward.

    The goal of an RL agent is to learn a policy, denoted by $\pi$, which is a mapping from states to actions that maximizes the cumulative reward $G_t$ over time $T$. The agent uses this policy to select actions at each time step, and the environment responds with a new state and a reward. The agent then updates its policy based on the observed rewards and states, aiming to improve its decision-making abilities and achieve higher rewards in the long run. RL does not require labeled data, because learns from observations and rewards via interaction with the environment.

%    RL algorithms can be broadly categorized into two main types: model-free and model-based. Model-free algorithms are then further divided into value-based and policy-based methods, such as Q-learning (value-based) and policy gradient methods (policy-based), learn directly from the interactions with the environment without explicitly modeling the transition probabilities and rewards. Model-based algorithms, on the other hand, learn a model of the environment and use it to make decisions. These algorithms have their strengths and weaknesses, and the choice between them depends on the specific problem and requirements.

    The sequence of states, actions and rewards that the agent experiences is called a trajectory, and it look like this:
    \begin{equation}
        \label{eq:trajectory}
        (S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \dots, S_{T-1}, A_{T-1}, R_T, S_T)
    \end{equation}

    This sequence of \emph{state-action-reward} can be finite or infinite, depending on the environment and the agent's goal. Pretty good example of this is the game of chess, where the game ends when one of the players wins or the game is a draw. In this case, the trajectory is finite, and the agent's goal is to maximize the cumulative reward over time $T$. On the other hand, the self-driving car example is an infinite-horizon problem, where the agent's goal is to maximize the cumulative reward over an infinite time horizon or until the car reaches its destination~\cite{FITMT25127}.

    \begin{figure}[h]
        \includegraphics[width=0.7\linewidth]{image/agent-environment}
        \centering
        \caption{The agent interacts with the environment and learns to maximize the cumulative reward over time $T$.}
        \label{fig:rl-introduction}
    \end{figure}

    Because RL agent is categorized by the way it learns, there are many different types of RL agents. We will try to differentiate them in this chapter, because it is important to understand teh differences between them, to understand the behavior of the agent and to choose the right algorithm for the problem. Based on the different computable approaches we can categorize RL agents:
    \begin{multicols}{2}
        \begin{itemize}
            \item \textbf{Value-based}
            \begin{itemize}
                \item \textcolor{darkgray}{No Policy (implicit)}
                \item Value function
            \end{itemize}
            \item \textbf{Policy-based}
            \begin{itemize}
                \item Policy
                \item \textcolor{darkgray}{No Value function}
            \end{itemize}
            \item \textbf{Actor-critic}
            \begin{itemize}
                \item Policy
                \item Value function
            \end{itemize}
        \end{itemize}
        \columnbreak
        \begin{itemize}
            \item \textbf{Model-based}
            \begin{itemize}
                \item Policy and/or Value function
                \item \textcolor{darkgray}{No model}
            \end{itemize}
            \item \textbf{Model-free}
            \begin{itemize}
                \item Policy and/or Value function
                \item Model
            \end{itemize}
        \end{itemize}
    \end{multicols}

    \emph{Model-based} algorithm are closely relate to \emph{Planning algorithms} which are trying to create a model of the environment and use it to make decisions, they have knowledge of the transition probabilities and rewards from the beginning, we will discuss them in \cref{sec:model-based-methods}. \emph{Value-based} algorithms can implicitly infer a policy, but do not explicitly compute it, we will discuss them in \cref{sec:value-based-methods}, and \emph{Policy-based} algorithms compute a policy function by themselves, and are discussed in \cref{sec:policy-based-methods}. Another agents like \emph{Actor-critic methods} compute a policy function and a state-value function at the same time~\cite{rl-course-david-silver, sutton2018reinforcement}.

    \begin{figure}[h]
        \includegraphics[width=0.5\linewidth]{image/model-value-policy}
        \centering
        \caption{The relationship between model, value function and policy. Source:~\cite{FITMT25127}}
        \label{fig:model-value-policy-introduction}
    \end{figure}


    \section{Markov Theory}\label{sec:markov-models-for-rl}
    This section provides introduction to Markov theory, which is the fundamental building blocks of Reinforcement Learning. We begin by \emph{Markov Process}, described in~\cref{subsec:markov-process}, which is a stochastic process that satisfies the Markov property. We then move on to \emph{Markov Reward Process}, described in~\cref{subsubsec:markov-reward-process}, which is a Markov process with a reward function. Finally, we describe the most important for RL \emph{Markov Decision Process}, described in~\cref{subsec:markov-decision-process}, which is a Markov Reward process with a decision-making ability.

    \subsection{Markov Process}\label{subsec:markov-process}
    The Markov process (MP) (also called Markov Chains) describes the states of an environment and models the dynamics of state transitions. In an MP, an agent can only observe the changing states of the environment and has no influence over them. Markov processes have two key properties. Firstly, state transitions are non-deterministic and influenced by randomness. States are modeled as realizations of random variables, defined in \cref{subsubsec:probability-functions}. Secondly, the future state is only dependent on the current state, and not on previous states, simplifying causality with the Markov property~\cite{FITMT25127}.

    \subsubsection{Probability Functions}\label{subsubsec:probability-functions}

    The first property of an MP states that each concrete state of an environment is the realization of a discrete random variable \(X\) from set \(V\), with a certain probability \(P(X=x)\), where \(x \in V\). A state is a realization of a random experiment that the environment assumes with a certain probability, and this can be represented as a probability function:
    \begin{equation}
        P(X = X(\omega)) = P(X = x)
    \end{equation}

    The repeated successive execution of a random experiment can be represented as a stochastic process, which is a sequence of random variables, e.g., \(X_t(\omega), X_{t+1}(\omega), \ldots X_n(\omega)\), where a single term can be shortened to \(X_t\) and it represents the state of the environment at time \(t\)~\cite{FITMT25127, rao2022foundations}.

    \subsubsection{Stochastic Process (Random Process)}\label{subsubsec:stochastic-process}
    A stochastic process is defined as a collection of random variables defined on a common probability space $(\Omega, \mathcal{F}, P)$, where $\Omega$ is a sample space, $\mathcal{F}$ is a $\sigma$-algebra, and $P$ is a probability measure; and the random variables, indexed by some set $T$, all take values in the same mathematical space $S$, which must be measurable with respect to some $\sigma$-algebra $\Sigma$.

    In other words, for a given probability space $(\Omega, \mathcal{F}, P)$ and a measurable space $(S, \Sigma)$, a stochastic process is a collection of $S$-valued random variables, which can be written as:~\cite{enwiki:1148510872}
    \[
        \{X(t) : t \in T\}.
    \]

    In stochastic processes the probability that the environment assumes a certain state depends on the realized states of previous random variables. For example, if the weather forecast is assumed to be a stochastic process, then the yesterday’s weather may still have an influence on tomorrow’s weather. To represent this causality complicates the modeling of stochastic processes, so that with definition of Markov property the dependence of future states is assumed only on the current state. This is the second important property of Markov process~\cite{FITMT25127}.

    \subsubsection{Markov Property}\label{subsubsec:markov-property}
    The Markov property, which is defined using conditional probability, states that ``The future is independent of the past, given the present."

    The stochastic process has the Markov property if and only if, for all time steps $t \in N$, the conditional probability of the next state given the current state is equal to the conditional probability of the next state given all the previous states $X_1, \ldots, X_t$. $\probP[X_{t+1}|X_t] = \probP[X_{t+1}|X_1,\ldots,X_t]$

    This property has several advantages in practical reinforcement learning, including the uniqueness and distinctiveness of states, as well as the ability to precisely formulate the probability of state transitions, defined as:~\cite{FITMT25127}
    \begin{equation}
        \mathcal{P}(x'|x)=\probP(X_{t+1}=x'|X_t=x)
    \end{equation}

    Given n possible states, $s\in S$, then the probability of transitioning from state $s$ to state $s'$ can be represented as a matrix $P$, and because probability summation rule, the sum of transition probabilities from state $s$ to any other state $s'$ must equal to 1.

    \begin{definition}
        The Markov process is stochastic process that satisfies the Markov property and is described as tuple $\left(\mathcal{S}, \mathcal{P}\right)$ for which holds:\cite{ABATE2021102207}
        \begin{itemize}
            \item $\mathcal{S} = s_1, s_2, ..., s_n$ is a finite set of states
            \item $\mathcal{P}$ is an $n\times n$ transition probability matrix which sums to 1 for each row, so each value $p_{ij}$ is the probability of transitioning from state $s_i$ to state $s_j$ in interval $\left< 0;1 \right>$
        \end{itemize}
    \end{definition}

    \begin{align*}
        \mathcal{P}=\begin{bmatrix}
                        p_{11} & p_{12} & p_{13} & \dots  & p_{1n} \\
                        p_{21} & p_{22} & p_{23} & \dots  & p_{2n} \\
                        \vdots & \vdots & \vdots & \ddots & \vdots \\
                        p_{n1} & p_{n2} & p_{n3} & \dots  & p_{nn}
        \end{bmatrix}
    \end{align*}

    \subsubsection{Starting States}
    The probability distribution of start states is denoted as $\mu : N \rightarrow [0,1]$ in order to perform simulations and compute the probability distribution of states at specific future time steps. A Markov Process is fully specified by the transition probability function $P$, which governs the complete dynamics of the process.
    \begin{itemize}
        \item Specification of the transition probability function $P$.
        \item Specification of the probability distribution of start states (denote this as $\mu : N \in[0, 1])$.
    \end{itemize}
    Given $\mu$ and $P$, we can generate sampling traces of the Markov Process and answer questions such as probability distribution of states at specific future time steps or expected time of first occurrence of a specific state, given a certain starting probability distribution $\mu$. The separation of concerns between $P$ and $\mu$ is key to the conceptualization of Markov Processes~\cite{rao2022foundations}.

    \subsubsection{Terminal States}
    Markov Processes can terminate at specific states (e.g., based on rules for winning or losing in games). Termination can occur after a variable number of time steps (episodic) or after a fixed number of time steps (as in many financial applications). If all sampling traces of the Markov Process reach a terminal state, they are called episodic sequences. The notion of episodic sequences is important in Reinforcement Learning. In some financial applications, the Markov Process terminates after a fixed number of time steps $T$, and states with time index $t = T$ are labeled as terminal states. States with time index $t < T$ transition to states with time index $t + 1$~\cite{rao2022foundations}.

    \begin{figure}[h!]
        \begin{center}
            \begin{tikzpicture}
                [->,>=stealth,shorten >=1pt,auto,node distance=3cm,
                thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

                % Nodes
                \node[main node] (A) {$S_0$}; % Starting state
                \node[main node] (B) [right of=A] {$S_1$};
                \node[main node] (C) [right of=B] {$S_2$};
                \node[main node] (T) [right of=C] {$S_3$}; % Terminal state

                % Edges
                \path[every node/.style={font=\sffamily\small}]
                (A) edge [bend left] node [above] {0.5} (B)
                (B) edge [bend left] node [below] {0.5} (A)
                (B) edge [bend left] node [above] {0.5} (C)
                (C) edge [bend left] node [below] {0.5} (B)
                (C) edge [bend left] node [above] {0.5} (T);
            \end{tikzpicture}
            \caption{Markov Process with Start state $S_0$ and Terminal state $S_3$, because there is no edge from $S_3$.}
            \label{fig:markov-process}
        \end{center}
        \centering
    \end{figure}

    The examples shown here only include states and transition probabilities. To fully define an environment within the framework of RL, actions and rewards also need to be defined.

    \subsection{Markov Reward Process}\label{subsubsec:markov-reward-process}
    Markov Reward Process (MRP) is a Markov Process with rewards. These rewards are random, and all we need to do is to specify the probability distributions of these rewards as we make state transitions. The main purpose of Markov Reward Processes is to calculate how much reward we would accumulate (in expectation, from each of the non-terminal states) if we let the process run indefinitely, bearing in mind that future rewards need to be discounted appropriately $\gamma$ (otherwise, the sum of rewards could blow up to $\infty$). In order to solve the problem of calculating expected accumulative rewards, defined in \cref{par:expected-reward}, from each non-terminal state, we will first set up some formalism for Markov Reward Processes and develop some theory on calculating rewards accumulation.

    The main objective of an RL agent is to maximize the sum of rewards from each time-step. The agent can observe different episodes in the Markov process, but lacks the means to determine the actual quality of an episode. By calculating the reward, we can precisely measure the goodness of an episode or even a single state using the \emph{state-value function}, defined in \cref{subsubsec:state-value-function}. This allows the agent to actively transition to favorable states and maximize the reward.

%    \begin{definition}
%        A Markov Reward Process is a Markov Process with a time-indexed sequence of Reward random variables $R_t \in D$ (a countable subset of $R$) for time steps $t = 1, 2, \cdot$, satisfying the Markov Property (including Rewards): $P[(Rt+1, St+1)|St, St−1, ..., S0] = P[(Rt+1, St+1)|St]$ for all $t ≥ 0$.
%    \end{definition}

%    \begin{definition}
%        \textbf{Markov Reward Process} is a Markov Process, along with a time-indexed sequence of Reward random variables $R_t \in D$ (a countable subset of $\mathbb{R}$) for time steps $t = 1, 2, \ldots$, satisfying the Markov Property (including Rewards): $P[(R_{t+1}, S_{t+1})|S_t, S_{t-1}, \ldots, S_0] = P[(R_{t+1}, S_{t+1})|S_t]$ for all $t \geq 0$.
%    \end{definition}

    \begin{definition}
        Markov Reward Processes is a tuple $(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$ for which holds:\cite{rao2022foundations}
        \begin{itemize}
            \item $\mathcal{S} = s_1, s_2, ..., s_n$ is a finite set of states
            \item $\mathcal{P}$ is an $n\times n$ transition probability matrix which sums to 1 for each row, so each value $p_{ij}$ is the probability of transitioning from state $s_i$ to state $s_j$ in interval $\left< 0;1 \right>$
            \item $\mathcal{R}$ is a sequence of random variables $R_1, R_2, ..., R_n$ where $R_t$ is a random variable that represents the reward for transitioning from state $s_t$ to state $s_{t+1}$
            \item $\gamma$ is a discount factor in interval $\left< 0;1 \right>$
        \end{itemize}
        \begin{equation}
            \mathcal{P}(s,r,s') = \probP\left[ R_{t+1} = r, S_{t+1} = s' | S_t = s \right] \text{ for time steps } t = 0, 1, 2, \ldots
        \end{equation}
        \begin{align*}
            \text{ for all } s \in N, r \in D, s' \in S, \text{ such that } \sum_{s' \in S} \sum_{r \in D} \mathcal{P}(s,r,s') = 1 \text{ for all } s \in N
        \end{align*}
    \end{definition}

    \subsubsection{Reward function}\label{par:reward-function}
    The reward function $\mathcal{R}(s)$ is a function that maps a state $s$ to a reward $r$ and specify how much reward and agent expects from the environment given current state $s$. If an agent is in a state $s$ at time $t$, the agent receives reward  $R_{t+1}$  at time $t + 1$,  when it transitions to a subsequent state $s'$. Rewards of an episode can be represented as a sequence $(R_1, R_2, \ldots, R_t)$~\cite{FITMT25127}.

    % TODO: Agent reference

    \paragraph{When to receive reward?} An RL agent, defined in \verb|\cref{TODO}|, should receive a reward for a good action and a penalty for a bad action. Good actions are those that lead to the agent's main goal, while bad actions are those that lead to a state that is not desirable for the agent. That is, an agent should not receive (one small) reward when it can then receive a large penalty, for example in chess by taking one piece, when it can then lose the game by getting checkmate~\cite{sutton2018reinforcement}.

    \subsubsection{Expected reward}\label{par:expected-reward}
    The expected reward, denoted as $G_t$ at time $t$ is the discounted sum of rewards in a single episode. Can be used to calculate the sum of discounted rewards in an episode.

    \begin{equation}
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    \end{equation}


    We can also calculate the expected rewards for state-action pairs as a two-argument function $r: S \times A \to R$.
    The expected reward of a state-action pair as two-argument function $r: \mathcal{S}\times \mathcal{A}\leftarrow \mathbb{R}$ is defined as:
    \begin{equation}
        r(s, a) = \sum_{t=0}^{\infty} \gamma^t R_{t+1} P(S_{t+1} = s|S_t = s, A_t = a)
    \end{equation}

    \paragraph{Discount factor $\gamma$}\label{par:discount-factor}
    The calculation also involves the discount factor $\gamma$ which is a value in the interval $\langle 0, 1 \rangle$. If $\gamma$ is equal to one, the series value goes to infinity. The agent can only calculate the reward in the case of always terminating episodes. If $\gamma$ is less than one, the reward has a finite value, allowing the agent to determine the quality of an episode. The discount factor is not only useful mathematically but also for tuning the agent's rewards. If early rewards in an episode are more significant than later ones, $\gamma$ should be close to zero. If the rewards represent monetary gains, then it is the case, as early rewards earn additional interest. On the other hand, the closer $\gamma$ is to one, the more important later rewards are.

    \subsubsection{State-value Function}\label{subsubsec:state-value-function}
    The state-value function provides information about the long-term expected reward for each state in an environment. With this information, an agent can determine which state to transition to in order to maximize the reward of an episode. Specifically, the agent should choose the state that has the highest long-term expected reward. If the agent observes a sequence of states and rewards, it can remember the subsequent rewards for each state, calculate the reward of an episode, and iteratively update the probabilities for higher occurrence of rewards over multiple episodes. As the number of episodes approaches infinity, the estimated probabilities converge to the true probabilities, and the long-term expected reward of a state can be accurately determined. Intuitively, the more episodes and consequent rewards an agent observes, the better it can estimate the value of each state. We will introduce the methods for determining the state-value function and the action-value function in the next subsection, along with a recursive iterative approach for calculating the state-value function based on Bellman's equation~\cite{FITMT25127}.

    Since episodes may start with different states due to state transitions probabilities, the expected reward of a particular state is the expected value of the conditional density function over the probabilities of rewards for that state. Thus, $G_t$ can be treated mathematically as a continuous random variable.
    To derive \emph{Bellman's equation}, defined later in this subsection, we need to make use of the definition of the \emph{(Recursive) State-value function}~\cite{FITMT25127, rao2022foundations}.
    \begin{equation}
        \begin{split}
            \textcolor[RGB]{51, 204, 51}{v(s)} &= E[G_t | S_t = s] \\
            &= E[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
            &= E[R_{t+1} + \gamma \textcolor[RGB]{51, 204, 51}{v(S_{t+1})} | S_t = s]\\
            &= \mathcal{R}(s) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}(s' | s) \textcolor[RGB]{51, 204, 51}{v(s')}
        \end{split}
    \end{equation}
    where $\gamma$ is the discount factor, $\mathcal{R}(s)$ is the immediate reward for state $s$, $\mathcal{P}(s' | s)$ is the transition probability from state $s$ to state $s'$, and $\mathcal{S}$ is the set of all possible states in the environment. The last equation expresses that the long-term expected reward of a state depends only on the immediate reward and the long-term expected reward of the subsequent states.

    In the case of Finite Markov Reward Processes, let's assume that the state space is denoted as $\mathcal{S} = \{s_1, s_2, ..., s_n\}$, and the subset of states of interest is denoted as $\mathcal{N}$ with $m \leq n$ states. We can use bold-face notation to represent functions as column vectors and matrices, since we are dealing with finite states/transitions. So, $V$ is a column vector of length $m$, $P$ is an $m \times m$ matrix, and $R$ is a column vector of length $m$ (with rows/columns corresponding to states in $\mathcal{N}$). We can express the equation in vector and matrix notation as follows:
    \begin{equation}
        \begin{split}
            V &= R + \gamma P \cdot V \\
            &= (I_m - \gamma P)^{-1} \cdot R
        \end{split}
    \end{equation}
    where $I_m$ is the identity matrix of size $m \times m$, and $\gamma$ is the discount factor~\cite{rao2022foundations}.

    By extending the equation to include actions, we arrive at the most important equation in all of reinforcement learning: the Bellman equation, defined in ~\cref{subsubsec:bellman-equation}. It states that to calculate the long-term expected reward from a state, an agent only needs to add together the reward of the current state and the long-term expected reward of the next state~\cite{FITMT25127}.

    \subsection{Markov Decision Process}\label{subsec:markov-decision-process}
    The Markov Decision Process (MDP) allows an agent to actively influence changes in the state of the environment through its actions. Within the MDP, the agent has the ability to jointly determine the subsequent state to which the environment should transition. The agent's primary goal is to strategically choose actions that maximize expected payoff. In the previous subsections, we addressed the aspect of \emph{sequential uncertainty (e.g., MRP)} using the Markov process framework and extended it to include the uncertain reward $R_t\in\mathcal{R}$ at each state transition $p(s',r,s)$, referred to as Markov reward processes. However, this framework lacks the notion of \emph{sequential decision making} and in this section this notion is introduced in terms of MDP, a generalization of MRP that includes the notion of \emph{sequential decision making}~\cite{rao2022foundations}.

    \begin{definition}
        Markov decision process (MDP) is Markov reward process with actions. It is an tuple $(S, A, \mathcal{P}, \mathcal{R}, \gamma)$, where:
        \begin{itemize}
            \item $\mathcal{S}$ is finite set of states $S$ (known as the State Space), a set $T \subseteq S$ (known as the set of Terminal States)
            \item $\mathcal{A}$ is finite set of actions $A$ (known as the Action Space)
            \item $\mathcal{P}$ is a transition probability function $p(s',r,s,a)$, which is a function that maps a state $s$, an action $a$, a next state $s'$ and a reward $r$ to a probability $p(s',r,s,a)$
            \item $\mathcal{R}$ is a reward function $r(s,a)$, which is a function that maps a state $s$ and an action $a$ to a reward $r(s,a)$
            \item $\gamma \in [0,1]$ is the discount factor
        \end{itemize}
    \end{definition}

    \subsubsection{Stochastic Policy}\label{subsubsec:policy}
    A policy, denoted by $\pi$, in the context of MDPs, is a function that maps states to actions. It represents the agent's strategy or decision-making rule for selecting actions based on the current state.
    \begin{definition}
        A policy is defined as: $\pi: S \rightarrow A$, where $S$ is the set of states and $A$ is the set of actions. Notation for a policy is as follows:
        \begin{equation}
            \pi(a|s)=\mathbb{P}(A_t=a|S_t=s)
        \end{equation}
    \end{definition}
    Policy refers to the specification of an Agent's actions based on the current state in a Markov Decision Process. The policy can be deterministic, meaning it selects a single action for each state. It is represented as a function $\pi_D : N \rightarrow A$, where $\pi_D(s)$ represents the action to be taken in state $s$. Or it can be stochastic, meaning it selects actions probabilistically based on some probability distribution over actions for each state. A policy is a function that maps states to actions and represents the agent's decision-making strategy. Mathematically, a Policy is represented as a function $\pi : N \times A \rightarrow [0, 1]$, where $N$ represents the state space and $A$ represents the action space. The function $\pi(s, a)$ represents the probability of taking action $a$ in state $s$ at time step $t = 0, 1, 2, \ldots$, for all $s \in N$ and $a \in A$. It is assumed that the sum of probabilities for all actions in a given state is equal to 1. A Policy is usually assumed to be Markovian, meaning that the action probabilities depend only on the current state and not the history. It is also assumed to be stationary, meaning that the action probabilities do not change over time. However, if the policy needs to depend on the time step $t$, we can include $t$ as part of the state, which would make the policy stationary but may increase computational cost due to the enlarged state space. In the more general case, where states or rewards are uncountable, the same concepts apply except that the mathematical formalism needs to be more detailed and more careful. Specifically, we'd end up with integrals instead of summations, and probability density functions (for continuous probability distributions) instead of probability mass functions (for discrete probability distributions). For ease of notation and more importantly, for ease of understanding of the core concepts (without being distracted by heavy mathematical formalism), we've chosen to stay with discrete-time, countable $S$, countable $A$~\cite{rao2022foundations}.

    Policies and action-value functions are closely related and are used interchangeably in many reinforcement learning algorithms, but they are conceptually distinct.

    \subsubsection{State-value Function for Stochastic Policy $\pi$}
    The value function $V^{\pi}(s)$ for a stochastic policy $\pi$ is the expected cumulative discounted reward the agent can obtain from state $s$ by following policy $\pi$ and then continuing to follow $\pi$ thereafter. It can be computed recursively using the Bellman equation, which relates the Value Function of a state to the rewards and transitions of the MDP.
    It is defined as:
    \begin{equation}
        \label{eq:Vpi}
        \begin{split}
            \textcolor[RGB]{51,204,51}{v^{\pi}(s)} &\doteq \mathbb{e}_{\pi}\left[ g_t|S_t=s \right] \\
            &= \mathbb{e}\left[ r_{t+1} = \gamma g_{t+1} | S_t=s \right] \\
            &= \mathbb{e}_{\pi}\left[ \sum_{k=0}^{\infty}\gamma^k r_{t+k+1}|S_t=s \right] \\
            &= \sum_{a} \pi(a|s) \sum_{s', r} p(r, s'|s, a) \left[r + \gamma \textcolor[RGB]{51,204,51}{v_{\pi}(s')}  \right]\text{, for all } s \in \mathcal{S}
        \end{split}
    \end{equation}
    where $\mathbb{E}_{\pi}$ denotes the expectation with respect to the states and rewards generated by following policy $\pi$.

    \subsubsection{Action-value Function}

    Action-Value Function $q^\pi(s, a)$, which maps a (state, action) pair to the expected reward originating from that pair when evaluated with policy $\pi$. Mathematically, an action-value function is defined as $Q: S \times A \rightarrow \mathbb{R}$, where $S$ is the set of states and $A$ is the set of actions. The Action-value function $a^{\pi}(s,a)$ represents the expected cumulative reward from taking action $a$ in state $s$, following the policy $\pi$ thereafter. In other words, it quantifies the value of selecting a particular action in a particular state under a specific policy. The Action-Value Function is denoted by $q^\pi$ and is crucial in developing various Dynamic Programming and Reinforcement Learning algorithms for the MDP Prediction problem.

    \begin{definition}
        The action-value function $Q^{\pi}(s, a)$ for a policy $\pi$ is the expected cumulative discounted reward the agent can obtain from state $s$, taking action $a$, and then following policy $\pi$ thereafter. It is defined as:
        \[
            q^{\pi}(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s, A_0 = a\right]
        \]
    \end{definition}

    To avoid confusion, $v^\pi$ is referred to as the State-Value Function, while $q^\pi$ is referred to as the Action-Value Function. The Action-Value Function provides information about the expected returns from specific state-action pairs and is useful for making decisions about which actions to take in an MDP. The relationship between the state-value function and the action-value function can be defined as:
    \[ v_\pi(s) = \max_{a\in\mathcal{A}} q_\pi(s, a) \]

    \subsubsection{Optimal State-value function}
    The goal of an agent is to maximize the reward, which means finding a policy that yields the highest possible return. To compare two policies, let $\pi$ and $\pi'$ be two different stochastic policies. Then $\pi'$ is considered better or equal to $\pi$ if $v_{\pi'}(s) \geq v_{\pi}(s)$ holds for all states $s$. It can be shown that there is always at least one policy that is better or equal to all other policies, and this policy is called the optimal policy $\pi^*$. If an agent uses the optimal policy, then for all states and actions the agent will use the optimal state-value function and the optimal action-value function:
    \begin{definition}
        The Optimal State-value function $v^*(s)$ is the maximum value function over all possible policies and is defined as:
        \begin{equation}
            \begin{split}
                v^*(s) = \max_{\pi} v^{\pi}(s) \\
            \end{split}
        \end{equation}
    \end{definition}

    \subsubsection{Optimal Action-value function}

    Conversely, if the optimal action-value function is found, the optimal policy can be derived.
    \begin{definition}
        The Optimal Action-value function $q^*(s, a)$ is the maximum action-value function over all possible policies. It is defined as:
        \begin{equation}
            \begin{split}
                q^*(s, a) &= \max_{\pi} q^{\pi}(s, a) \\
            \end{split}
        \end{equation}
    \end{definition}

    The optimal policy function $\pi^*(a | s)$ always selects an action $a$ with a selection probability of 1 for which $q_*(s, a)$ is maximal for a given state $s$. It also follows that the optimal policy is deterministic, meaning for the same state, the optimal policy function always selects exactly the same action. The transition from stochastic to deterministic policy is explained by the \emph{greedy selection}\footnote{Always choose action with the highest expexted reward.} of actions in the last equation.

    \subsubsection{Bellman's Optimality Equation}\label{subsubsec:bellman-optimality-equation}
    The optimal state-value function $v^*(s)$ must satisfy Bellman's equation, which provides an recursive rule for determining the long-term expected return of each state. In order to incorporate actions, the State-value function for a Markov Reward Process (MRP) is extended to include actions as the second parameter, leading to Bellman's optimality equation for $v^*(s)$:
    \begin{equation}
        \begin{split}
            \textcolor[RGB]{51,204,51}{v^*(s)} &= \max_{a\in\mathcal{A}} \in \mathcal{A} \mathbb{E}[R_{t+1} + \gamma \textcolor[RGB]{51,204,51}{v^*(S_{t+1})} | S_t = s, A_t = a] \\
            &= \max_{a\in\mathcal{A}} \sum_{s', r} p(r, s'|s, a) \left[r + \gamma \textcolor[RGB]{51,204,51}{v^*(s')}  \right]
        \end{split}
    \end{equation}

    This equation derives the recursive relation to the subsequent state, allowing the reference to the optimal policy to be omitted. Similarly, the Action-value function $q^*(s, a)$ can be derived as~\cite{rao2022foundations}:
    \begin{equation}
        \begin{split}
            \textcolor[RGB]{51,204,51}{q^*(s, a)} &= \mathbb{E}[R_{t+1} + \gamma \max_{a'\in\mathcal{A}} \textcolor[RGB]{51,204,51}{q^*(S_{t+1}, a')} | S_t = s, A_t = a] \\
            &= \sum_{s', r} p(r, s'|s, a) \left[r + \gamma \max_{a'\in\mathcal{A}} \in \mathcal{A} \textcolor[RGB]{51,204,51}{q^*(s', a')}  \right]
        \end{split}
    \end{equation}


    The Bellman Optimality equation can be used to recursively calculate value functions by updating the value of each state based on the immediate reward and the long-term expected reward of the next state~\cite{rao2022foundations}.

    Now that we have a solid understanding of Markov Decision Processes (MDP), which form the foundation for Reinforcement Learning (RL), we can delve into the different learning methods used in RL.


    \section{Value-based learning}\label{sec:value-based-learning}
    This section presents value-based algorithms that iteratively compute state-value or action-value functions of states and actions. These functions are then used to derive continuously improving policies. Therefore, these algorithms are referred to as value-based algorithms. First some terms should be mentioned: \emph{Prediction} is in most cases interchangeable with \emph{Evaluation}, the same applies to \emph{Improvement}, which can be changed to \emph{Control}, these two terms will be used later in this section. However, we will try to stick to the \emph{Prediction} and \emph{Evaluation}, as it is more common in the literature and makes a bit more sense based on the context of the topic.

    \subsection{Dynamic Programming}\label{subsec:dynamic-programming}
    The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies in Markov decision processes (MDPs) when a perfect model of the environment is available. Classical DP algorithms are limited in reinforcement learning due to their assumptions of a perfect model and computational expense. DP uses value functions to organize the search for good policies, and the Bellman optimality equations define optimal value functions (\emph{State-value function} and \emph{Action-value function}). DP algorithms are effective for problems with finite state, action, and reward sets. However, for continuous problems, exact solutions are limited. One approach is to approximate solutions by discretizing the state and action spaces, and applying finite-state DP methods. Second approach is to use function approximation to approximate the value functions. The latter approach is the focus of this thesis.

    DP planning algorithms consists of the evaluation of a given policy, called policy prediction, defined in \cref{par:policy-prediction} and the subsequent improvement of the policy, called policy improvement, defined in \cref{par:policy-improvement}. The goal of these two steps is to infer $v^*(s)$ or $q^*(s,a)$ for all the states and actions in a procedure called Generalized Policy Iteration, defined in \cref{subsubsec:generalized-policy-iteration}~\cite{sutton2018reinforcement, FITMT25127}.

    \subsubsection{Policy Iteration}\label{subsubsec:policy-iteration}
    Policy iteration consist of 2 steps: \emph{policy evaluation} and \emph{policy improvement}. The policy prediction step, defined in \cref{par:policy-prediction}, is used to estimate the value function $v_{\pi}$ for a given policy $\pi$. The policy improvement step, defined in \cref{par:policy-improvement} is used to improve the policy $\pi$ by selecting the action that maximizes the value function $v_{\pi}$~\cite{sutton2018reinforcement}.

    \paragraph{Policy Prediction}\label{par:policy-prediction}
    Policy evaluation, or prediction, is the process of computing the state-value function for an arbitrary policy. The state-value function, denoted as $v^\pi(s)$, represents the expected sum of discounted rewards from a given state $s$ when following policy $\pi$. If the dynamics of the environment are completely known, the policy evaluation can be formulated as a system of simultaneous linear equations, which can be solved iteratively using update rules based on the Bellman equation for $v^\pi$. This process involves finding successive approximations of the value function, starting from an initial approximation, and updating it according to the Bellman equation until convergence is achieved to $v_k(s) \approx v_{\pi}(s)$ with:
    \[
        v_{k+1}(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s, a) v_k(s') \right)
    \]
    for all states. This equation is the analogous equation to Bellman's optimality equation for $v^*(s)$ in iteration form. Once $v_{\pi}(s)$ is found for a given policy the action-value function can be determined~\cite{sutton2018reinforcement, FITMT25127}:
    \[
        q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a] = R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s'|s,a) v_{\pi}(s')
    \]

    \paragraph{Policy Improvement}\label{par:policy-improvement}
    This equation is then the Bellman's equation for $q_{\pi}(s,a)$ and can be used in the Policy Improvement step to create a new policy. It should also be ensured that the new, deterministic policy is better or equal to the old policy, when for all states applies:
    \[
        q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)
    \]
    This means that choosing action $a' = \pi'(s)$ in state $s$ under policy $\pi$ produces a better result than action $a = \pi(s)$. Next, $q_{\pi'}(s, a)$ is computed for the new policy $\pi'$ in the Policy Evaluation step and improved with $\pi'(s) = \arg\max_a q_{\pi'}(s, a)$~\cite{sutton2018reinforcement, FITMT25127}.

    \subsubsection{Value Iteration}\label{subsubsec:value-iteration}
    Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep. In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates. Because the $\max$ operation in the update equation \cref{def:value-iteration} is the only difference between these updates, this just means that the $\max$ operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted finite MDPs. The algorithm pseudocode is presented in \cref{alg:value-iteration}~\cite{sutton2018reinforcement}.

    \begin{definition}
        \label{def:value-iteration}
        \textbf{Value Iteration Equation} is that iteratively computes the state-value function $V_k(s)$ for a given policy $\pi$ using the Bellman equation:
        \[
            V_{k+1}(s) \doteq \max_a \mathbb{E}\left[ R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a \right]
            = \max_a \sum_{s', r} p(s', r|s, a) [r + V_k(s')]
        \]
    \end{definition}

    \subsubsection{Generalized Policy Iteration}\label{subsubsec:generalized-policy-iteration}
    If Policy Evaluation and Policy Improvement steps are executed enough times and no more improvement of the old policy over the new policy is found, then optimal $q^*$ and $\pi^*$ are found. Well-known algorithms in DP which implement this Generalized Policy Iteration are the Policy Iteration algorithm and its special case the Value Iteration algorithm. They use the equations Bellman's equations for $v^*(s)$ and $q^*(s,a)$ and a variant of the Generalized Policy Iteration method, where $v_{\pi}(s)$ does not have to be determined. The iteration in the Policy Evaluation step is immediately followed by the Policy Improvement step~\cite{sutton2018reinforcement, 7110602}.

    \begin{figure}[h!]
        \centering
        \begin{minipage}[b]{0.5\textwidth}
            \includegraphics[width=\textwidth]{image/gpi2}
            \caption{Generalized Policy Iteration convergence.}
            \label{fig:gpi}
        \end{minipage}
    \end{figure}

    \subsection{Monte Carlo Methods}\label{subsec:monte-carlo-methods}
    In this chapter, we explore Monte Carlo methods for estimating value functions in the context of reinforcement learning (RL) without assuming complete knowledge of the environment (\emph{transition probabilities} and \emph{reward function}). Monte Carlo methods use sample sequences of states, actions, and rewards from actual or simulated interactions with the environment for learning. These methods can be used to solve the RL problem for episodic tasks, where experience is divided into episodes and value estimates and policies are updated only on the completion of an episode.

    \begin{wrapfigure}{r}{0.35\textwidth}
        \begin{center}
            \includegraphics[width=0.33\textwidth]{image/mc-evaluation-improvement}
        \end{center}
    \end{wrapfigure}

    MC methods allow an agent to infer the optimal policy $\pi^*$ from the optimal action-value function $q^*(s,a)$ estimated through experience. MC methods use the iterative procedure Generalized Policy Iteration to incrementally infer $\pi^*$. First, a finite trajectory $(S_0, A_0, R_1, S_1, A_1, ..., R_n, S_n)$ is generated, where actions are selected according to a stochastic policy $\pi(a|s)$ and states and rewards come from the unknown environmental dynamics. From this episode, the return $G_t$ is computed for all state-action pairs reached. Then, the policy evaluation step is performed using the update equation~\cite{sutton2018reinforcement, FITMT25127, rao2022foundations}:

%    \begin{wrapfigure}[h!]
%        \includegraphics[width=\textwidth]
%        \caption{Monte Carlo convergence.}
%        \label{fig:mc}
%    \end{wrapfigure}

    \begin{equation}
        \label{eq:mc}
        q_{k+1}(s, a) = q_k(s, a) + \alpha(G_t - q_k(s, a))
    \end{equation}
    where $\alpha \in (0,1)$ is a learning rate and $G_t$ is the sum of discounted rewards starting from action $a$ in state $s$. The term $G_t - q_k(s, a)$ corrects the value of $q_{k+1}(s,a)$ in the direction of the target $G_t$. The index $k$ represents the current episode, and in the limiting case, $q_k(s,a) = q_{\pi}(s,a)$, where $\pi$ is the optimal policy~\cite{sutton2018reinforcement}.
    \begin{equation}
        \pi_0 \xrightarrow{E} q_{\pi_0} \xrightarrow{I}
        \pi_1 \xrightarrow{E} q_{\pi_1} \xrightarrow{I}
        \pi_2 \xrightarrow{E} q_{\pi_2} \xrightarrow{I}
        \cdot
        \pi_* \xrightarrow{E} q_{\pi_*}
    \end{equation}

    Since MC methods do not require knowledge of the environment dynamics, a non-deterministic policy should be used for episode generation to ensure exploration of all states and actions. A stochastic policy, such as the $\epsilon$-greedy approach, is commonly used in MC methods for exploration~\cite{FITMT25127}.

    \paragraph{Epsilon-Greedy Policy Exploration}
    The \textbf{$\epsilon$-greedy} approach for exploration selects actions randomly according to:
    \begin{equation}
        \pi'(a | S) =
        \begin{cases}
            1 - \epsilon + \frac{\epsilon}{|\mathcal{A}|} & \text{if } a = A^* \\
            \frac{\epsilon}{|\mathcal{A}|} & \text{if } a \neq A^* \text{ for } a \in \mathcal{A}
        \end{cases}
    \end{equation}
    where a greedy action is the action with the highest estimated action value according to the current policy. The process of modifying action values in the policy evaluation step and setting a new policy in the policy improvement step is referred to as training the RL agent. It follows from \cref{eq:mc} that MC methods use the $G_t$ return of an episode, which can only be computed when the episode is completed. Therefore, MC methods can only be used in environments with finite, i.e. always ending, episodes. This strategy is called \textbf{Bootstrapping}, when the methods do not need to wait for the end of the episode, but only need \emph{n-steps} to improve the policy, the whole strategy is called \emph{n-steps Bootstrapping} and next method \emph{Temporal-Difference} uses this strategy to improve convergence to optimal value-function~cite{FITMT25127, sutton2018reinforcement, rl-course-david-silver}.

    \subsection{Temporal-Difference Methods}\label{subsec:temporal-difference-methods}

    Temporal-difference (TD) learning is a fundamental concept in reinforcement learning that combines ideas from Monte Carlo and dynamic programming (DP). TD methods allow for learning from raw experience without a model of the environment's dynamics, similar to Monte Carlo methods, but also update estimates based on other learned estimates, without waiting for a final outcome (like DP methods) using \emph{n-step Bootstrapping}, defined in \cref{subsubsec:n-step-bootstrapping}. The relationship between TD, DP, and Monte Carlo is a recurring theme in reinforcement learning theory, and these ideas can be blended and combined in various ways. Both TD and Monte Carlo methods are used for policy evaluation, estimating the value function for a given policy. However, TD methods differ from Monte Carlo methods in that they update their value function estimates based on the non-terminal states observed in the experience, and they do not have to wait until the end of the episode to determine the increment to the value function. Instead, TD methods only need to wait until the next time step to form a target and make a useful update using the observed reward and the estimate of the value function for the next state~\cite{rl-course-david-silver, sutton2018reinforcement}.

    \subsubsection{n-step Bootstrapping}\label{subsubsec:n-step-bootstrapping}
    The n-step bootstrapping is a strategy to improve the convergence of the value-function. The idea is to use the value-function to estimate the value-function, instead of using the return of the episode. The n-step bootstrapping for \emph{n-step return} is defined as:
    \begin{equation}
        G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})
    \end{equation}

    The simplest TD method is TD(0), also known as one-step TD, which updates the value function using the following equation:
    \begin{equation}
        V(S_t) = V(S_t) + \alpha \cdot (R_{t+1} + \gamma \cdot V(S_{t+1}) - V(S_t))
    \end{equation}
    where \( V(S_t) \) is the estimated value of state \( S_t \), \( \alpha \) is the learning rate, \( R_{t+1} \) is the reward received at time step \( t+1 \), \( \gamma \) is the discount factor, and \( V(S_{t+1}) \) is the current estimate of the value of the next state. And for the n-step bootstrapping is defined as:
    \begin{equation}
        V_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n})\text{, for }0\leq t \leq T
    \end{equation}

    One of the key advantages of TD methods over Monte Carlo methods is that TD methods do not have to wait until the end of the episode to update the value function. Instead, they can make updates at each time step, using the observed reward and the estimate of the value function for the next state. This makes TD methods more computationally efficient and allows for online learning in environments where episodes are long or never-ending. Like DP and Monte Carlo methods, TD methods also use the iterative Generalized Policy Iteration (GPI) procedure to determine the optimal policy. The policy evaluation step in TD methods differs from Monte Carlo methods in that it uses the TD error, which is the difference between the TD target and the current estimate of the value function, as the update rule~\cite{sutton2018reinforcement}.

    The TD target is the sum of the reward and the discounted value of the next state, and it serves as the target for the TD update. The TD error and TD target are defined as follows:

    \[ \text{TD error} = R_{t+1} + \gamma \cdot V(S_{t+1}) - V(S_t) \]
    \[ \text{TD target} = R_{t+1} + \gamma \cdot V(S_{t+1}) \]

%
    The TD error at each time is the error in the estimate made at that time, depending on the next state and next reward. It is available at time $t + 1$, i.e., (t is the error in $V(S_t)$. If V does not change during the episode, then the Monte Carlo error can be written as a sum of \emph{TD errors}:
    \begin{equation}
        \begin{split}
            G_t - V(S_t) &= R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V (S_{t+1}) - \gamma V (S_{t+1})\\
            &= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \ldots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} \delta_T - V(S_T)\\
            &= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \ldots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t} (R_T + \gamma V(S_T) - V(S_T))\\
            &= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k
        \end{split}
    \end{equation}
%
    The TD error represents the error in the current estimate of the value function, and the TD target represents the updated estimate of the value function for the current state. By using the TD error and TD target in the update rule, TD methods can learn from raw experience and update the value function at each time step, making them well-suited for online learning tasks.

    In addition to TD(0), there are also other variations of TD methods such as TD($\lambda$), which is a family of methods that use eligibility traces to combine TD(0) updates with Monte Carlo-like updates. TD($\lambda$) methods have a parameter $\lambda$ that controls the trade-off between the bias and variance of the updates. When $\lambda$ is set to 0, TD($\lambda$) reduces to TD(0), and when $\lambda$ is set to 1, TD($\lambda$) reduces to Monte Carlo updates. TD($\lambda$) methods update the value function for not only the current state but also for all the states visited in the episode, weighted by their eligibility traces, defined in \verb|\cref{TODO}|, which are updated at each time step. The eligibility trace keeps track of the recent history of state visits and decays over time with a decay factor $\lambda$, which determines the credit assignment to different states~\cite{sutton2018reinforcement}.
%

    \subsubsection{On-Policy Learning (SARSA)}\label{sec:td-sarsa}
    The TD methods use the action-value function for the evaluation step and iteratively determine $q_\pi(s,a)$. A TD method that implements this practically is called SARSA and adjusts the action-value function in the evaluation step for a state-action pair at each time-step of the episode as:
    \[
        q_{t+1}(s, a) = q_t(s, a) + \alpha \left( R_{t+1} + \gamma q_t(s', a') - q_t(s, a) \right) \quad (2.21)
    \]

    The TD methods, such as SARSA, utilize the action-value function for evaluating and updating the action-value estimates. SARSA adjusts the action-value function at each time-step of the episode based on the sequence of transitions \( (s, a, r, s', a') \) generated by the environment dynamics and the current epsilon-greedy policy. The updated action-value function is used to determine the new policy at the next time-step. This process is repeated iteratively until optimal action-value and policy functions are found. SARSA is an on-policy algorithm, as it evaluates and improves the same policy used for selecting actions:
    \[
        q_{t+1}(s, a) = q_t(s, a) + \alpha \left( R_{t+1} + \gamma \max_{a^*} q_t(s', a^*) - q_t(s, a) \right)
    \]

    \subsubsection{Off-Policy Learning (Q Learning)}\label{sec:td-q-learning}
    On the other hand, off-policy algorithms, like Q-learning, evaluate and improve one policy while using a different policy for selecting actions. This can be useful, for example, when introducing a new version of an agent with a different policy to learn from an old agent with a well-performing policy. In Q-learning, the action maximizing action (\( a^* \)) of all action-value functions at a fixed state \( s' \) is always chosen when updating the action-value function for a given state-action pair \( (s, a) \), regardless of which action \( a^* \) of the policy was chosen. After updating the action-value function, the policy improvement step is performed using the epsilon-greedy approach with respect to the current policy (\( \pi \)) and optionally the old policy (\( \mu \)) as well~\cite{FITMT25127, sutton2018reinforcement}.

    An off-policy TD control algorithm known as Q-learning, defined by:
    \begin{equation}
        \label{eq:q-learning}
        Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right]
    \end{equation}

    In this case, the learned action-value function, \(Q\), directly approximates \(q^*\), the optimal action-value function, independent of the policy being followed. This dramatically simplifies the analysis of the algorithm and enables early convergence proofs. The policy still has an effect in that it determines which state-action pairs are visited and updated. However, all that is required for correct convergence is that all pairs (transition matrix, example in \cref{fig:q-learning}) continue to be updated. This is a minimal requirement in the sense that any method guaranteed to find optimal behavior in the general case must require it. The Q-learning algorithm is show in \cref{alg:q_learning}~\cite{rl-course-david-silver, sutton2018reinforcement}.

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.9\textwidth]{image/q-learning}
        \caption{Q-learning table. How the training process updates the Q-table and plan moves~\cite{article}.}
        \label{fig:q-learning}
    \end{figure}

    \subsection{Function Approximation}\label{subsec:function-approximation}
    The Monte Carlo and Temporal Difference algorithms discussed earlier are known as tabular methods because they rely on a matrix of action returns and values of dimension $|\mathcal{S}|\mathcal{A}|$. However, when the number of states or actions is extremely large, as in the case of \emph{Portfolio Allocation}, where the state space is the entire stock market, which may include data from thousands of stocks, the economic situation of the entire world, and the current political situation, all of these things need to be taken into account, so the state space is huge. The agent managing which assets it allocates funds to uses these observations, which very often change every signle second, so this matrix becomes excessively huge. This poses a problem in terms of memory requirements, and it becomes almost impossible for an agent using MC or TD methods to comprehensively evaluate all the states and derive a policy~\cite{sutton2018reinforcement}.

    Approximate Solution Methods, which utilize parametrized function approximators such as decision trees, regression methods, or neural networks, differ from Tabular Methods in that they do not rely on matrices to estimate action-value returns. These methods have emerged as a solution approach to the limitations of Tabular Methods, as they are more memory efficient and capable of generalization. When neural networks with hidden layers are used as function approximators, the resulting algorithms are classified as Deep Reinforcement Learning (DRL)~\cite{FITMT25127}.

    The Approximate Solution Methods use a function approximator \(f_\theta\) to approximate the action-value function \(q_\pi(s,a)\) or the state-value function \(v_\pi(s)\) as:

    \(\hat{v}(s,w) \approx v_{\pi}(s)\) or \(\hat{q}(s,a,w) \approx q_{\pi}(s,a)\), where \(w \in \mathbb{R}^d, d \in \mathbb{N}\)

    The weight vector $\mathbf{w}$ represents the weight vectors of the neural network in the context of iterative Stochastic Gradient Descent (SGD) procedure. The loss function $E(\mathbf{w})$ is defined over this weight vector, and the weight vector is updated during SGD as long as the error continuously decreases, i.e., until $(s, a, \mathbf{w}) \approx q_{\pi}(s, a)$.

    For the loss function, Mean Square Error (MSE) can be used, which is commonly used in regression optimization problems. In this case, $\mathcal{T}$ is the set of training data from which the target variable $y$ is taken. However, unlike in classical supervised learning, where the target variable is known, here $y = q_{\pi}(s,a)$, so the target is assumed to be the loss function. Thus, for function approximators applied to MC methods and SGD assumed as the optimization algorithm, the loss function for one-step SARSA is defined as:

    \[ \frac{1}{2} \left( R_{t+1} + \gamma q_{\pi}(s',a', \mathbf{w}) - q_{\pi}(s, a, \mathbf{w}) \right)^2 \]

    The weights for one-step SARSA are updated using the following rule:

    \[ \mathbf{w}_{t+1} = \mathbf{w}_t + \alpha \left( R_{t+1} + \gamma q_{\pi}(s',a', \mathbf{w}) - q_{\pi}(s, a, \mathbf{w}) \right) \nabla q_{\pi}(s, a, \mathbf{w}) \]

    The update is performed in a way that each vector's component responsible for a part of the error is updated accordingly, with a higher share of the error being adjusted if the slope of the error with respect to that component is large. The weights are adjusted until the difference between two subsequent updates is less than a defined $\varepsilon$ value (typically a small number), at which point $q_{\pi}$ is considered found and the optimal policy can be derived. In practice, early-stopping is often used in deep learning instead of $\varepsilon$.

    Now that the vectors of state-action values are no longer represented as a look-up table, they can be written as an approximation by the neural network. Additionally, the $\varepsilon$-greedy approach for action selection can be applied in the policy improvement step~\cite{FITMT25127}.

    \begin{equation}
        \mathbf{\hat{y}} = \begin{pmatrix}
                               \hat{q}(s,\textcolor[RGB]{51,204,51}{a_1}, \mathbf{w}) \\
                               \vdots                                                 \\
                               \hat{q}(s, \textcolor[RGB]{51,204,51}{a_n}, \mathbf{w})
        \end{pmatrix}
    \end{equation}


    \section{Policy-based learning}\label{sec:policy-based}
    Methods that directly parameterize the policy, analogous to parameterizing state-value and action-value functions in subsection 2.3.4, are discussed in this section. Subsection 2.4.1 introduces and explains stochastic policy gradient methods. Subsequent subsections discuss various stochastic policy-gradient algorithms. Subsection 2.4.3 elaborates on the concept of actor-critic methods, while subsection 2.4.4 explains trust-region methods and the policy gradient algorithm (PPO) used in Chapter 4.

    All algorithms that use a parameterized policy are referred to as policy gradient methods. We can represent the policy with a parameter vector \( \theta \in \mathbb{R}^d \), where \( d \in \mathbb{N} \). Then, extending Definition 2.6, we can formulate the probability of executing action \( a \) in state \( s \) with parameters \( \theta \) at time-step \( t \) of policy \( \pi_\theta \) as:

    \[ \pi(a|s,\theta) = P(A_t = a|S_t = s,\theta_t = \theta) \]

    If a performance measure \( J(\theta) \) is introduced on the parameter vector, then the policy gradient methods can iteratively update the parameter vector using the gradient ascent procedure as follows:

    \[ \theta_{t+1} = \theta_t + \alpha \nabla J(\theta_t) \]

    Deriving a loss function for the performance measure in policy gradient methods is not as straightforward as in the case of value-based methods. Similar to the distinction between MC methods and TD methods, we need to differentiate between finite and infinite episodes. For both cases, the theorem on policy gradients provides a way to compute the gradient \( \nabla J(\theta) \).

    For finite episodes, we can measure the quality of the parameter vector based on the return of the first state of the episode. The policy gradient for

    \begin{equation}
        \begin{split}
            \nabla v_{\pi_\theta}(\mathbf{s}) &= \nabla \left( \sum_{a} \pi(a | s, \mathbf{\theta}) q_{\pi_\theta}(s,a) \right) \\
            &= \sum_{a} (\nabla \pi(a | s, \mathbf{\theta}) q_{\pi_\theta}(s,a) + \pi(a | s, \mathbf{\theta}) \nabla q_{\pi_\theta}(s,a)) \\
            &\mspace{10mu}\vdots\\
            &= \sum_{s}\sum_{k=0}^{\infty} \mathbb{P}(s_0 \rightarrow s, k, \pi_\theta) \sum_{a} \nabla \pi(a | s, \mathbf{\theta}) q_\pi(s,a)
        \end{split}
    \end{equation}


    where $P(s \rightarrow s', k, \pi_{\theta})$ is the probability of a state transition from $s$ to $s'$ in $k$ steps under policy $\pi(a | s, \theta)$. This probability thus serves as a weighting of the gradient of the return. Moreover, if $\eta(s) = \sum_{k=0}^{\infty} P(s \rightarrow s', k, \pi_{\theta})$ is defined, it can be further written:

    \begin{equation}
        \label{eq:policy_gradient}
        \begin{split}
            \nabla J(\mathbf{\theta}) &= \nabla v_{\pi_{\mathbf{\theta}}}(s) \\
            &= \sum_{s}\eta(s) \sum_{a}\nabla \pi(a | s, \mathbf{\theta})q_{\pi_{\mathbf{\theta}}}(s,a) \\
            &= \sum{s'}\eta(s') \sum_{s}\frac{\eta(s)}{\sum_{'s}\eta(s')} \sum{a} \nabla \pi(a | s, \mathbf{\theta})q_{\pi_{\mathbf{\theta}}}(s,a) \\
            &= \sum_{s'}\eta(s') \sum_{s}\mu(s) \sum_{a}\nabla \pi(a | s, \mathbf{\theta})q_{\pi_{\mathbf{\theta}}}(s,a)\\
            &\propto \sum_{s}\mu(s) \sum_{a}\nabla \pi(a | s, \mathbf{\theta})q_{\pi_{\mathbf{\theta}}}(s,a) \\
            &= E_{\pi} \left[\sum_{a}q_{\pi_{\mathbf{\theta}}}(s,a) \nabla \pi(a | s, \mathbf{\theta})  \right]
        \end{split}
    \end{equation}

    The gradient of \(J(\theta)\) is proportional to the probability sum of \(\sum_s \mu(s)\), which denotes the likelihood that the agent is in state \(s\), multiplied by the sum of weighted gradients of the probabilities for the selection of actions. The proportionality constant is represented as \(\sum_s \eta(s)\).

    Practically, this equation provides insight that when the agent is in a state, it should move in the direction of weights that represent the greatest increase in the probability of selecting an action in that state, weighted by the expected return for that state and action~\cite{FITMT25127}.

    In the case of infinite episodes, the performance measure \(J(\theta)\) is the average reward per time-step. The gradient \(\nabla J(\theta)\) can then be determined by the theorem of policy gradients for infinite episodes:
    \begin{equation}
        \nabla J(\theta) = \sum_s \mu(s) \sum_a \nabla \pi(a | s, \theta) q_\pi(\theta) (s, a)
    \end{equation}
    The calculations of the gradient for the infinite case correspond to those of the episodic case.

    \subsection{REINFORCE - Monte Carlo Policy Gradient}\label{subsec:monte-carlo-policy-gradient-methods}
    The REINFORCE algorithm, also known as Monte-Carlo Policy Gradient, is a purely policy-based approach that utilizes the return of complete finite episodes for parameter updates. Similar to classical Monte Carlo (MC) methods, it adjusts the return gradually with each time-step, unlike MC methods where the return is updated at the end of each episode. In this algorithm, the weight vector $\theta$ is updated using the product of the return $G_t$ and the gradient vector $\nabla \ln \pi(a | s, \theta)$, which represents the steepest increase in the probability of selecting action $a$ in state $s$. This weight adjustment ensures that $\theta$ is updated with the scaling factor $G_t$.

    \begin{equation}
        \theta_{t+1} = \theta_{t} + \alpha \gamma^{t} G_t \nabla \ln \pi(a | s, \theta_t)
    \end{equation}


    The algorithm employs stochastic gradient ascent for optimization, as the weights are updated at each time-step. Depending on whether the action space of the environment is discrete or continuous, the policy parametrizations described in the previous subsection can be used for the gradient $\nabla \ln \pi(a | s, \theta_t)$. One drawback of this algorithm is the high variability in returns between time-steps and the associated "slowness" of the learning process. This variance arises from the non-adaptive and loose formulation of the scaling factor.

    \subsection{Actor-Critic Policy Gradient Methods}\label{subsec:actor-critic-policy-gradient-methods}
    The Actor-Critic Policy Gradient methods utilize the long-term expected return for an action in a state, denoted as $q_{\pi_\theta}(s, a)$, as a scaling factor for the largest increase in the probability of selecting that action in the given state. In order to address the issues of high variance and slow learning in REINFORCE, the theorem on policy gradients is extended to include a baseline $b(s)$ that acts adaptively on the scaling factor for each state. The policy gradient update formula can be written as:

    \begin{equation}
        \nabla J(\theta) \propto \sum \mu(s) \sum (q_\pi(\theta) (s, a) - b(s)) \nabla \pi(a | s, \theta)
    \end{equation}

    If the baseline is set to zero for all states, the initial \cref{eq:policy_gradient} is obtained. Alternatively, a vector of constants can also be used as the baseline. The concept of Actor-Critic methods can be formulated, when a function approximator with bootstrapping, such as TD(0) or SARSA(0), is used as the baseline. In this case, the Critic computes the scaling factor and the Actor adjusts the policy parameter $\mathbf{\theta}$. The weight update formula for the Actor network is then as follows:
    \begin{equation}
        \begin{split}
            \theta^{ t + 1 } &= \theta_{ t } + \alpha( q_{ \pi_{ \theta } } ( s , a ) - b( s ) ) \frac{ \nabla_{ \pi } ( a | s, \mathbf{ \theta_{ t } } ) } { \pi( a | s, \mathbf{ \theta_{ t } } ) } \\
            &= \theta_{ t } + \alpha( R_{ t + 1 } + \hat{ q } ( s' , a', \mathbf{ w_{ t } } ) - \hat{ q } ( \mathbf{ s }, a, \mathbf{ w_{ t } ) } ) \frac{\nabla_{ \pi } ( a | s , \mathbf{\theta_{ t } } ) } { \pi( a | s, \mathbf{ \theta_{ t } ) } } \\
        \end{split}
    \end{equation}
    where:
    \begin{equation}
        \begin{split}
            \frac { \nabla_{ \pi } ( a | s, \mathbf{ \theta_{ t } } ) } { \pi ( a | \mathbf{ s }, \theta_{ t } ) } = \nabla \ln( \pi( a | s, \mathbf{ \theta_{ t } } ) )
        \end{split}
    \end{equation}

    \subsection{Trust Region Policy Methods}\label{subsec:trust-region-policy-methods}
    TODO


    \section{Actor-Critic}\label{sec:actor-critic}
    TODO


    \section{Model-based}\label{sec:model-based}
    As mentioned in Section~\cref{sec:rl-introduction}, RL algorithms can be broadly categorized into two main types: model-free and model-based. Model-based methods rely on \emph{planning} as they primary component, while model-free methods rely on \emph{learning}~\cite{sutton2018reinforcement}. Despite their distinctions, both approaches involve computing value functions and using them to update approximate value functions based on future events.

    One popular approach is to use a probabilistic model of the environment, which can be represented as a transition function $P(s_{t+1}|s_t, a_t)$ that describes the probability of transitioning from state $s_t$ to state $s_{t+1}$ when taking action $a_t$. Some popular probabilistic model-based RL methods include Monte Carlo Tree Search, Neural Network Dynamics, and Probabilistic Ensembles with Trajectory Samplin.

    Another approach in model-based RL is to use a learned deterministic model of the environment, which can be represented as a function $f(s_t, a_t)$ that directly maps states and actions to next states. Deterministic model-based RL methods, such as World Models, learn an encoder to represent states, a recurrent neural network (RNN) to model dynamics, and a decoder to generate predicted next states.

    \subsection{Planning}\label{subsec:planning}
    A model of the environment refers to anything that an agent can use to predict the outcome of its actions. It can be either stochastic, where there are multiple possible outcomes with associated probabilities, or deterministic, where the outcome is fixed. Distribution models provide a description of all possible outcomes and their probabilities, while sample models generate a single outcome sampled from the probabilities.

    Planning, in the context of artificial intelligence, refers to a computational process that takes a model of the environment as input and produces or improves a policy for interacting with the environment. There are two main approaches to planning: \emph{state-space} planning and \emph{plan-space} planning. In \emph{state-space} planning, the focus is on searching through the \textbf{space of states} to find an optimal policy or path to a goal. Value functions are computed over states to guide the search. On the other hand, in \emph{plan-space} planning, the search is conducted through the \textbf{space of plans}, where operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space methods are challenging to efficiently apply to the stochastic sequential decision problems that are the primary focus in reinforcement learning~\cite{sutton2018reinforcement}.

    The common structure in \emph{state-space} planning and learning methods, as presented in this chapter, is that value functions are computed as intermediate steps using simulated experience. Value functions are used to estimate the expected future rewards of different actions or states, and they play a key role in improving the policy of an agent. Simulated experience is generated by using the model to simulate the environment, and the agent can update its value functions based on this simulated experience to guide its decision-making process.

    \begin{center}
        \begin{tikzpicture}[auto]
% Nodes
            \node[] (s) {model};
            \node[right=1cm of s] (a) {simulated experience};
            \node[right=2cm of a] (r) {values};
            \node[right=1cm of r] (s_p) {policy};

% Arrows
            \draw[thick, ->] (s) -- (a);
            \draw[thick, ->] (a) -- (r) node[midway, anchor=south] {backups};
            \draw[thick, ->] (r) -- (s_p);
        \end{tikzpicture}
    \end{center}

    Dynamic programming methods fit the structure of making sweeps through the space of states, generating possible transitions, computing backed-up values, and updating state estimates. Other state-space planning methods also fit this structure, with differences in updates, order, and retention of backed-up information. Planning methods are related to learning methods in estimating value functions through backing-up operations. Learning methods use real experience from the environment, while planning uses simulated experience from a model. Ideas and algorithms can be transferred between planning and learning. Planning in small, incremental steps allows for efficient interruption and redirection, which is beneficial for intermixing planning with acting and learning of the model. Planning in small steps may be the most efficient approach for large planning problems~\cite{sutton2018reinforcement}.

    See \cref{alg:random-sample-one-step-tabular-q-planning}

    \subsection{Dyna-Q}\label{sec:dyna-q}
%    \begin{wrapfigure}{r}{0.35\textwidth}
%        \label{fig:dyna-q-learning-graph}
%        \begin{center}
%            \includegraphics[width=0.33\textwidth]{image/dyna-q-graph}
%        \end{center}
%    \end{wrapfigure}
    Dyna-Q includes all of the processes shown in the \cref{fig:dyna-q-learning}: planning, acting, model-learning, and direct RL—all occurring continually. The Dyna-Q algorithm is a learning method that combines both planning and direct RL approaches (MC and TD). Planning involves optimizing strategies through simulations using a learned environment model, while direct RL focuses on improving the value and policy functions using real experiences from interactions with the environment, as depicted in \cref{fig:dyna-q-learning}. The Dyna agent interacts with the real environment to collect empirical data and iteratively improves its policy using \cref{eq:q-learning}. At the same time, an environment model is learned using real experiences, and simulation data generated by the model are used to update the action-value function using the Q-Learning update method.

    In tasks such as path planning, obtaining real experiences from interactions with the environment can be costly and time-consuming. However, using the learned environment model to generate simulation experiences is faster and more convenient, which can reduce the training time of the algorithm. See the~\cref{alg:dyna-q}.

    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{image/dyna-q-learning}
        \caption{Dyna-Q learning process.}
        \label{fig:dyna-q-learning}
    \end{figure}

    \subsection{Trajectory Sampling}\label{subsec:trajectory-sampling}

    \subsection{Summary}\label{subsec:summary}

    Model-based RL offers several advantages over model-free methods. One major advantage is sample efficiency, as the agent can use the learned model of the environment to plan and generate simulated trajectories for learning, reducing the need for costly real-world interactions. Additionally, model-based RL can enable the agent to handle complex, high-dimensional state spaces and long-horizon tasks more effectively.

    However, model-based RL also faces challenges. One challenge is the accuracy of the learned model, as any errors in the model can lead to suboptimal policies. Another challenge is the computational cost of planning and decision-making using the learned model, as it requires additional computation compared to direct action selection in model-free methods.

    In conclusion, model-based RL is a promising approach that can offer sample-efficient learning and improved performance in complex environments. Various methods, such as probabilistic models and deterministic models, have been proposed in the literature. Despite some challenges, model-based RL continues to be an active area of research in machine learning.


    \section{Exploration vs. Exploitation}\label{sec:exploration-vs-exploitation}

    \subsection{Greedy}\label{subsec:greedy}

    \subsection{Epsilon-Greedy}\label{subsec:epsilon-greedy}

    \subsection{Upper Confidence Bound}\label{subsec:upper-confidence-bound}


%    \section{Deep Reinforcement Learning}\label{sec:deep-reinforcement-learning}
%    TODO

\end{document}
