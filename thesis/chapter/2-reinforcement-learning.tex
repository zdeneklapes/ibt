\documentclass[../chapters.tex]{subfiles}
\begin{document}
    \chapter{Reinforcement Learning}\label{ch:reinforcement-learning}

    In this chapter, we will explore the application of Reinforcement Learning (RL) techniques in the context of portfolio allocation.
    We will begin by providing an overview of RL in context of AI, its fundamental concepts, including the agent, environment, state, action, and reward.
    We will also highlight the motivation for using RL in portfolio allocation and discuss its potential benefits and challenges.
    First we need understand the differences between types of learning:

    \paragraph{Supervised Learning}
    In supervised learning (SL), a model is trained on a labeled dataset, where the input data is paired with corresponding output labels.
    The model learns to make predictions based on the labeled examples, and the goal is to minimize the error between predicted outputs and actual labels.
    Common applications of supervised learning include image classification, speech recognition, and sentiment analysis.

    \paragraph{Semi-supervised Learning}
    Semi-supervised learning (SSL) is a combination of supervised and unsupervised learning.
    It uses a small labeled dataset along with a large unlabeled dataset for training.
    The model leverages the limited labeled examples to learn patterns from the unlabeled data, and then makes predictions on unseen data.
    SSL is useful when obtaining labeled data is expensive or time-consuming.
    It is often used in scenarios where obtaining a large labeled dataset is challenging, such as in medical diagnosis or fraud detection.

    \paragraph{Unsupervised Learning}
    In unsupervised learning (UL), the model learns from unlabeled data without any predefined output labels.
    The goal is to find underlying patterns, structures, or relationships within the data.
    Common unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection.
    UL is used in scenarios where labeled data is scarce or not available, and the model needs to discover patterns autonomously from the data.

    \paragraph{Reinforcement Learning}
    Reinforcement learning is a type of learning paradigm where an agent interacts with an environment and learns to take actions to maximize a cumulative reward signal~\cite{sutton2018reinforcement}.
    The agent learns through \texttt{trial-and-error}, exploring different actions and receiving feedback in the form of rewards or penalties based on its actions.
    RL is used in scenarios where the optimal action is not known, and the agent needs to learn from feedback to make sequential decisions.
    Common applications of RL include game playing, robotic control, autonomous driving, and financial trading.


    \section{Introduction}\label{sec:rl-introduction}
    Reinforcement learning (RL) is an exciting field at the intersection of artificial intelligence (AI) and machine learning (ML) that deals with training agents to make optimal decisions in dynamic environments.
    RL is inspired by the way humans learn from experience, where an agent interacts with an environment, receives feedback in the form of rewards or penalties, and uses this feedback to learn and improve its decision-making abilities.

    At the heart of RL lies the concept of an agent, which that takes actions in an environment to achieve specific goals.
    The environment is typically modeled as a Markov decision process (MDP), which is a mathematical framework that describes how an agent interacts with an environment in discrete time steps.
    An MDP is defined by a tuple $(S, A, P, R)$, where $S$ is the state space, $A$ is the action space, $P$ is the transition probability function, and $R\in\mathbb{R}$ is the reward.

    The goal of an RL agent is to learn a policy, denoted by $\pi$, which is a mapping from states to actions that maximizes the cumulative rewards over time $T$.
    The agent uses this policy to select actions at each time step, and the environment responds with a new state and a reward.
    The agent then updates its policy based on the observed rewards and states, aiming to improve its decision-making abilities and achieve higher rewards in the long run.

    RL algorithms can be broadly categorized into two main types: model-free and model-based.
    Model-free algorithms are then further divided into value-based and policy-based methods, such as Q-learning (value-based) and policy gradient methods (policy-based), learn directly from the interactions with the environment without explicitly modeling the transition probabilities and rewards.
    Model-based algorithms, on the other hand, learn a model of the environment and use it to make decisions.
    These algorithms have their strengths and weaknesses, and the choice between them depends on the specific problem and requirements.

    The sequence of states, actions and rewards that the agent experiences is called a trajectory, and it look like this:
    \begin{equation}
    (S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \dots, S_{T-1}, A_{T-1}, R_T, S_T)
    \end{equation}

    This sequence of \emph{state-action-reward} can be finite or infinite, depending on the environment and the agent's goal.
    Pretty good example of this is the game of chess, where the game ends when one of the players wins or the game is a draw.
    In this case, the trajectory is finite, and the agent's goal is to maximize the cumulative reward over time $T$.
    On the other hand, the self-driving car example is an infinite-horizon problem, where the agent's goal is to maximize the cumulative reward over an infinite time horizon or until the car reaches its destination.

    \begin{figure}[h!]
        \includegraphics[width=0.8\linewidth]{image/agent-environment}
        \centering
        \caption{The agent interacts with the environment and learns to maximize the cumulative reward over time $T$.}
        \label{fig:rl-introduction}
    \end{figure}


    \section{Markov Models for Reinforcement Learning}\label{sec:markov-models-for-rl}
    TODO

    \subsection{Markov Process}\label{subsec:markov-process}
    TODO

    \subsection{Markov Reward Process}\label{subsec:markov-reward-process}
    TODO

    \subsection{Markov Decision Process}\label{subsec:markov-decision-process}
    TODO

    \subsubsection{State-value Function}\label{subsubsec:state-value-function}
    TODO

    \subsubsection{Action-value Function}\label{subsubsec:action-value-function}
    TODO

    \subsubsection{Stochastic Policy Function}\label{subsubsec:stochastic-policy-function}
    TODO

    \subsubsection{Reward function}\label{subsubsec:reward-function}
    TODO

    \subsubsection{Bellman's Equation}\label{subsec:bellman-equation}
    TODO

    \subsubsection{Bellman's Optimality Equation}\label{subsec:bellman-optimality-equation}
    TODO


    \section{Model-based methods}
    As mentioned in Section~\ref{sec:rl-introduction}, RL algorithms can be broadly categorized into two main types: model-free and model-based.
    Model-based methods rely on \emph{planning} as they primary component, while model-free methods rely on \emph{learning}~\cite{sutton2018reinforcement}.
    Despite their distinctions, both approaches involve computing value functions and using them to update approximate value functions based on future events.

    One popular approach is to use a probabilistic model of the environment, which can be represented as a transition function $P(s_{t+1}|s_t, a_t)$ that describes the probability of transitioning from state $s_t$ to state $s_{t+1}$ when taking action $a_t$.
    Some popular probabilistic model-based RL methods include Monte Carlo Tree Search, Neural Network Dynamics, and Probabilistic Ensembles with Trajectory Samplin.

    Another approach in model-based RL is to use a learned deterministic model of the environment, which can be represented as a function $f(s_t, a_t)$ that directly maps states and actions to next states.
    Deterministic model-based RL methods, such as World Models, learn an encoder to represent states, a recurrent neural network (RNN) to model dynamics, and a decoder to generate predicted next states.

    \subsection{Planning}\label{subsec:planning}
    A model of the environment refers to anything that an agent can use to predict the outcome of its actions.
    It can be either stochastic, where there are multiple possible outcomes with associated probabilities, or deterministic, where the outcome is fixed.
    Distribution models provide a description of all possible outcomes and their probabilities, while sample models generate a single outcome sampled from the probabilities.

    Planning, in the context of artificial intelligence, refers to a computational process that takes a model of the environment as input and produces or improves a policy for interacting with the environment.
    There are two main approaches to planning: \emph{state-space} planning and \emph{plan-space} planning.
    In \emph{state-space} planning, the focus is on searching through the \textbf{space of states} to find an optimal policy or path to a goal.
    Value functions are computed over states to guide the search.
    On the other hand, in \emph{plan-space} planning, the search is conducted through the \textbf{space of plans}, where operators transform one plan into another, and value functions, if any, are defined over the space of plans.
    Plan-space methods are challenging to efficiently apply to the stochastic sequential decision problems that are the primary focus in reinforcement learning~\cite{sutton2018reinforcement}.

    The common structure in \emph{state-space} planning and learning methods, as presented in this chapter, is that value functions are computed as intermediate steps using simulated experience.
    Value functions are used to estimate the expected future rewards of different actions or states, and they play a key role in improving the policy of an agent.
    Simulated experience is generated by using the model to simulate the environment, and the agent can update its value functions based on this simulated experience to guide its decision-making process.
%
%    \begin{figure}[h!]
%        \includegraphics[width=0.8\linewidth]{image/model-based-planning}
%        \centering
%        \caption{Model-based simulated experience diagram: to plan and generate simulated trajectories for learning policy.}
%        \label{fig:model-based-planning}
%    \end{figure}
    \begin{center}
        \begin{tikzpicture}[auto]
            % Nodes
            \node[] (s) {model};
            \node[right=1cm of s] (a) {simulated experience};
            \node[right=2cm of a] (r) {values};
            \node[right=1cm of r] (s_p) {policy};

            % Arrows
            \draw[thick, ->] (s) -- (a);
            \draw[thick, ->] (a) -- (r) node[midway, anchor=south] {backups};
            \draw[thick, ->] (r) -- (s_p);
        \end{tikzpicture}
    \end{center}

    Dynamic programming methods fit the structure of making sweeps through the space of states, generating possible transitions, computing backed-up values, and updating state estimates.
    Other state-space planning methods also fit this structure, with differences in updates, order, and retention of backed-up information.
    Planning methods are related to learning methods in estimating value functions through backing-up operations.
    Learning methods use real experience from the environment, while planning uses simulated experience from a model.
    Ideas and algorithms can be transferred between planning and learning.
    Planning in small, incremental steps allows for efficient interruption and redirection, which is beneficial for intermixing planning with acting and learning of the model.
    Planning in small steps may be the most efficient approach for large planning problems.
    \newline
    \newline
    \begin{algorithm}[H]
        \SetKwInOut{Input}{Input}
        \SetKwInOut{Output}{Output}
        \SetAlgoLined
        \Input{
            $S$: Set of states \\
            $A$: Set of actions \\
            $\alpha$: Learning rate \\
            $\gamma$: Discount factor \\
        }
        \While{True}{
            Select a state $s \in S$, action $a \in A(s)$ at random\;
            Send $s, a$ to a sample model, and obtain a sample next reward $r$ and a sample next state $s'$\;
            Apply one-step tabular Q-learning update to $s, a, r, s'$:
            \[
                Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left( r + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a) \right)
            \]
        }
        \caption{Random-sample one-step tabular Q-planning}
    \end{algorithm}

    \subsection{Trajectory Sampling}

    \subsection{Summary}

    Model-based RL offers several advantages over model-free methods.
    One major advantage is sample efficiency, as the agent can use the learned model of the environment to plan and generate simulated trajectories for learning, reducing the need for costly real-world interactions.
    Additionally, model-based RL can enable the agent to handle complex, high-dimensional state spaces and long-horizon tasks more effectively.

    However, model-based RL also faces challenges.
    One challenge is the accuracy of the learned model, as any errors in the model can lead to suboptimal policies.
    Another challenge is the computational cost of planning and decision-making using the learned model, as it requires additional computation compared to direct action selection in model-free methods.

    In conclusion, model-based RL is a promising approach that can offer sample-efficient learning and improved performance in complex environments.
    Various methods, such as probabilistic models and deterministic models, have been proposed in the literature.
    Despite some challenges, model-based RL continues to be an active area of research in machine learning.


    \section{Model-free methods}

    \subsection{Value-based methods}
    TODO

    \subsubsection{Dynamic Programming}\label{subsubsec:dynamic-programming}
    TODO

    \paragraph{Policy Iteration}\label{par:policy-iteration}
    TODO

    \paragraph{Value Iteration}\label{par:value-iteration}
    TODO

    \paragraph{Monte Carlo Methods}\label{par:monte-carlo-methods}
    TODO

    \paragraph{Temporal Difference Methods}\label{par:temporal-difference-methods}
    TODO

    \subsection{Policy-based methods}\label{subsec:policy-based-methods}

    \subsubsection{Policy Gradient Methods}\label{subsubsec:policy-gradient-methods}
    TODO

    \subsubsection{Actor-Critic Methods}\label{subsubsec:actor-critic-methods}
    TODO

    \subsubsection{Monte Carlo Policy Gradient Methods}\label{subsubsec:monte-carlo-policy-gradient-methods}
    TODO

\end{document}
