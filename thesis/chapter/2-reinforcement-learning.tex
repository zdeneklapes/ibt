\documentclass[../xlapes02]{subfiles}
\begin{document}
    \chapter{Reinforcement Learning}\label{ch:reinforcement-learning}

    The motivation for this chapter comes from the influential book on reinforcement learning by Richard Sutton and Andrew Barto and Foundations of Reinforcement Learning with Application in Finance by Ashwin Rao and Tikhon Jelvis~\cite{rao2022foundations, sutton2018reinforcement}.

    This chapter explore the application of Reinforcement Learning techniques in the context of portfolio allocation. In \cref{sec:markov-models-for-rl} we will provide an introduction to Markov models, first explore \emph{Markov Processes}, \emph{Markov Reward Processes} and \emph{Markov Decision Processes}, which are the fundamental building blocks of Reinforcement Learning. Afterward, we divide Reinfocement Learning into two main categories. In \cref{sec:model-based-methods}, we will discuss \emph{Model-based} Reinforcement Learning methods, and in \cref{sec:model-free-methods} we will discuss \emph{Model-free} Reinforcement Learning methods in details with advantages and disadvantages of each approach. Let start with introduction to AI in general and its different types of learning:

    \paragraph{Supervised Learning}
    In supervised learning (SL), a model is trained on a labeled dataset, where the input data is paired with corresponding output labels. The model learns to make predictions based on the labeled examples, and the goal is to minimize the error between predicted outputs and actual labels. Common applications of supervised learning include image classification, speech recognition, and sentiment analysis.

    \paragraph{Semi-supervised Learning}
    Semi-supervised learning (SSL) is a combination of supervised and unsupervised learning. It uses a small labeled dataset along with a large unlabeled dataset for training. The model leverages the limited labeled examples to learn patterns from the unlabeled data, and then makes predictions on unseen data. SSL is useful when obtaining labeled data is expensive or time-consuming. It is often used in scenarios where obtaining a large labeled dataset is challenging, such as in medical diagnosis or fraud detection.

    \paragraph{Unsupervised Learning}
    In unsupervised learning (UL), the model learns from unlabeled data without any predefined output labels. The goal is to find underlying patterns, structures, or relationships within the data. Common unsupervised learning tasks include clustering, dimensionality reduction, and anomaly detection. UL is used in scenarios where labeled data is scarce or not available, and the model needs to discover patterns autonomously from the data.

    The last type is Reinforcement Learning and this entire chapter will be devoted to it, let's dive into it in more detail.


    \section{Introduction}\label{sec:rl-introduction}
    Reinforcement learning (RL) is an exciting field at the intersection of artificial intelligence (AI) and machine learning (ML) that deals with training agents to make optimal decisions in dynamic environments. RL is inspired by the way humans learn from experience, like \texttt{trial-and-error}, and an agent interacts with an environment the same way and receives feedback in the form of \emph{rewards} (typically positive number, e.g.: $1$) or \emph{penalties} (typically negative number, e.g.: $-1$), and uses this feedback to learn and improve its decision-making abilities.

    At the heart of RL lies the concept of an agent, which that takes actions in an environment to achieve specific goals. The environment is typically modeled as a Markov decision process (MDP), which is a mathematical framework that describes how an agent interacts with an environment in discrete time steps. An MDP is defined by a tuple $(S, A, P, R)$, where $S$ is the state space, $A$ is the action space, $P$ is the transition probability function, and $R\in\mathbb{R}$ is the reward.

    The goal of an RL agent is to learn a policy, denoted by $\pi$, which is a mapping from states to actions that maximizes the cumulative reward $G_t$ over time $T$. The agent uses this policy to select actions at each time step, and the environment responds with a new state and a reward. The agent then updates its policy based on the observed rewards and states, aiming to improve its decision-making abilities and achieve higher rewards in the long run. RL does not require labeled data, because learns from observations and rewards via interaction with the environment.

%    RL algorithms can be broadly categorized into two main types: model-free and model-based. Model-free algorithms are then further divided into value-based and policy-based methods, such as Q-learning (value-based) and policy gradient methods (policy-based), learn directly from the interactions with the environment without explicitly modeling the transition probabilities and rewards. Model-based algorithms, on the other hand, learn a model of the environment and use it to make decisions. These algorithms have their strengths and weaknesses, and the choice between them depends on the specific problem and requirements.

    The sequence of states, actions and rewards that the agent experiences is called a trajectory, and it look like this:
    \begin{equation}
        \label{eq:trajectory}
        (S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \dots, S_{T-1}, A_{T-1}, R_T, S_T)
    \end{equation}

    This sequence of \emph{state-action-reward} can be finite or infinite, depending on the environment and the agent's goal. Pretty good example of this is the game of chess, where the game ends when one of the players wins or the game is a draw. In this case, the trajectory is finite, and the agent's goal is to maximize the cumulative reward over time $T$. On the other hand, the self-driving car example is an infinite-horizon problem, where the agent's goal is to maximize the cumulative reward over an infinite time horizon or until the car reaches its destination~\cite{FITMT25127}.

    \begin{figure}[h]
        \includegraphics[width=0.7\linewidth]{image/agent-environment}
        \centering
        \caption{The agent interacts with the environment and learns to maximize the cumulative reward over time $T$.}
        \label{fig:rl-introduction}
    \end{figure}

    Because RL agent is categorized by the way it learns, there are many different types of RL agents. We will try to differentiate them in this chapter, because it is important to understand teh differences between them, to understand the behavior of the agent and to choose the right algorithm for the problem. Based on the different computable approaches we can categorize RL agents:
    \begin{multicols}{2}
        \begin{itemize}
            \item \textbf{Value-based}
            \begin{itemize}
                \item \textcolor{darkgray}{No Policy (implicit)}
                \item Value function
            \end{itemize}
            \item \textbf{Policy-based}
            \begin{itemize}
                \item Policy
                \item \textcolor{darkgray}{No Value function}
            \end{itemize}
            \item \textbf{Actor-critic}
            \begin{itemize}
                \item Policy
                \item Value function
            \end{itemize}
        \end{itemize}
        \columnbreak
        \begin{itemize}
            \item \textbf{Model-based}
            \begin{itemize}
                \item Policy and/or Value function
                \item \textcolor{darkgray}{No model}
            \end{itemize}
            \item \textbf{Model-free}
            \begin{itemize}
                \item Policy and/or Value function
                \item Model
            \end{itemize}
        \end{itemize}
    \end{multicols}

    \emph{Model-based} algorithm are closely relate to \emph{Planning algorithms} which are trying to create a model of the environment and use it to make decisions, they have knowledge of the transition probabilities and rewards from the beginning, we will discuss them in \cref{sec:model-based-methods}. \emph{Value-based} algorithms can implicitly infer a policy, but do not explicitly compute it, we will discuss them in \cref{sec:value-based-methods}, and \emph{Policy-based} algorithms compute a policy function by themselves, and are discussed in \cref{sec:policy-based-methods}. Another agents like \emph{Actor-critic methods} compute a policy function and a state-value function at the same time~\cite{rl-course-david-silver, sutton2018reinforcement}.

    \begin{figure}[h]
        \includegraphics[width=0.5\linewidth]{image/model-value-policy}
        \centering
        \caption{The relationship between model, value function and policy. Source:~\cite{FITMT25127}}
        \label{fig:model-value-policy-introduction}
    \end{figure}


    \section{Markov Theory}\label{sec:markov-models-for-rl}
    This section provides introduction to Markov theory, which is the fundamental building blocks of Reinforcement Learning. We begin by \emph{Markov Process}, described in~\cref{subsec:markov-process}, which is a stochastic process that satisfies the Markov property. We then move on to \emph{Markov Reward Process}, described in~\cref{subsubsec:markov-reward-process}, which is a Markov process with a reward function. Finally, we describe the most important for RL \emph{Markov Decision Process}, described in~\cref{subsec:markov-decision-process}, which is a Markov Reward process with a decision-making ability.

    \subsection{Markov Process}\label{subsec:markov-process}
    The Markov process (MP) (also called Markov Chains) describes the states of an environment and models the dynamics of state transitions. In an MP, an agent can only observe the changing states of the environment and has no influence over them. Markov processes have two key properties. Firstly, state transitions are non-deterministic and influenced by randomness. States are modeled as realizations of random variables, defined in \cref{subsubsec:probability-functions}. Secondly, the future state is only dependent on the current state, and not on previous states, simplifying causality with the Markov property~\cite{FITMT25127}.

    \subsubsection{Probability Functions}\label{subsubsec:probability-functions}

    The first property of an MP states that each concrete state of an environment is the realization of a discrete random variable \(X\) from set \(V\), with a certain probability \(P(X=x)\), where \(x \in V\). A state is a realization of a random experiment that the environment assumes with a certain probability, and this can be represented as a probability function:
    \begin{equation}
        P(X = X(\omega)) = P(X = x)
    \end{equation}

    The repeated successive execution of a random experiment can be represented as a stochastic process, which is a sequence of random variables, e.g., \(X_t(\omega), X_{t+1}(\omega), \ldots X_n(\omega)\), where a single term can be shortened to \(X_t\) and it represents the state of the environment at time \(t\)~\cite{FITMT25127, rao2022foundations}.

    \subsubsection{Stochastic Process (Random Process)}\label{subsubsec:stochastic-process}
    A stochastic process is defined as a collection of random variables defined on a common probability space $(\Omega, \mathcal{F}, P)$, where $\Omega$ is a sample space, $\mathcal{F}$ is a $\sigma$-algebra, and $P$ is a probability measure; and the random variables, indexed by some set $T$, all take values in the same mathematical space $S$, which must be measurable with respect to some $\sigma$-algebra $\Sigma$.

    In other words, for a given probability space $(\Omega, \mathcal{F}, P)$ and a measurable space $(S, \Sigma)$, a stochastic process is a collection of $S$-valued random variables, which can be written as:~\cite{enwiki:1148510872}
    \[
        \{X(t) : t \in T\}.
    \]

    In stochastic processes the probability that the environment assumes a certain state depends on the realized states of previous random variables. For example, if the weather forecast is assumed to be a stochastic process, then the yesterday’s weather may still have an influence on tomorrow’s weather. To represent this causality complicates the modeling of stochastic processes, so that with definition of Markov property the dependence of future states is assumed only on the current state. This is the second important property of Markov process~\cite{FITMT25127}.

    \subsubsection{Markov Property}\label{subsubsec:markov-property}
    The Markov property, which is defined using conditional probability, states that ``The future is independent of the past, given the present."

    The stochastic process has the Markov property if and only if, for all time steps $t \in N$, the conditional probability of the next state given the current state is equal to the conditional probability of the next state given all the previous states $X_1, \ldots, X_t$. $\probP[X_{t+1}|X_t] = \probP[X_{t+1}|X_1,\ldots,X_t]$

    This property has several advantages in practical reinforcement learning, including the uniqueness and distinctiveness of states, as well as the ability to precisely formulate the probability of state transitions, defined as:~\cite{FITMT25127}
    \begin{equation}
        \mathcal{P}(x'|x)=\probP(X_{t+1}=x'|X_t=x)
    \end{equation}

    Given n possible states, $s\in S$, then the probability of transitioning from state $s$ to state $s'$ can be represented as a matrix $P$, and because probability summation rule, the sum of transition probabilities from state $s$ to any other state $s'$ must equal to 1.

    The Markov process is stochastic process that satisfies the Markov property and is described as tuple $\left(\mathcal{S}, \mathcal{P}\right)$ for which holds:\cite{ABATE2021102207}
    \begin{itemize}
        \item $\mathcal{S} = s_1, s_2, ..., s_n$ is a finite set of states
        \item $\mathcal{P}$ is an $n\times n$ transition probability matrix which sums to 1 for each row, so each value $p_{ij}$ is the probability of transitioning from state $s_i$ to state $s_j$ in interval $\left< 0;1 \right>$
    \end{itemize}

    \begin{equation}
        \mathcal{P}=\begin{bmatrix}
                        p_{11} & p_{12} & p_{13} & \dots  & p_{1n} \\
                        p_{21} & p_{22} & p_{23} & \dots  & p_{2n} \\
                        \vdots & \vdots & \vdots & \ddots & \vdots \\
                        p_{n1} & p_{n2} & p_{n3} & \dots  & p_{nn}
        \end{bmatrix}
    \end{equation}

    \subsubsection{Starting States}
    The probability distribution of start states is denoted as $\mu : N \rightarrow [0,1]$ in order to perform simulations and compute the probability distribution of states at specific future time steps. A Markov Process is fully specified by the transition probability function $P$, which governs the complete dynamics of the process.
    \begin{itemize}
        \item Specification of the transition probability function $P$.
        \item Specification of the probability distribution of start states (denote this as $\mu : N \in[0, 1])$.
    \end{itemize}
    Given $\mu$ and $P$, we can generate sampling traces of the Markov Process and answer questions such as probability distribution of states at specific future time steps or expected time of first occurrence of a specific state, given a certain starting probability distribution $\mu$. The separation of concerns between $P$ and $\mu$ is key to the conceptualization of Markov Processes~\cite{rao2022foundations}.

    \subsubsection{Terminal States}
    Markov Processes can terminate at specific states (e.g., based on rules for winning or losing in games). Termination can occur after a variable number of time steps (episodic) or after a fixed number of time steps (as in many financial applications). If all sampling traces of the Markov Process reach a terminal state, they are called episodic sequences. The notion of episodic sequences is important in Reinforcement Learning. In some financial applications, the Markov Process terminates after a fixed number of time steps $T$, and states with time index $t = T$ are labeled as terminal states. States with time index $t < T$ transition to states with time index $t + 1$~\cite{rao2022foundations}.

    \begin{figure}[h!]
        \begin{center}
            \begin{tikzpicture}
                [->,>=stealth,shorten >=1pt,auto,node distance=3cm,
                thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

                % Nodes
                \node[main node] (A) {$S_0$}; % Starting state
                \node[main node] (B) [right of=A] {$S_1$};
                \node[main node] (C) [right of=B] {$S_2$};
                \node[main node] (T) [right of=C] {$S_3$}; % Terminal state

                % Edges
                \path[every node/.style={font=\sffamily\small}]
                (A) edge [bend left] node [above] {0.5} (B)
                (B) edge [bend left] node [below] {0.5} (A)
                (B) edge [bend left] node [above] {0.5} (C)
                (C) edge [bend left] node [below] {0.5} (B)
                (C) edge [bend left] node [above] {0.5} (T);
            \end{tikzpicture}
            \caption{Markov Process with Start state $S_0$ and Terminal state $S_3$, because there is no edge from $S_3$.}
            \label{fig:markov-process}
        \end{center}
        \centering
    \end{figure}

    The examples shown here only include states and transition probabilities. To fully define an environment within the framework of RL, actions and rewards also need to be defined.

    \subsection{Markov Reward Process}\label{subsubsec:markov-reward-process}
    Markov Reward Process (MRP) is a Markov Process with rewards. These rewards are random, and all we need to do is to specify the probability distributions of these rewards as we make state transitions. The main purpose of Markov Reward Processes is to calculate how much reward we would accumulate (in expectation, from each of the non-terminal states) if we let the process run indefinitely, bearing in mind that future rewards need to be discounted appropriately $\gamma$ (otherwise, the sum of rewards could blow up to $\infty$). In order to solve the problem of calculating expected accumulative rewards from each non-terminal state, we will first set up some formalism for Markov Reward Processes and develop some theory on calculating rewards accumulation.

    The main objective of an RL agent is to maximize the sum of rewards from each time-step, also known as the return. The agent can observe different episodes in the Markov process, but lacks the means to determine the actual quality of an episode. By calculating the return, we can precisely measure the goodness of an episode or even a single state using the \emph{state-value function}, defined in \cref{subsubsec:state-value-function}. This allows the agent to actively transition to favorable states and maximize the return.

%    \begin{definition}
%        A Markov Reward Process is a Markov Process with a time-indexed sequence of Reward random variables $R_t \in D$ (a countable subset of $R$) for time steps $t = 1, 2, \cdot$, satisfying the Markov Property (including Rewards): $P[(Rt+1, St+1)|St, St−1, ..., S0] = P[(Rt+1, St+1)|St]$ for all $t ≥ 0$.
%    \end{definition}

%    \begin{definition}
%        \textbf{Markov Reward Process} is a Markov Process, along with a time-indexed sequence of Reward random variables $R_t \in D$ (a countable subset of $\mathbb{R}$) for time steps $t = 1, 2, \ldots$, satisfying the Markov Property (including Rewards): $P[(R_{t+1}, S_{t+1})|S_t, S_{t-1}, \ldots, S_0] = P[(R_{t+1}, S_{t+1})|S_t]$ for all $t \geq 0$.
%    \end{definition}

    \begin{definition}
        Markov Reward Processes is a tuple $(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$ for which holds:\cite{rao2022foundations}
        \begin{itemize}
            \item $\mathcal{S} = s_1, s_2, ..., s_n$ is a finite set of states
            \item $\mathcal{P}$ is an $n\times n$ transition probability matrix which sums to 1 for each row, so each value $p_{ij}$ is the probability of transitioning from state $s_i$ to state $s_j$ in interval $\left< 0;1 \right>$
            \item $\mathcal{R}$ is a sequence of random variables $R_1, R_2, ..., R_n$ where $R_t$ is a random variable that represents the reward for transitioning from state $s_t$ to state $s_{t+1}$
            \item $\gamma$ is a discount factor in interval $\left< 0;1 \right>$
        \end{itemize}
        \begin{equation}
            \mathcal{P}(s,r,s') = \probP\left[ R_{t+1} = r, S_{t+1} = s' | S_t = s \right] \text{ for time steps } t = 0, 1, 2, \ldots
        \end{equation}
        \begin{align*}
            \text{ for all } s \in N, r \in D, s' \in S, \text{ such that } \sum_{s' \in S} \sum_{r \in D} \mathcal{P}(s,r,s') = 1 \text{ for all } s \in N
        \end{align*}
    \end{definition}

    \paragraph{Reward function}
    The reward function $\mathcal{R}(s)$ is a function that maps a state $s$ to a reward $r$ and specify how much reward and agent expects from the environment given current state $s$. If an agent is in a state $s$ at time $t$, the agent receives reward  $R_{t+1}$  at time $t + 1$,  when it transitions to a subsequent state $s'$. Rewards of an episode can be represented as a sequence $(R_1, R_2, \ldots, R_t)$~\cite{FITMT25127}.

    \paragraph{Expected reward}
    The expected reward $G_t$ of a state $s$ is the discounted sum of rewards in a single episode.
    \begin{equation}
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    \end{equation}

    We can also calculate the expected rewards for state-action pairs as a two-argument function $r: S \times A \to R$.
    The expected reward of a state-action pair as two-argument function $r: \mathcal{S}\times \mathcal{A}\leftarrow \mathbb{R}$ is defined as:
    \begin{equation}
        r(s, a) = \sum_{t=0}^{\infty} \gamma^t R_{t+1} P(S_{t+1} = s|S_t = s, A_t = a)
    \end{equation}

    The calculation also involves the discount factor $\gamma$ which is a value in the interval $\langle 0, 1 \rangle$. If $\gamma$ is equal to one, the series value goes to infinity. The agent can only calculate the return in the case of always terminating episodes. If $\gamma$ is less than one, the return has a finite value, allowing the agent to determine the quality of an episode. The discount factor is not only useful mathematically but also for tuning the agent's rewards. If early rewards in an episode are more significant than later ones, $\gamma$ should be close to zero. If the rewards represent monetary gains, then it is the case, as early rewards earn additional interest. On the other hand, the closer $\gamma$ is to one, the more important later rewards are.

    The return, denoted as $G_t$, can be used to calculate the sum of discounted rewards in an episode. The state-value function, as defined in \cref{subsubsec:state-value-function}, determines the expected long-term return starting from a specific state. Since episodes may start with different states due to state transitions probabilities, the expected return of a particular state is the expected value of the conditional density function over the probabilities of returns for that state. Thus, $G_t$ can be treated mathematically as a continuous random variable~\cite{FITMT25127, rao2022foundations}.

    \paragraph{Receive rewards}
    The agent receives rewards for its actions in the environment. An agent should receive a reward for a good action and a penalty for a bad action. Good actions are those that lead to the agent's main goal, while bad actions are those that lead to a state that is not desirable for the agent. That is, an agent should not receive (one small) reward when it can then receive a large penalty, for example in chess by taking one piece, when it can then lose the game by getting checkmate{sutton2018reinforcement}.

    \subsection{Markov Decision Process}\label{subsec:markov-decision-process}
    Markov decision process (MDP) is about \emph{sequential decision-making} under conditions of \emph{sequential uncertainty}. In previous subsections we discussed the aspect of \emph{sequential uncertainty} using the framework of Markov Processes, and extended it to incorporate uncertain Reward $R_t\in\mathcal{R}$ at each state transition $p(s',r|s)$, referred to as Markov Reward Processes. However, this framework lacks the notion of \emph{sequential decision-making}. Now we further extend the Markov Reward Processes framework to incorporate the concept of \emph{sequential decision-making}, which is formally known as Markov Decision Processes.


    %%%
    Similar to the definitions of Markov Processes and Markov Reward Processes, for ease of exposition, the definitions and theory of Markov Decision Processes below will be for discrete-time, for countable state spaces and countable set of pairs of next state and reward transitions (with the knowledge that the definitions and theory are analogously extensible to continuous-time and uncountable spaces, which we shall indeed encounter later in this book).

    \begin{definition}
        A Markov Decision Process comprises of:
        \begin{itemize}
            \item A countable set of states $S$ (known as the State Space), a set $T \subseteq S$ (known as the set of Terminal States), and a countable set of actions $A$ (known as the Action Space).
            \item A time-indexed sequence of environment-generated random states $S_t \in S$ for time steps $t = 0, 1, 2, \ldots$, a time-indexed sequence of environment-generated Reward random variables $R_t \in D$ (a countable subset of $\mathbb{R}$) for time steps $t = 1, 2, \ldots$, and a time-indexed sequence of agent-controllable actions $A_t \in A$ for time steps $t = 0, 1, 2, \ldots$. (Sometimes we restrict the set of actions allowable from specific states, in which case, we abuse the $A$ notation to refer to a function whose domain is $\mathbb{N}$ and range is $A$, and we say that the set of actions allowable from a state $s \in \mathbb{N}$ is $A(s)$.)
            \item \emph{Markov Property}
            \[
                P[(R_{t+1}, S_{t+1})|(S_t, A_t, S_{t-1}, A_{t-1}, \ldots , S_0, A_0)] = P[(R_{t+1}, S_{t+1})|(S_t, A_t)] \text{ for all } t \geq 0
            \]
            \item \emph{Termination} If an outcome for $S_T$ (for some time step $T$) is a state in the set $T$, then this sequence outcome terminates at time step $T$. As in the case of Markov Reward Processes, we denote the set of non-terminal states $S - T$ as $N$ and refer to any state in $N$ as a non-terminal state. The sequence: $S_0, A_0, R_1, S_1, A_1, R_1, S_2, \ldots$ terminates at time step $T$ if $S_T \in T$ (i.e., the final reward is $R_T$ and the final action is $A_{T-1}$).
        \end{itemize}
    \end{definition}

    \subsubsection{State-value Function}\label{subsubsec:state-value-function}
    The state-value function provides information about the long-term expected return for each state in an environment. With this information, an agent can determine which state to transition to in order to maximize the return of an episode. Specifically, the agent should choose the state that has the highest long-term expected return. However, in a Markov Reward Process (MRP), there are no actions available for the agent to transition to different states. Actions need to be defined in the context of a Markov Decision Process (MDP), which will be introduced later.

    If the agent observes a sequence of states and rewards, it can remember the subsequent rewards for each state, calculate the return of an episode, and iteratively update the probabilities for higher occurrence of returns over multiple episodes. As the number of episodes approaches infinity, the estimated probabilities converge to the true probabilities, and the long-term expected return of a state can be accurately determined. Intuitively, the more episodes and consequent returns an agent observes, the better it can estimate the value of each state. We will introduce the methods for determining the state-value function and the action-value function in the next subsection, along with a recursive iterative approach for calculating the state-value function based on Bellman's equation~\cite{FITMT25127}.

    To derive Bellman's equation, we need to make use of the definition of the state-value function:
    \[
        v(s) = E[G_t | S_t = s] = E[R_{t+1} + \gamma G_{t+1} | S_t = s] = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = R(s) + \gamma \sum_{s' \in \mathcal{S}} P(s' | s) v(s')
    \]
    where $\gamma$ is the discount factor, $R(s)$ is the immediate reward for state $s$, $P(s' | s)$ is the transition probability from state $s$ to state $s'$, and $\mathcal{S}$ is the set of all possible states in the environment. The last equation expresses that the long-term expected return of a state depends only on the immediate reward and the long-term expected return of the subsequent state.

    By extending the equation to include actions, we arrive at the most important equation in all of reinforcement learning: the Bellman equation~\cref{subsubsec:bellman-equation}. It states that to calculate the long-term expected return from a state, an agent only needs to add together the reward of the current state and the long-term expected return of the next state~\cite{FITMT25127}.

    \subsubsection{Stochastic Policy}\label{subsubsec:policy}
    A policy, denoted by $\pi$, in the context of MDPs, is a function that maps states to actions. It represents the agent's strategy or decision-making rule for selecting actions based on the current state. Mathematically, a policy is defined as: $\pi: S \rightarrow A$, where $S$ is the set of states and $A$ is the set of actions. Notation for a policy is as follows:
    \begin{equation}
        \pi(a|s)=\mathbb{P}(A_t=a|S_t=s)
    \end{equation}
    The policy can be deterministic, meaning it selects a single action for each state, or it can be stochastic, meaning it selects actions probabilistically based on some probability distribution over actions for each state. A policy is a function that maps states to actions and represents the agent's decision-making strategy\cite{sutton2018reinforcement}.

    Policies and action-value functions are closely related and are used interchangeably in many reinforcement learning algorithms, but they are conceptually distinct.

%    In the more general case, where states or rewards are uncountable, the same concepts apply except that the mathematical formalism needs to be more detailed and more careful. Specifically, we'd end up with integrals instead of summations, and probability density functions (for continuous probability distributions) instead of probability mass functions (for discrete probability distributions). For ease of notation and more importantly, for ease of understanding of the core concepts (without being distracted by heavy mathematical formalism), we've chosen to stay with discrete-time, countable $S$, countable $A$,

    \subsubsection{Value Function for stochastic Policy $\pi$}
    The value function $V^{\pi}(s)$ for a stochastic policy $\pi$ is the expected cumulative discounted reward the agent can obtain from state $s$ by following policy $\pi$ and then continuing to follow $\pi$ thereafter. It is defined as:
    \begin{equation}
        \label{eq:Vpi}
        \begin{split}
            V^{\pi}(s) &\doteq \mathbb{E}_{\pi}\left[ G_t|S_t=s \right] \\
            &= \mathbb{E}\left[ R_{t+1} = \gamma G_{t+1} | S_t=s \right] \\
            &= \mathbb{E}_{\pi}\left[ \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}|S_t=s \right] \\
            &= \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s', r} p(R_{t+1}, S_{t+1}|S_t, A_t) \left[r + \gamma v_{\pi}(s')  \right]\text{, for all } s \in \mathcal{S}
        \end{split}
    \end{equation}
    where $\mathbb{E}_{\pi}$ denotes the expectation with respect to the states and rewards generated by following policy $\pi$.

    \subsubsection{Action-Value Function}
    An action-value function, also known as a Q-function, denoted by $Q$, in the context of MDPs, is a function that estimates the expected cumulative reward of an agent starting from a given state, taking a specific action, and following a specific policy $\pi$ thereafter. Mathematically, an action-value function is defined as $Q: S \times A \rightarrow \mathbb{R}$, where $S$ is the set of states and $A$ is the set of actions. The action-value function $Q(s,a)$ represents the expected cumulative reward from taking action $a$ in state $s$, following the policy $\pi$ thereafter. In other words, it quantifies the value of selecting a particular action in a particular state under a specific policy. An action-value function is a function that estimates the expected cumulative reward of taking a specific action from a specific state, following a specific policy thereafter~\cite{GPT3.5}.

    The action-value function $Q^{\pi}(s, a)$ for a policy $\pi$ is the expected cumulative discounted reward the agent can obtain from state $s$, taking action $a$, and then following policy $\pi$ thereafter. It is defined as:
    \[
        Q^{\pi}(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} | S_0 = s, A_0 = a\right]
    \]

    \subsubsection{Optimal Value and Action-Value Functions}
    The optimal value function $V^*(s)$ is the maximum value function over all possible policies. It is defined as:
    \[
        V^*(s) = \max_{\pi} V^{\pi}(s)
    \]
    Similarly, the optimal action-value function $Q^*(s, a)$ is the maximum action-value function over all possible policies. It is defined as:
    \[
        Q^*(s, a) = \max_{\pi} Q^{\pi}(s, a)
    \]
    %%%

    \subsubsection{Stochastic Policy Function}\label{subsubsec:stochastic-policy-function}
    TODO

    \subsubsection{Bellman's Equation}\label{subsubsec:bellman-equation}
    The Bellman equation is a fundamental equation in reinforcement learning that expresses the relationship between the state-value function and the expected return from a state. It is given by:

    \begin{equation}
        v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
    \end{equation}

    where $v(s)$ is the state-value function, $R_{t+1}$ is the immediate reward, $S_{t+1}$ is the next state, and $\gamma$ is the discount factor.

    The Bellman equation can be used to iteratively calculate the state-value function by updating the value of each state based on the immediate reward and the long-term expected return of the next state\cite{rao2022foundations}.

    \subsubsection{Bellman's Optimality Equation}\label{subsubsec:bellman-optimality-equation}
    TODO


    \section{Model-based methods}\label{sec:model-based-methods}
    As mentioned in Section~\cref{sec:rl-introduction}, RL algorithms can be broadly categorized into two main types: model-free and model-based. Model-based methods rely on \emph{planning} as they primary component, while model-free methods rely on \emph{learning}~\cite{sutton2018reinforcement}. Despite their distinctions, both approaches involve computing value functions and using them to update approximate value functions based on future events.

    One popular approach is to use a probabilistic model of the environment, which can be represented as a transition function $P(s_{t+1}|s_t, a_t)$ that describes the probability of transitioning from state $s_t$ to state $s_{t+1}$ when taking action $a_t$. Some popular probabilistic model-based RL methods include Monte Carlo Tree Search, Neural Network Dynamics, and Probabilistic Ensembles with Trajectory Samplin.

    Another approach in model-based RL is to use a learned deterministic model of the environment, which can be represented as a function $f(s_t, a_t)$ that directly maps states and actions to next states. Deterministic model-based RL methods, such as World Models, learn an encoder to represent states, a recurrent neural network (RNN) to model dynamics, and a decoder to generate predicted next states.

    \subsection{Planning}\label{subsec:planning}
    A model of the environment refers to anything that an agent can use to predict the outcome of its actions. It can be either stochastic, where there are multiple possible outcomes with associated probabilities, or deterministic, where the outcome is fixed. Distribution models provide a description of all possible outcomes and their probabilities, while sample models generate a single outcome sampled from the probabilities.

    Planning, in the context of artificial intelligence, refers to a computational process that takes a model of the environment as input and produces or improves a policy for interacting with the environment. There are two main approaches to planning: \emph{state-space} planning and \emph{plan-space} planning. In \emph{state-space} planning, the focus is on searching through the \textbf{space of states} to find an optimal policy or path to a goal. Value functions are computed over states to guide the search. On the other hand, in \emph{plan-space} planning, the search is conducted through the \textbf{space of plans}, where operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space methods are challenging to efficiently apply to the stochastic sequential decision problems that are the primary focus in reinforcement learning~\cite{sutton2018reinforcement}.

    The common structure in \emph{state-space} planning and learning methods, as presented in this chapter, is that value functions are computed as intermediate steps using simulated experience. Value functions are used to estimate the expected future rewards of different actions or states, and they play a key role in improving the policy of an agent. Simulated experience is generated by using the model to simulate the environment, and the agent can update its value functions based on this simulated experience to guide its decision-making process.

    \begin{center}
        \begin{tikzpicture}[auto]
% Nodes
            \node[] (s) {model};
            \node[right=1cm of s] (a) {simulated experience};
            \node[right=2cm of a] (r) {values};
            \node[right=1cm of r] (s_p) {policy};

% Arrows
            \draw[thick, ->] (s) -- (a);
            \draw[thick, ->] (a) -- (r) node[midway, anchor=south] {backups};
            \draw[thick, ->] (r) -- (s_p);
        \end{tikzpicture}
    \end{center}

    Dynamic programming methods fit the structure of making sweeps through the space of states, generating possible transitions, computing backed-up values, and updating state estimates. Other state-space planning methods also fit this structure, with differences in updates, order, and retention of backed-up information. Planning methods are related to learning methods in estimating value functions through backing-up operations. Learning methods use real experience from the environment, while planning uses simulated experience from a model. Ideas and algorithms can be transferred between planning and learning. Planning in small, incremental steps allows for efficient interruption and redirection, which is beneficial for intermixing planning with acting and learning of the model. Planning in small steps may be the most efficient approach for large planning problems~\cite{sutton2018reinforcement}.
    \begin{algorithm}[H]
        \label{alg:random-sample-one-step-tabular-q-planning}
        \SetKwInOut{Input}{Input}
        \SetKwInOut{Output}{Output}
        \SetAlgoLined
        \Input{
            $S$: Set of states \\
            $A$: Set of actions \\
            $\alpha$: Learning rate \\
            $\gamma$: Discount factor \\
        }
        \While{True}{
            Select a state $s \in S$, action $a \in A(s)$ at random\;
            Send $s, a$ to a sample model, and obtain a sample next reward $r$ and a sample next state $s'$\;
            Apply one-step tabular Q-learning update to $s, a, r, s'$:
            \[
                Q(s, a) \leftarrow Q(s, a) + \alpha \cdot \left( r + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a) \right)
            \]
        }
        \caption{Random-sample one-step tabular Q-planning}
    \end{algorithm}

    \subsection{Trajectory Sampling}\label{subsec:trajectory-sampling}

    \subsection{Summary}\label{subsec:summary}

    Model-based RL offers several advantages over model-free methods. One major advantage is sample efficiency, as the agent can use the learned model of the environment to plan and generate simulated trajectories for learning, reducing the need for costly real-world interactions. Additionally, model-based RL can enable the agent to handle complex, high-dimensional state spaces and long-horizon tasks more effectively.

    However, model-based RL also faces challenges. One challenge is the accuracy of the learned model, as any errors in the model can lead to suboptimal policies. Another challenge is the computational cost of planning and decision-making using the learned model, as it requires additional computation compared to direct action selection in model-free methods.

    In conclusion, model-based RL is a promising approach that can offer sample-efficient learning and improved performance in complex environments. Various methods, such as probabilistic models and deterministic models, have been proposed in the literature. Despite some challenges, model-based RL continues to be an active area of research in machine learning.


    \section{Value-based methods}\label{sec:value-based-methods}


    \section{Policy-based methods}\label{sec:policy-based-methods}


    \section{Model-free methods}\label{sec:model-free-methods}

    \subsection{Value-based methods}\label{subsec:value-based-methods}
    TODO

    \subsubsection{Dynamic Programming}\label{subsubsec:dynamic-programming}
    TODO

    \paragraph{Policy Iteration}\label{par:policy-iteration}
    TODO

    \paragraph{Value Iteration}\label{par:value-iteration}
    TODO

    \paragraph{Monte Carlo Methods}\label{par:monte-carlo-methods}
    TODO

    \paragraph{Temporal Difference Methods}\label{par:temporal-difference-methods}
    TODO

    \subsection{Policy-based methods}\label{subsec:policy-based-methods}

    \subsubsection{Policy Gradient Methods}\label{subsubsec:policy-gradient-methods}
    TODO

    \subsubsection{Actor-Critic Methods}\label{subsubsec:actor-critic-methods}
    TODO

    \subsubsection{Monte Carlo Policy Gradient Methods}\label{subsubsec:monte-carlo-policy-gradient-methods}
    TODO

\end{document}
