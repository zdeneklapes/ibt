@article{Doe:2020,
    author = "Doe, John",
    title = "Jak citovat",
    subtitle = "Citace článku",
    journal = "Seriál o tvorbě prací",
    journalsubtitle = "Formální náležitosti",
    howpublished = "online",
    address = "Brno",
    publisher = "Fakulta informačních technologií VUT v Brně",
    contributory = "Přeložil Jan NOVÁK",
    edition = "1",
    version = "verze 1.0",
    month = 2,
    year = "2020",
    revised = "revidováno 12. 2. 2020",
    volume = "4",
    number = "24",
    pages = "8--21",
    cited = "2020-02-12",
    doi = "10.1000/BC1.0",
    issn = "1234-5678",
    note = "Toto je zcela vymyšlená citace",
    url = "https://merlin.fit.vutbr.cz"
}

@book{sutton2018reinforcement,
    title = {Reinforcement Learning, second edition: An Introduction},
    author = {Sutton, R.S. and Barto, A.G.},
    isbn = {9780262039246},
    lccn = {2018023826},
    series = {Adaptive Computation and Machine Learning series},
    url = {https://books.google.cz/books?id=5s-MEAAAQBAJ},
    year = {2018},
    publisher = {MIT Press}
}

@book{rao2022foundations,
    title = {Foundations of Reinforcement Learning with Applications in Finance},
    author = {Rao, A. and Jelvis, T.},
    isbn = {9781000801101},
    url = {https://books.google.cz/books?id=n\_-VEAAAQBAJ},
    year = {2022},
    publisher = {CRC Press}
}

@article{meanvarianceportfoliooptimazation,
    author = {Soeryana, E. and Fadhlina, N. and Firman, Sukono and Rusyaman, E. and Supian, S.},
    year = {2017},
    month = {01},
    pages = {012003},
    title = {Mean-variance portfolio optimization by using time series approaches based on logarithmic utility function},
    volume = {166},
    journal = {IOP Conference Series: Materials Science and Engineering},
    doi = {10.1088/1757-899X/166/1/012003}
}

@misc{finrl-portfolio-allocation-2020,
    doi = {10.48550/ARXIV.2111.03995},
    url = {https://arxiv.org/abs/2111.03995},
    author = {Guan, Mao and Liu, Xiao-Yang},
    keywords = {Portfolio Management (q-fin.PM), Artificial Intelligence (cs.AI), FOS: Economics and business, FOS: Economics and business, FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach},
    publisher = {arXiv},
    year = {2021},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{sirucek-2015,
    author = {Širůček, Martin and Křen, Lukáš},
    journal = {Acta Universitatis Agriculturae et Silviculturae Mendelianae Brunensis},
    month = {9},
    number = {4},
    pages = {1375--1386},
    publisher = {Mendel University Press},
    title = {{Application of Markowitz Portfolio Theory by Building Optimal Portfolio on the US Stock Market}},
    volume = {63},
    year = {2015},
    doi = {10.11118/actaun201563041375},
    url = {http://dx.doi.org/10.11118/actaun201563041375},
}

@misc{Model-Free-Reinforcement-Learning-for-Asset-Allocation,
    doi = {10.48550/ARXIV.2209.10458},
    url = {https://arxiv.org/abs/2209.10458},
    author = {Oshingbesan, Adebayo and Ajiboye, Eniola and Kamashazi, Peruth and Mbaka, Timothy},
    keywords = {Portfolio Management (q-fin.PM), Machine Learning (cs.LG), FOS: Economics and business, FOS: Economics and business, FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Model-Free Reinforcement Learning for Asset Allocation},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{rl-course-david-silver,
    author = {David Silver},
    title = {Introduction to Reinforcement Learning with David Silver},
    year = {2015},
    url = {https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver},
    howpublished = "online",
    cited = "2023-04-08",
}

@misc{FinRL-Tutorials,
    author = {AI4Finance-Foundation},
    title = {FinRL-Tutorials},
    year = {2021},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/AI4Finance-Foundation/FinRL-Tutorials}},
    commit = {c677be3e1b17f2f28a625170963f6d04ff110636}
}

@misc{Pyfolio,
    author = {Quantopian Inc.},
    title = {Pyfolio},
    year = {2020},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/quantopian/pyfolio}},
    commit = {4b901f6d73aa02ceb6d04b7d83502e5c6f2e81aa}
}

@misc{investopedia,
    author = {Investopedia},
    title = {Investopedia},
    url = {https://www.investopedia.com},
    howpublished = "online",
    cited = "2023-04-16",
}

@misc{kurniawati2021partially,
    title = {Partially Observable Markov Decision Processes (POMDPs) and Robotics},
    author = {Hanna Kurniawati},
    year = {2021},
    eprint = {2107.07599},
    archivePrefix = {arXiv},
    primaryClass = {cs.RO}
}

@misc{finta,
    author = {peerchemist},
    title = {Finta},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/peerchemist/finta}},
    commit = {af01fa594995de78f5ada5c336e61cd87c46b151}
}

@misc{enwiki:1146097966,
    author = "{Wikipedia contributors}",
    title = "Pearson correlation coefficient --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Pearson_correlation_coefficient&oldid=1146097966",
    note = "[Online; accessed 18-April-2023]"
}

@mastersthesis{FITMT25127,
    author = "David Vosol",
    type = "Master's thesis",
    title = "Aplikace posilovan\'{e}ho u\v{c}en\'{i} v \v{r}\'{i}zen\'{i} autonomn\'{i}ho vozidla",
    school = "Brno University of Technology, Faculty of Information Technology",
    year = 2022,
    location = "Brno, CZ",
    language = "czech",
    url = "https://www.fit.vut.cz/study/thesis/25127/"
}

@misc{enwiki:1148510872,
    author = "{Wikipedia contributors}",
    title = "Stochastic process --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Stochastic_process&oldid=1148510872",
    note = "[Online; accessed 10-April-2023]"
}

@article{ABATE2021102207,
    title = {Adaptive formal approximations of Markov chains},
    journal = {Performance Evaluation},
    volume = {148},
    pages = {102207},
    year = {2021},
    issn = {0166-5316},
    doi = {https://doi.org/10.1016/j.peva.2021.102207},
    url = {https://www.sciencedirect.com/science/article/pii/S0166531621000249},
    author = {Alessandro Abate and Roman Andriushchenko and Milan Češka and Marta Kwiatkowska},
    keywords = {Markov models, Probabilistic model checking, Approximation techniques, Adaptive aggregation},
    abstract = {We explore formal approximation techniques for Markov chains based on state–space reduction that aim at improving the scalability of the analysis, while providing formal bounds on the approximation error. We first present a comprehensive survey of existing state-reduction techniques based on clustering or truncation. Then, we extend existing frameworks for aggregation-based analysis of Markov chains by allowing them to handle chains with an arbitrary structure of the underlying state space – including continuous-time models – and improve upon existing bounds on the approximation error. Finally, we introduce a new hybrid scheme that utilises both aggregation and truncation of the state space and provides the best available approach for approximating continuous-time models. We conclude with a broad and detailed comparative evaluation of existing and new approximation techniques and investigate how different methods handle various Markov models. The results also show that the introduced hybrid scheme significantly outperforms existing approaches and provides a speedup of the analysis up to a factor of 30 with the corresponding approximation error bounded within 0.1%.}
}

@ARTICLE{7110602,
    author = {Liu, Derong and Wei, Qinglai and Yan, Pengfei},
    journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
    title = {Generalized Policy Iteration Adaptive Dynamic Programming for Discrete-Time Nonlinear Systems},
    year = {2015},
    volume = {45},
    number = {12},
    pages = {1577-1591},
    doi = {10.1109/TSMC.2015.2417510} }
}

@inproceedings{inproceedings,
    author = {Chan, Ka and Lenard, C. and Mills, Terence},
    year = {2012},
    month = {12},
    pages = {},
    title = {An Introduction to Markov Chains},
    doi = {10.13140/2.1.1833.8248}
}

@inproceedings{perotto:hal-02363599,
    TITLE = {{Tuning the Discount Factor in Order to Reach Average Optimality on Deterministic MDPs}},
    AUTHOR = {Perotto, Filipo Studzinski and Vercouter, Laurent},
    URL = {https://hal.science/hal-02363599},
    BOOKTITLE = {{International Conference on Innovative Techniques and Applications of Artificial Intelligence}},
    ADDRESS = {Cambridge, United Kingdom},
    YEAR = {2018},
    DOI = {10.1007/978-3-030-04191-5\_7},
    KEYWORDS = {Average Dynamic Programming ; Sensitive Dynamic Programming ; Reinforcement Learning ; Markovian Decision Process},
    PDF = {https://hal.science/hal-02363599/file/2018___SGAI__CR____Tuning_the_discount_for_average_optimal_D_MDPs.pdf},
    HAL_ID = {hal-02363599},
    HAL_VERSION = {v1},
}

@misc{adamczyk2023bounding,
    title = {Bounding the Optimal Value Function in Compositional Reinforcement Learning},
    author = {Jacob Adamczyk and Volodymyr Makarenko and Argenis Arriojas and Stas Tiomkin and Rahul V. Kulkarni},
    year = {2023},
    eprint = {2303.02557},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}


@misc{enwiki:1043516653,
    author = "{Wikipedia contributors}",
    title = "Modern portfolio theory --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2021",
    url = "https://en.wikipedia.org/w/index.php?title=Modern_portfolio_theory&oldid=1043516653",
    note = "[Online; accessed 16-April-2023]"
}

@misc{enwiki:1141766585,
    author = "{Wikipedia contributors}",
    title = "Dow Jones Industrial Average --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    url = "https://en.wikipedia.org/w/index.php?title=Dow_Jones_Industrial_Average&oldid=1141766585",
    note = "[Online; accessed 16-April-2023]"
}

@article{fama-2004,
    author = {Fama, Eugene F and French, Kenneth R},
    journal = {Journal of Economic Perspectives},
    month = {8},
    number = {3},
    pages = {25--46},
    publisher = {American Economic Association},
    title = {{The Capital Asset Pricing Model: Theory and Evidence}},
    volume = {18},
    year = {2004},
    doi = {10.1257/0895330042162430},
    howpublished = {\url{https://mba.tuck.dartmouth.edu/bespeneckbo/default/AFA611-Eckbo%20web%20site/AFA611-S6B-FamaFrench-CAPM-JEP04.pdf}},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TRPO

@misc{schulman2017trust,
    title = {Trust Region Policy Optimization},
    author = {John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
    year = {2017},
    eprint = {1502.05477},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

% PPO

@misc{schulman2017proximal,
    title = {Proximal Policy Optimization Algorithms},
    author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    year = {2017},
    eprint = {1707.06347},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

% A2C

@misc{mnih2016asynchronous,
    title = {Asynchronous Methods for Deep Reinforcement Learning},
    author = {Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
    year = {2016},
    eprint = {1602.01783},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

% SAC

@misc{haarnoja2018soft,
    title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
    author = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
    year = {2018},
    eprint = {1801.01290},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

% TD3

@misc{fujimoto2018addressing_td3,
    title = {Addressing Function Approximation Error in Actor-Critic Methods},
    author = {Scott Fujimoto and Herke van Hoof and David Meger},
    year = {2018},
    eprint = {1802.09477},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

% DQN

@misc{mnih2013playing_dqn,
    title = {Playing Atari with Deep Reinforcement Learning},
    author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year = {2013},
    eprint = {1312.5602},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

% DDPG

@misc{lillicrap2019continuous_ddpg,
    title = {Continuous control with deep reinforcement learning},
    author = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
    year = {2019},
    eprint = {1509.02971},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{rl-model-free,
    doi = {10.48550/ARXIV.1802.09081},
    url = {https://arxiv.org/abs/1802.09081},
    author = {Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey},
    keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Temporal Difference Models: Model-Free Deep RL for Model-Based Control},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{rl-model-based,
    doi = {10.48550/ARXIV.1901.08740},
    url = {https://arxiv.org/abs/1901.08740},
    author = {Yu, Pengqian and Lee, Joon Sern and Kulyatin, Ilya and Shi, Zekun and Dasgupta, Sakyasingha},
    keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Model-based Deep Reinforcement Learning for Dynamic Portfolio Optimization},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ddpg-algorithm,
    doi = {10.48550/ARXIV.1509.02971},
    url = {https://arxiv.org/abs/1509.02971},
    author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
    keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Continuous control with deep reinforcement learning},
    publisher = {arXiv},
    year = {2015},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ppo-algorithm,
    doi = {10.48550/ARXIV.1707.06347},
    url = {https://arxiv.org/abs/1707.06347},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
    keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Proximal Policy Optimization Algorithms},
    publisher = {arXiv},
    year = {2017},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{a2c-algorithm,
    doi = {10.48550/ARXIV.1802.09477},
    url = {https://arxiv.org/abs/1802.09477},
    author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
    keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Addressing Function Approximation Error in Actor-Critic Methods},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{sac-algorithm,
    doi = {10.48550/ARXIV.1801.01290},
    url = {https://arxiv.org/abs/1801.01290},
    author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
    keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
    publisher = {arXiv},
    year = {2018},
    copyright = {arXiv.org perpetual, non-exclusive license}
}


@bachelorsthesis{FITBT24626,
    author = "Julie Gyselov\'{a}",
    type = "Bachelor's thesis",
    title = "Controlling Autonomous Systems Based on Partially Observable Markov Decision Processes",
    school = "Brno University of Technology, Faculty of Information Technology",
    year = 2022,
    location = "Brno, CZ",
    language = "english",
    url = "https://www.fit.vut.cz/study/thesis/24626/"
}

@article{he-2002,
    author = {He, Guangliang and Litterman, Robert},
    journal = {SSRN Electronic Journal},
    publisher = {Elsevier BV},
    title = {{The Intuition Behind Black-Litterman Model Portfolios}},
    year = {2002},
    doi = {10.2139/ssrn.334304},
    howpublished = {\url{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=334304}},
}

@misc{ai4finance-Optimistic-Bull-or-Pessimistic-Bear,
    doi = {10.48550/ARXIV.1907.01503},
    url = {https://arxiv.org/abs/1907.01503},
    author = {Li, Xinyi and Li, Yinchuan and Zhan, Yuancheng and Liu, Xiao-Yang},
    keywords = {Statistical Finance (q-fin.ST), FOS: Economics and business, FOS: Economics and business},
    title = {Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation},
    publisher = {arXiv},
    year = {2019},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{article-q-learning-memory,
    author = {Zhao, Meng and Lu, Hui and Yang, Siyi and Guo, Fengjuan},
    year = {2020},
    month = {03},
    pages = {1-1},
    title = {The Experience-Memory Q-Learning Algorithm for Robot Path Planning in Unknown Environment},
    volume = {PP},
    journal = {IEEE Access},
    doi = {10.1109/ACCESS.2020.2978077}
}
