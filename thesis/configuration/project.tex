% Informace o práci/projektu / Information about the thesis
%---------------------------------------------------------------------------
\projectinfo{
%
%Prace / Thesis
    project={BP},            %typ práce BP/SP/DP/DR  / thesis type (SP = term project)
    year={2023},             % rok odevzdání / year of submission
    date=\today,             % datum odevzdání / submission date
%
%Nazev prace / thesis title
    title.cs={Využití zpětnovazebného učení pro automatickou alokaci akciového portfolia},  %  thesis title in czech language (according to assignment)
    title.en={Reinforcement Learning for Automated Stock Portfolio Allocation}, %  thesis title in english
    title.length={14.5cm}, %  setting the length of a block with a thesis title for adjusting a line break (can be defined here or below)
    sectitle.length={14.5cm}, %  setting the length of a block with a second thesis title for adjusting a line break (can be defined here or below)
    dectitle.length={14.5cm}, %  setting the length of a block with a thesis title above declaration for adjusting a line break (can be defined here or below)
%
%Autor / Author
    author.name={Zdeněk},   % jméno autora / author name
    author.surname={Lapeš},   % příjmení autora / author surname
%
%Ustav / Department
    department={UITS}, % UIFS/UITS/UPGM / fill in appropriate abbreviation of the department according to assignment: UPSY/UIFS/UITS/UPGM
%
% Školitel / supervisor
    supervisor.name={Milan},   % jméno školitele / supervisor name
    supervisor.surname={Češka},   % příjmení školitele / supervisor surname
    supervisor.title.p={doc. RNDr.},   %titul před jménem (nepovinné) / title before the name (optional)
    supervisor.title.a={Ph.D.},    %titul za jménem (nepovinné) / title after the name (optional)
%
% Klíčová slova / keywords
    keywords.cs={
        umělá inteligence, AI, posilované učení, alokace akciového portfolia, moderni teorie portfolia, Q-learning, neuronové sítě, akciový trh
    },
    keywords.en={
        artificial intelligence, AI, reinforcement learning, stock portfolio allocation, modern portfolio theory, Q-learning, neural networks, stock market
    },
%
% Abstrakt / Abstract
    abstract.cs={
        Tato práce je zaměřena na téma posilovacího učení aplikovaného na úlohu alokace portfolia. K dosažení tohoto cíle práce nejprve uvádí přehled základní teorie, která zahrnuje nejnovější metody založené na hodnotách a politikách. Následně je v práci popsáno prostředí portfolia Stock a nakonec jsou uvedeny podrobnosti o experimentu a implementaci. Podrobně je rozebrána tvorba datových souborů a její zdůvodnění a metodika. RL agent je poté vycvičen a otestován na třech datových sadách a získané výsledky jsou slibné a překonávají běžné benchmarky. Bylo však zjištěno, že roční výnos agenta stále není lepší než výnosy generované nejlepšími světovými investory. Pipeline byla implementována v jazyce Python 3.10 a ke sledování všech datových sad, modelů a hyperparametrů byla použita technologie Weights \& Biases. Závěrem lze říci, že tato práce představuje významný krok vpřed ve vývoji efektivnějších RL agentů pro finanční investice, kteří mají potenciál překonat i výkonnost nejlepších světových investorů.
    }, % abstrakt v českém či slovenském jazyce / abstract in czech or slovak language
    abstract.en={
        This thesis is focused on the topic of reinforcement learning applied to a task of portfolio allocation. To accomplish this objective, the thesis first presents an overview of the fundamental theory, which includes the latest value-based and policy-based methods. Following that, the thesis describes the Stock portfolio environment, and finally, the experimental and implementation details are presented. The creation of datasets is discussed in detail, along with the rationale and methodology behind it. The RL agent is then trained and tested on three datasets, and the results obtained are promising and outperform common benchmarks. However, it was discovered that the annual return of the agent is still not better than the returns generated by the world's top investors. The pipeline was implemented in Python 3.10, and technology from Weights \& Biases was used to monitor all datasets, models, and hyperparameters. In conclusion, this work represents a significant step forward in the development of more effective RL agents for financial investments, with the potential to exceed even the performance of the world's greatest investors.
    }, % abstrakt v anglickém jazyce / abstract in english
%  Declaration (for thesis in english should be in english; for project practice can be commented out)
    declaration={
        I hereby declare that this Bachelor's thesis was prepared as an original work by the author under the supervision of Mr. Milan Češka, Ph.D.. I have listed all the literary sources, publications and other sources, which were used during the preparation of this thesis.
    },
%
%  Acknowledgement (optional, ideally in the language of the thesis)
    acknowledgment={
        I would like to thank my supervisor, Mr. Milan Češka, for his guidance and support during the preparation of this thesis. I would also like to thank my family and friends for their support.
    },
%
%  Extended abstract (approximately 3 standard pages) - can be defined here or below
    extendedabstract={ % path must be like that because this is inserted directly into xlapes02.tex
        Tato práce se zaměřuje na využití posilovaného učení pro alokaci akciového portfolia. V úvodu jsou vysvětleny základní pojmy a algoritmy posilovaného učení, včetně \emph{Markovových procesů}, které jsou jeho základem. Poté si řekneme o \emph{Value based} metodách, mezi které patří Dynamic Programming, Monte Carlo a Temporal Difference. Value-Based metody se snaží naučit, jaká je kvalitu stavů, ve kterých se agent nacházel, a nebo kvalitu akcí provedených agentem v daných stavech. Dále jsou vysvětleny \emph{Policy based} metody, mezi které patří Stochastic Policy Gradient a REINFORCE (Monte Carlo Policy Gradients), které se snaží najít nejlepší rozhodovací strategii agenta, při které bude schopen dosáhnout svého cíle. Na závěr zmíníme teorii o hlubokém posilovaném učení, která využívá neuronové sítě pro naučení vhodné policy a také vysvětlíme koncept učení \emph{Aktor-kritik}, který kombinuje Value based a Policy based metody pro trenování agenta. V naší práci jsme použili následujicí algoritmy A2C, SAC, DDPG, PPO a TD3.

        V dalších kapitolách se věnujeme problematice \emph{alokace portfolia} a přiblížíme přístup k trenování agenta s posilovaným učením. Popíšeme také, jak jsme přistupovali k vytváření datasetu, včetně výběru jednotlivých indikátorů z následujicích analýz \emph{fundamentalní} a \emph{technickou} k popisu finančního prostředí. Poté vysvětlíme implementaci \emph{prostředí pro alokaci portfolia}, stanovíme akční prostor, který vymezuje možnosti agenta, v úkonu vybírání, jak moc bude která spolenost nakoupena do portfolia. V neposlední řadě vysvětlíme funkci odměn, která slouží k ohodnocování kvality provedených rozhodnutí agentem v prostředí.

        Dále představujeme způsob, kterým jsme vybírali hyperparametry, které pomáhají agentovi učit se chovat v daném prostředí. Hyperparametry volíme pomocí tzv. metody \emph{hyperparameters tuning}. A rozebereme zde seznamem parametrů, nad kterými se tuning prováděl, a vysvětlíme, jak a proč byli hodnoty těchto parametrů ohraničeny. Jako další experiment představujeme zkoumání vlivu datasetu na výkon agenta. V této části jsou představeny výsledky agentů, kteří byli natrénováni na třech různých datasetech a porovnáme, jak se agentovi dařilo v prostředí chovat. Dále je zde prezentován vliv každého datasetu na výkon agenta. Agenti jsou natrénováni na těchto datasetech \emph{fundamentální dataset}, \emph{technický dataset} a \emph{kombinace} a jsou porovnáni mezi sebou. Následuje popis experimentu zaměřeného na \emph{robustnost modelu}. Nejprve je proveden výběr nejlepší konfigurace z předchozího experimentu, kde byl prováděn hyperparametrický tuning. Následně natrénujeme několik modelů na stejné konfiguraci hyperparametrů a ty potom provnáme podle metriky \emph{výkon portfolia}, která vyjadřuje, jak velké zhodnocení dosáhla daná skladba akcií v portfoliu.

        V posledním experimentu jsou naše natrénované modely porovnány s \emph{veřejně dostupným} modelem využívajícím posilované učení pro alokaci portfolia, dále s standardními \emph{strategiemi} pro alokaci portfolia, jako je \emph{Maximalizace Sharpeho poměru} nebo \emph{Minimalizace rizika} portfolia, a nakonec s \emph{indexy}, jako jsou \emph{Dow Jones Index}, \emph{Nasdaq Composite}, \emph{S\&P500} a \emph{Russell 2000}. A experimentálně bylo zjistěno, že agent je schopen porazit všechny porovnávané strategie a většinu porovnávaných indexů ve zhodnocení portfolia. Také je dikutováno, jak si agent vedl, např. v době propadů na burze, a jakých výsledků bylo možné dosáhnout průměrným ročním zhodnocením.

        Experimenty byly prováděny z účelem vytvořit a zjistit, jakých výsledků může agent dosáhnout, zda bude možné použivat daného agenta na reálné burze. Z výsledků výše popsaných experimentů jsme experimentálně zjistili, že agent je schopen provádět alokaci portfolia dostatečně přesně, aby porazil standardní strategie a většinu porovnávaných indexů. Během experimentů jsme zjistili, jakých výsledků je možné agentem dosáhnout. Všechny výsledky jsou prezentovány veřejně ze stránky \emph{Weights \& Biases}, kde jsou uloženy všechny datasety a modely, a také současně všechny logované údalosti při tréninku a testování modelů. Tím jsme chtěli zajistit reprodukovatelnost výsledků a snadnou dostupnost pro další výzkumníci v této oblasti.
    },
%
    faculty={FIT}, % FIT/FEKT/FSI/FA/FCH/FP/FAST/FAVU/USI/DEF
}
